WHY DEVOPS ?
TO DELIVER THE APPLICATION SPEEDILY.

APPLICATION/PRODUCT/SOFTWARE --- > APP

WHAT IS MEANING OF APPLICATION ?
COLLECTION OF SERVICES.
EX: PAYTM, WHATSAPP

======================================
SDLC: SOFTWARE DEVELOPMENT LIFECYCLE.

PLAN : WHAT, SERVICES, COST, -------------
CODE : SELECT ONE PROGRAMMING
BUILD: COMBING THE BACKEND + FRONT END (DEP)
TEST : TO CHECK CODE IS WORKING PROPERLY OR NOT
DEPLOY: INSTALLING THE APP ON SERVER
OPERATE: TO CONFIGURE SERVER
MONITOR: TO CHECK HEALTH OF THE APP & SERVER


CLOUD: AWS/AZURE/GCP  --- > 
AWS: 34%    AZURE: 20%  GCP:10%


TOOL: 27 TOOLS
AWS : 27 SERVICES

PROJECTS: 10 PROJECTS
7 MAJOR
3 MINOR
TIMINGS: 5 TO 6:30

TASKS
ASSIGNMENTS

PLACEMENT: FREE
DOBUTS: 
1. ASK ME ON CLASS
2. ONLINE ONE-TO-ONE (ZOOM) 9 AM TO 7 PM
3. OFFLINE: 9 AM TO 7 PM  
4. TELEGRAM 

EXP: 3 - 4 YEARS 

AUTOMATION: 100%

CODING ?
ANS: NOT MANDATORY BUT ADVANTAGE IF HAVE

PYTHON/BASH/YAML


CERTIFACTIONS: 2


COURSE DETAILS:
CLASS: 100
TIMINGS: 5 TO 6:30 [70 CLASS, 20 INTERACTION]
FEE: 16K (2 INSTALLMENTS) 
REC: 5K 

DEVOPS
AWS
LINUX
SECURITY
AI
FINOPS

================================================================
SDLC : SOFTWATRE DEVELOPMENT LIFE CYCLE

DEPLOYMENT: PROCESS OF INSTALLING APPLICATION TO SERVER.


DEV              OPS
----------------------------------
1. PLAN		5. DEPLOY
2. CODE		6. OPERATE 
3. BUILD	7. MONITOR
4. TEST


=============================================================

TYPES OF ARCH:
1. 1-TIER	: stand alone 		: VLC
2. 2-TIER	: client server		: Bank apps
3. 3-TIER	: web app		: swiggy, netflix
4. N-TIER	: distributed app	: -----------

TIER = LAYER = SERVER

TYPES OF SERVERS

1. WEB SERVER = TO SHOW THE APP = FRONT END CODE = UI/UX DEV
2. APP SERVER = TO USE THE APP = BACKEND END CODE = PROGRAMMER
3. DB SERVER = TO STORE & RETRIVE DATA = DB LANG = MYSQL, SQL

=============================================================

DAY-01: PRACTICAL SESSION

SERVER: WHICH SERVERS SERVICES TO USER.


EC2 INSTANCE = SERVER = VM = INSTANCE



==================================================\
apt install apache2      : TO DOWNLOAD APACHE
systemctl start apache2  : TO START APACHE
systemctl status apache2 : TO CHECK STATUS APACHE
q

cd /var/www/html/	: FOLDER TO PUT FRONTEND CODE
vim index.html
press i 
THIS IS MY SWIGGY APP
esc :wq
80

==============================================================
LINUX:

IN LINUX WE ARE GOING TO WORK ON CLI MODE.

WINDOWS, LINUX, MACOS:
EVERY OS WILL HAVE 2 MODES.

1. GUI	: GRAPHICAL USER INTERFACE
2. CLI	: COMMAND LINE INTERFACE


COMMANDS:
PART-1:

touch file1	: to create a file
ls/ll		: to list the files
mkdir folder1	: to create a folder
cd folder1	: to go inside folder1
clear		: to clear the terminal/screen
vim index.html	: to write the content on index.html file
i		: to go for insert mode
esc :wq		: to save and exit from file
cat index.html	: to see the content of a file

-------------------------------------------------------
PART-2:

cp index.html file1	: copy the file content to other file
mv index.html file3	: to rename the file
rm index.html		: to remove
rm *			: to remove every file
touch file{1..100}	: to create file1 to file100
-------------------------------------------------------
PART-3:

cat -n file1	: to show the file with number of lines

head file1	: to print top 10 lines of a file
head -5  file1	: to print top 5 lines of a file
head -12  file1	: to print top 12 lines of a file

tail file1	: to print bottom 10 lines of a file
tail -5  file1	: to print bottom 5 lines of a file
tail -12  file1	: to print bottom 12 lines of a file

sed -n '5,15p' file1 : to print line 5 to 15 

mv file1 /root	: to move file1 to /root folder
pwd		: to print current working directory

======================================================


BUCKETS:
CRR	: CROSS REGION REPLICATION
BUCKETS WILL BE ON DIFFERENT REGION.

SRR	: SAME REGION REPLICATION
BUCKETS WILL BE ON SAME REGION.

NOTE: IN CLOUD WHEN DATA IS TRANSFERRING FROM ONE REGION TO ANOTHER REGION
WE NEED TO PAY FOR IT.

bucket01 -- > management -- > replication rules


PRE SIGNED URL: TO EXPOSE THE OBJECT FOR SPECIFIC TIME.


STORAGE CLASSES IN S3:
PURPOSE: TO MOVE FILES FROM ONE STORAGE CLASS TO OTHER
ADV: WE CAN SAVE THE AMOUNT 


STATIC WEB HOSTING: (NOT FULLY DONE)
git clone https://github.com/Ironhack-Archive/online-clone-amazon.git
upload the objects to s3 bucket
PERMISSIONS: ------


BATCH OPERATIONS:
USED TO PERFORM SINGLE OPERATION WITH MULTIPLE RESOURCCES.


----------------------------------------------------------------------------------

HTTP : HYPER TEXT TRANSFER PROTOCL
PORT: 80
PURPOSE: TO ACCESS THE APPLICATION IN SEVER

SSH: SECURE SHELL
PORT: 22
PURPOSE: TO CONNECT TO THE SERVER



WHAT IS SERVER ?
SERVERS THE SERVICES TO USER.

HOW TO CREATE SERVER ?
WE HAVE 7 STEPS

1. NAME & TAGS: TO IDENTIFY
2. AMI : 0S, SOFTWARES
3. INSTANCE TYPE : CPU & RAM
4. KEYPAIR : LOGIN
5. NETWORKING: VPC & SG
6. STORAGE : EBS (8 GB * 16 TB)
7. ADVANCE DETAILS

TO CREATE SERVER WHAT WE NEED TO HAVE ?
AWS ACCOUNT


PORTS:
22 : SSH -- > TO LOGIN TO SERVER
80 : HTTP -- > TO SHOW THE APP
443: HTTPS -- > TO SHOW APP SAFELY

PLACEMENT GROUP:
HELPS TO PLACE SERVERS ON OUR REQUIRMENT

WHERE SERVER IS GOING TO PLACE: RACKS

=================================================
GOLDEN AMI

HOW TO CHANGE SECURITY GROUP RULES:

SELECT THE SERVER -- > 
DETAILS -- >
SECURITY
SECURITY GROUPS
EDIT INBOUND RULES
HTTP & ANYWHERE IPV4


=========================================================
10-12-2024:

AMI: AMAZON MACHINE IMAGE
PURPOSE: TO PROVIDE THE OPERATING SYSTEM TO SERVER
WHO: AWS CLOUD WILL GIVE AMI FOR ALL USERS
TYPES: FREE & PAID
HOW MANY AMI'S ARE FREE: 17 (AS OF TODAY)

CREATE A SERVER AND LOGIN TO SERVER
sudo -i

#this command will download webserver(httpd) git(to get code)
yum install httpd git -y

#to start the webserver(httpd)
systemctl start httpd
systemctl status httpd
q

#This is the path where we store frontend code in server
cd /var/www/html/

#This command will download the code from GitHub
git clone https://github.com/karishma1521success/swiggy-clone.git

mv swiggy-clone/* .

CREATING AMI
SELECT SERVER-1
CLICK ON ACTIONS
IMAGE AND TEMPLATES
CREATE IMAGE

IMAGE NAME: SWIGGY-IMAGE
DESCRIPTION: OPTINAL
SAVE

IT WILL TAKE FEW MINS TO MAKE AMI AVAILABLE

SELECT AMI
LAUNCH INSTANCE FROM AMI
CREATE INSTANCE
START SERVICE


LEARNING-1: WHEN WE USE MORE SERVERS
AWS WILL BLOCK THE CPUS
WE NEED TO INCREASE QUOTAS

LEARNING-2: IF HTTPD SERVICE STOPPED APP WILL NOT RUN
systemctl start httpd
chkconfig httpd on  --- > this command make sure our service will stop

=======================================================================

MODIFIYING VOLUME:
SELECT SERVER -- > CLICK ON STORAGE -- > SELECT VOLUME -- > ACTIONS -- > MODIFY

ADDING ANOTHER VOLUME
CREATE A VOLUME
VOLUME -- > CREATE -- > SIZE: ABC -- > AZ: SELECT AS YOUR SERVER AZ -- > TAGS: Name = New server -- > CREATE

ATTACHING VOLUME TO SERVER:
SELECT VOLUME -- > ACTIONS -- > ATTACH -- > SELECT YOUR SERVER -- > /dev/xvdb -- > save

BOOT/ROOT VOL : WHICH HAS OS
NON-ROOT VOL: WHICH WILL NOT HAVE OS


SERVER MIGRATION:
1. CREATE A SERVER ON US-EAST-1A AND DEPLOY APP

2. TAKE A SNAPSHOT OF SERVER
SELECT SERVER -- > STORAGE -- > VOLUME -- > ACTIONS -- > CREATE SNAPSHOT -- > CREATE

3. COPYING THE SNAPSHOT:
SELECT SNAPSHOT -- > ACTIONS -- > COPY SNAPSHOT -- > DESTINATION: AP-SOUTH-1 -- > COPY

4. CREATE EBS FROM SNAPSHOT
SELECT SNAPHSOT -- > ACTIONS -- > CREATE A VOLUME -- > AZ: US-EAST-2B -- > CREATE

5. CREATE A SERVER ON AP-SOUTH-1 & DETACH EXISTING VOLUME
CREATE A SERVER -- > SELECT -- > STOP 
VOLUME -- > SELECT -- > ACTIONS -- > DETACH

6. ATTACH THE VOLUME CREATED FROM SNAPSHOT
VOLUME -- > ACTIONS -- > ATTACH -- > SELECT SERVER -- > DEVICENAME: FIRST OPTION -- > ATTACH


============================================================
STEP-1: CREATE A SERVER-1 AND LAUCH AMAZON APP
STEP-2: CREATE A SERVER-2 AND LAUCH AMAZON APP

WHEN USERS ARE INCREASING LOAD IS INCREASING.
IF I DONT DISTRIBUTE THE LOAD THEN SERVER WILL BE CRASHED.
TO DISTRIBUTE THE LOAD WE NEED TO CREATE LOAD BALANCER.

WHY TO USE LOAD BALANCER IN REAL TIME:
TO DISTRIBUTE THE LOAD/TRAFFIC

TRAFFIC: INCOMMING REQUEST AND OUTGING RESPONSE.


TYPES:
1. APPLICATION LOAD BALANCER (REAL TIME)
2. NETWORK  LOAD BALANCER
3. GATEWAY  LOAD BALANCER
4. CLASSIC  LOAD BALANCER



HTTP: HYPERTEXT TRANSFER PROTOCOL : 80
HTTPS: HYPERTEXT TRANSFER PROTOCOL SECURITY: 443

PRACTICAL PART:
STEP-1: CREATE 2 SERVERS AND LAUCH AMAZON APP
STEP-2: SELECT THE LOAD BALANCER
STEP-3: CREATE LOAD BALANCER -- > APPLICATION LOAD BALANCER
STEP-4: 
Load balancer name: AMAZON
Scheme: Internet-facing
Load balancer IP address type: ipv4
Network mapping: select all Availability Zones
Security groups: select the sg with 80 & 443

STEP-5:
Crete target group 
Target group name: amazon-target-group
next
select 2 servers
include as pending below
create target group

Go back to previous tab and click on refresh
and finally click on create load balancer.

HOW TO AD NEW SERVER TO TARGET GROUP:
TARGET GROUP 
SELECT TARGET GROUP
REGISTER TARGET 


PROVISIONING MEANS CREATING.

TYPES OF ALOGRTHMS:
1. ROUND ROBIN
2. STICKY ROUND ROBIN
3. WEIGHTED
4. IP/URL
5. LEAST CONNECTIONS
6. LEAST TIME

IMP POINTS:
1. LB WORKS ON OSI MODEL.
2. LB DISTRIBUTE TRAFFIC ON ALOGRITHEMS.
3. DEAFULT ALOGORITHM FOR LB IS ROUND ROBIN.
4. TO CREATE A LB ATLEAST WE NEED TO HAVE 2 SERVERS.
5. IN REAL TIME SERVERS WILL AUTOMATIACLLY ADD TO LB.


=========================================================================

==========================================================
ASG = AUTO SCALING GROUP
WHY: TO ADD/REMOVE SERVERS AUTOMATICALLY.
IF WE DELETE A SERVER ASG WILL RECREATE SAME SERVER.

LOAD IS HIGH -- > ADD THE NEW SERVERS
LOAD IS LOW  -- > TO DELETE EXISTING SERVERS

WHEN WE USE AUTO SCALING GROUPS:
IF THE LOAD IS CHANGING FREQUENTLY WE CAN USE ASG.

TYPES OF SCALING:
1. HORIZONTAL SCALING : WE CREATE NEW SERVERS 
2. VERTICAL SCALING   : FOR EXISTING SERVERS WE CAN INCREASE CPU & RAM

WEB & APP : HORIZONTAL SCALING
DB        : VERTICAL SCALING

TRACKING POLICY: USED TO SCALE THE SERVERS
1. CPU 
2. NETWORK IN
3. NETWORK OUT
4. COUNT PER TARGET


TEMPLATE: IT CONSIST OF CONFIGURATION OF A SERVER WHICH IS CREATED BY ASG.
WITHOUT TEMPLATE WE CANT CREATE ALL SERVERS WITH SAME CONFIG.

STEPS TO CREATE ASG:
AUTO SCALING GROUP -- > CREATE
NAME: SWIGGY
CREATE A LAUNCH TEMPLATE
NAME: SWIGGY-TEMPLATE
NOTE: GIVE THE CONFIGURATION JUST LIKE WE GIVE FOR EC2.
CREATE TEMPLATE

AZ AND SUBNETS: US-EAST-1A, US-EAST-1B, US-EAST-1C
LOAD BALANCING: ATTACH TO A NEW LOAD BALANCER
SELECT CREATE A NEW TARGET GROUP
NEXT

DESIRED : 2 -- > HOW MANY SERVERS YOU WANT NOW
MIN: 2 -- > ALWAYS ATLEAST I WANT 2 SERVERS
MAX: 10 -- > IF LOAD IS INCREASE I WANT 10 SERVERS

AUTOMATIC SCALING POLICY:
TARGET TRACKING SCALING POLICY:
CPU -- > VALUE: 50 -- > NEXT

http://internal-swiggy-1-1906495425.ap-south-1.elb.amazonaws.com/

NOTIFICATION -- > CREATE A TOPIC -- > NAME: SWIGGY SERVERS -- > EMAIL: give your email -- > NEXT -- > NEXT -- > CREATE AUTO SCALING GROUP
 

HOW TO INCREASE LOAD:

LOGIN TO SERVER:
amazon-linux-extras install epel -y #(amazon Linux 2)
yum install stress -y #(amazon Linux 2023)
stress
stress --cpu 8 --io 4 --vm 2 --vm-bytes 128M --timeout 500s


NOTE: TO DELETE SERVERS WE NEED TO DELETE ASG.
==============================================================
IAM: IDENTITY & ACCESS MANAGEMENT
WHY TO USE: TO PROVIDE PERMISSIONS AND RESTRICTIONS TO USERS IN AWS.


AUTHENTICATION : PERMISSION TO LOGIN
AUTHORIZATION  : PERMISSION TO WORK


IN REAL TIME WE DONT USE ROOT USER FOR ROUTINE WORKS
WE USE AWS IAM USER IN REAL TIME.


IAM USER: 

IAM -- > USERS -- > CREATE USER -- > NAME: ABC -- > Provide user access to the AWS Management Console -- > I want to create an IAM user -- > Next -- > Attach policies directly -- > AmazonS3FullAccess -- > 


IAM GROUP:
USED TO GIVE PERMISSION FOR MULTIPLE USERS.
A SINGLE USER CAN BE PART OF MAX 10 GROUPS.
GROUP CANNOT BE NESTED.
IF USER HAVE PERMISSION ON GROUP LEVEL AND USER LEVEL BOTH OF THE APPLIES.

IAM -- > USER GROUPS -- > NAME: DEVOPS -- > SELECT USERS -- > Attach policies directly -- > CREATE

IF U FROGOT USERNANE &PASSWORD:
SELECT USER -- > SECURITY CREDNTIALS -- > MANAGE CONSOLE ACCESS


IAM ROLES:
ROLES ARE USED BY SERVICES.
ROLE IS SIMLAR TO IAM USER.
ROLES ALSO HAVING PERMISSION LIKE USERS.
WE CAN SET TIME LIMIT FOR ROLES.
ROLE DOESNT HAVE ANY CREDS.
IF TWO SERVICES WANT TO COMMUNICATE OR WORK TOGETHER WE USE ROLES.
EX: EC2 -- > S3

http://54.242.59.171/

ROLE -- > CREATE ROLE -- > AWS SERVICE -- > EC2 -- > PERMISSIONS: S3 FULL ACCESS -- > NAME: S3 ROLE -- > CREATE

ATTACH ROLE TO SRVER:
SELECT SERVER -- > ACTIONS -- > SECURITY -- > MODIFY IAM ROLE -- > S3 ROLE -- > UPDATE

DEPLOY THE APP FROM SCRIPT
command: aws s3 ls -- > execute this on server
aws s3 cp /var/log/httpd/access_log s3:bucketname

REVOKE SESSION: TO REMOVE THE EXISTING PERMISSION OF A ROLE.

INLINE POLICY: CREATING CUSTOM POLICY FOR RESOURCES

USER -- > ADD PERMISSION -- > CREATE INLINE POLICY -- > SERVICE: S3 -- > All S3 actions (s3:*) -- > bucket: BUCKETNAME -- > next


=================================================================================

ACCESS KEY & SECRET ACCESS KEY = USED TO ASSIGN PERMISSION TO SERVER
BY DEFAULT WE CAN CREATE ONLY 2 KEYS.
WHAT WILL YOU DO WHEN YOU LOST THE KEYS?
ANS: DEACTIVATE THE KEYS
NOTE: PLS DONT DELETE THEM WITHOUT DEACTIVATE


USER -- > KEYS -- > ATTACH TO SERVER -- > SERVER

CREATE A USER -- > SECURITY CREDENTIALS -- > CREATE ACCESS KEY -- > 
Command Line Interface (CLI)-- > create 


aws configure -- > Run this command on server 

AWS Access Key ID [None]: *********************** [pls put your own keys :(]
AWS Secret Access Key [None]: ************************
Default region name [None]: ap-south-1
Default output format [None]: table


CLI -- > COMMAND LINE INTERFACE
WHY TO USE: TO OPERATE/CONTROL AWS SERVICES THROUGH COMMANDS

aws s3 ls 			: to list the buckets
aws s3 ls s3://bucketname	: to list files inside the bucket
aws s3 mb s3://newbucket	: to create a bucket
aws s3 rb s3://newbucket	: to delete a bucket
aws s3 cp file s3://bucket	: to copy file1 to bucket
aws s3 cp folder s3://bucket --recursive : to copy folder to bucket
aws s3 cp copy-s3-uri .		: to copy files from bucket to server
aws s3 sync s3://bucket1 s3://bucket2	: to copy all files from one bucket to another.


IAM:
aws iam list-users		: TO LIST IAM USERS
aws iam list-groups		: TO LIST IAM GROUPS
aws iam list-roles		: TO LIST IAM ROLES

aws iam create-group --group-name devops
aws iam create-user --user-name rahamabc
aws iam create-role --role-name demorole

aws iam delete-group --group-name devops
aws iam delete-user --user-name rahamabc
aws iam delete-role --role-name demorole

EC2:

aws ec2 run-instances --image-id ami-04a37924ffe27da53 --instance-type t2.micro --count 1 
aws ec2 describe-instances   : to show complete info of all the servers
aws ec2 stop-instances --instance-ids i-054a200a4effc917c
aws ec2 start-instances --instance-ids i-054a200a4effc917c
aws ec2 terminate-instances --instance-ids i-054a200a4effc917c

=========================================
PASSWORD POLICY: TO SET OUR OWN POLOCIES FOR PASSWORD

IAM 
DASHBOARD
ACCOUN SETTINGS
PASSWORD POLICY
EDIT

CLOUD TRAIL: IT WILL SHOW SERVICES ACCESSED BY USER
BY DEFAULT IT STORES LAST 90 DAYS OF ACTIVITIES.
$2.00 per 1,00,000 management events delivered.
$0.10 per 100,000 network activity events delivered

BY DEFAULT CLOUD TRAIL WILL RECORD ALL EVENTS
BUT I WANT ON SPECIFI EVENTS (S3, EC2) WE NEED TO CREATE CLOUD TRAIL 

Trail name: my-users-events
Log file SSE-KMS encryption: Disable
NEXT
Events: Management & Data
Resource: s3
Create trail

Q1. HOW DO YOU TRACK USERATVITIES IN AWS ?
A:  CLOUD TRAIL

Q2. ONE OF THE USER DELETED ON SERVER IN AWS ACCOUNT HOW DO YOU FIND THEM ?
A:  CLOUD TRAIL

Q3. BY DEFAULT HOW MANY DAYS EVENTS SHOULD BE STORED ?
A: 90 DAYS

Q4. CAN WE FILER EVENTS SEPERATELY FOR A RESOURCE ?
A: YES 
==============================================================================
=====================================================================================
EFS: ELASTIC FILE SYSTEM
PURPOSE: TO SHARE DATA BLW TWO SERVERS
SIZE: GROW UP TO PETABYTES
TYPE: SERVER LESS
PROTOCOLS: NFSV4.0 & NFSV4.1

INTEGRATIONS: EC2, ECS, EKS, Lambda, Fargate.
TYPES: 1. REGIONAL  2. ONE-ZONE
MODES: 1. GENERAL PURPOSE 2. ELASTIC
PRICING: 5 GB/YEAR FREE
BACKUP: WE CAN GET BACKUPS


EFS -- > CUSTOMIZE -- > NAME: ONE  -- > REGIONAL -- > ELASTIC -- > GIVE SG WITH NFS ENABLE -- > NEXT -- > CREATE.

NOTE: IF NFS IS NOT ENABLE ON SG YOU CANT GET THE DATA.

CREATE 2 SERVERS AND INSTALL & START HTTPD
yum install httpd -y
systemctl start httpd

GO TO EFS -- > SELECT EFS -- > ATTACH -- > COPY PASTE COMMANDS ON BOTH SERVERS

sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-0870d9cc0bbc34037.efs.us-east-1.amazonaws.com:/ /var/www/html/ 

change path to /var/www/html
NOW CREATE SOME FILES IN /var/www/html FOLDER

NOW LETS CHECK WITH CODE
yum install git -y
git clone https://github.com/Ironhack-Archive/online-clone-amazon.git
mv online-clone-amazon/* .
===========================================================================
CLOUD WATCH: TO MONITOR THE CLOUD RESOURCE
NOTE:BY DEFAULT MONITORING FOR EC2 IS ENABLED


STEP-1: CREATE EC2 WIH UBUNTU

STEP-2: CREAT A DASHBOARD
CLOUD WATCH:
CREATE DASHBOARD
NAME: DASHBOARD1
WIDGET: LINE
EC2
EC2: PER INSTANCE METRICS
GIVE ID OF YOUR SERVER
SELECT CPUUTILIZATION
CREATE WIDGET

TO GET MORE LOAD:
sudo -i
apt update
apt install stress -y

GO TO DASHBOARD AND CLICK ON SHARE THE DASHBOARD
GENERATE A LINK AND COPY PASTE



CREATE ALARM
EC2
EC2: PER INSTANCE METRICS
CPUUtilization
CREATE A SNS TOPIC
EC2 ACTION:STOP
 
====================================================================================


DNS: DOMAIN NAME SYSTEM 
PORT: 53
PURPOSE: CONVERTING IP TO HOSTNAME (192.168.1.0 = www.swiggy.com)
DOMAIN PROVIDERS: GODADDY, BIGROCK, HOSTINGER, AWS, --------

TYPES OF RECORDS:
A	: IPV4
AAAA	: IPV6
CNAME	: HOSTNAME --> HOSTNAME
MX	: MAIL SERVER

PRACTICAL PART:
STEP-1: BUY A DOMAIN FROM DOMAIN FROM BELOW SITES (GODADDY,HOSTINGER, BIG ROCK)
STEP-2: CREATE A SERVER IN AWS AND LAUNCH AMAZON APP
STEP-3: GO TO ROUTE 53 -- > HOSTED ZONES -- > CREATE -- > GIVE DOMAIN -- > PUBLIC -- > CREATE
STEP-4: UPDATE NAME SERVER VALUES IN DOMAIN REGISTRAR (BIG ROCK)
STEP-5: CREATE RECORD -- > GIVE IP -- > SAVE


NOW LETS CONVER LOAD BALANCER DNS TO OUR OWN DNS
CREATE ONE MORE SERVER AND ATTACH BOTH SERVERS TO LOAD BALANCER

STEP-6: SELECT RECORD -- > EDIT --> ALIAS --> SELECT APP LB -- > REGION -- > SELECT LB -- > SAVE


ROUTING POLICIES:
SIMPLE
WEIGHTED
LATENCY
FAILOVER
GEO PROXIMITY
GEO LOCATION
MULTI ANSWER VALUE

===============================================================================

CIDR: CLASSLESS INTER DOMAIN ROUTING

VPC 	= WALL

STEP-1: CREATE VPC
VPC -- > CREATE VPC -- > NAME: SWIGGY -- > CIDR: 10.0.0.0/16 -- > CREATE

SUBNET = ROOM

STEP-2: CREATE SUBNET [PUBLIC = HALL]
SUBNET -- > CREATE -- > VPC: SWIGGY -- > NAME: WEB-SUBNET  -- > CIDR: 10.0.0.0/24 -- > CREATE

STEP-3: CREATE A INTERNET GATEWAY
INTERNET GATEWAY -- > CREATE -- > NAME: SWIGGY-IGW -- > CREATE -- > ATTACH TO VPC -- > SWIGGY

STEP-4: CREATE ROUTE TABLE
ROUTE TABLE -- > NAME: SWIGGY-WEB-RTB -- > VPC: SWIGGY -- > CREATE
EDI ROUTES -- > ADD -- > 0.0.0.0/0 -- > IGW : SWIGGY -- > ADD

ASSOCIATE THE WEB SERVER TO THE ROUTE TABLE.
SUBNET ASSOCIATION -- > SELECT WEBSUBNET -- > ASSOCIATE.

CREATE  A WEB SERVER WITH SWIGGY VPC AND WEB SUBNET.
NOTE: SELECT EDIT OPTION IN NETWORK
VPC	: SWIGGY
SUBNET	: SWIGY-WEB-SUBNET

NOTE: ENABLE AUTO ASSIGN PUBLIC IP FOR WEB SERVER WHILE CREATING.
CREATE NEW SG 

STEP-5: CREATE SUBNET
SUBNET -- > CREATE -- > VPC: SWIGGY -- > NAME: APP-SUBNET  -- > CIDR: 10.0.1.0/24 -- > CREATE

STEP-6: CREATE A NAT GATEWAY
NAT GATEWAY -- > CREATE -- > NAME: SWIGGY-NAT -- > SUBNET: WEB SUBNET -- > Elastic IP allocation -- > CREATE

STEP-7: CREATE ROUTE TABLE
ROUTE TABLE -- > NAME: APP-RTB -- > VPC: SWIGGY -- > CREATE
EDI ROUTES -- > ADD -- > 0.0.0.0/0 -- > IGW : SWIGGY -- > ADD
ASSOCIATE THE APP SERVER TO THE ROUTE TABLE.

CREATE  A APP SERVER WITH SWIGGY VPC AND APP SUBNET.
NOTE: SELECT EDIT OPTION IN NETWORK
VPC	: SWIGGY
SUBNET	: SWIGY-APP-SUBNET

NOTE: DISABLE AUTO ASSIGN PUBLIC IP FOR APP SERVER WHILE CREATING.
CREATE NEW SG 

NOW CONNECTING TO APP SERVER:

OPEN WEB SERVER & sudo -i
vim pemfile -- > copy & paste the pem file content -- > chmod 400 pemfile
ssh "pemfile" ec2-user@public-ip 

PEERING: ESTABLISHING CONNECTION BLW 2 VPCS

====================================================RDS
DATABASES
CREATE 
Standard create
ENGINE: MYSQL
VERSION: 8.0.39
TEMPLATE: PRODUCTION
AVALIABILITY: Multi-AZ DB Cluster
DB cluster identifier: database-1
Master username: admin
Password: systemmanager
standard: db.m5d.large
strorage: 400
CONNECTIVITY: Connect to ec2
NOTE: All the config of ec2 will be applicable for RDS


NOTE: SECRET MANAGER IS OUT OF SCOPE BUT WE CAN USE IT FOR OUR CREDS ROTATION.


TO GET THE USERNAME & PASSWORD:
VIEW CREDENTIAL DETAILS 
MANAGE CREDENTIALS
RETRIVE SECRETS

===================================================================
INFRASTRUCTURE: RESOURCES USED TO RUN OUR APP ON CLOUD
EX: S3,EC2, VPC, RDS ----------------

CFT: CLOUD FORMATION TEMPLATE
PURPOSE: TO CREATE RESOURCES FROM CODE
FORMAT: YAML/JSON
IAC: INFRA AS A CODE
WHY TO TO USE IAC : TO AVOID MISTAKES & MANUAL WORK

ADVANTAGES:
1. WRITE CODE FOR ONCE AND USE MULTIPLE TIMES
2. WE CAN SAVE TIME
3. WE CAN AVOID MANUAL WORK
4. WE CAN LIMIT THE MISTAKES

TEMPLATE: IT IS A FILE WHICH CONSIST OF RESOURCE INFROMATION.
STACK: GROUP OF RESOURCE

STEPS FOR PRACTICAL PART:
1. SELECT CFT
2. Build from Infrastructure Composer
3. Drag and drop the ec2 resource and copy code from chatgpt.
4. Stack name
5. CREATE SNS TOPIC FOR SENDING EMAIL.

PROMPT:
generate a cft code for aws iam user in simple way 


Type: AWS::EC2::Instance
Properties:
  ImageId: ami-123456789
  KeyName: kypair0
  SecurityGroups: 
    - mysg
  Tags: 
    - Name: raham
 

=================================================================

INFRASTRUCTURE:
resources used to run our application on cloud.
ex: ec2, s3, elb, vpc, Asg --------------


in general we used to deploy infra on manual 

Manual:
1. time consume
2. Manual work
3. committing mistakes

Automate -- > Terraform -- > code -- > hcl (Hashicorp configuration languge)


WHAT IS TERRAFORM:
its a tool used to make infrastructure automation.
its a free and not open source.
its platform independent.
it comes on the year 2014.
who: Mitchel Hashimoto 
owned: Hashicorp -- > recently IBM is maintaining.
terraform is written on the go language.
We can call terraform as IAC TOOL.
it is Cloud Agonistic (it can used to work with any cloud and even on prem)
it can manage things from onprem also.


HOW IT WORKS:
terraform uses code to automate the infra.
we use HCL : HashiCorp Configuration Language.

IAC: Infrastructure as a code.

Code --- > execute --- > Infra 

ADVANTAGES:
1. Reusable 
2. Time saving
3. Automation
4. Avoiding mistakes
5. Dry run (Dont Repeat Yourself)

CLOUD ALTERNATIVES:
CFT = AWS
ARM = AZURE
GDE = GOOGLE

TERRAFROM = ALL CLOUDS

SOME OTHER ALTERNATIVES:
PULUMI
OpenTofu
ANSIBLE
CHEF
PUPPET



TERRAFORM VS ANSIBLE:
Terraform will create server
and these servers will be configure by ansible.


INSTALLING TERRAFORM:

sudo yum install -y yum-utils shadow-utils
sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
sudo yum -y install terraform

aws configure (or) give a role
check ll .aws/ for the configuration 

MAIN ITEMS IN FILE:
blocks
lables (name, type of resource)
arguments


Configuration files:
it will have resource configuration.
here we write inputs for our resource 
based on that input terraform will create the real world resources.
extension is .tf 

mkdir terraform
cd terraform

vim main.tf 

provider "aws" {
region = "us-east-1"
}

resource "aws_instance" "one" {
ami = "ami-03eb6185d756497f8"
instance_type = "t2.micro"
}

I : INIT
P : PLAN
A : APPLY
D : DESTROY

TERRAFORM COMMANDS:
terraform init	: initialize the provider plugins on backend
it will store information of plugins in .terraform folder
without plugins we cant create resources.
each provider will have its own plugins.
we can get every provider from terraform registry.
once plugins are downloaded we should not need to run init every time.


terraform plan	: to create an execution plan
it will take inputs given by users and plan the resource creation
if we haven't given inputs for few fields it will take default values.

terraform apply : to create resources
as per the given inputs on configuration file it will create the resources in real word.

terrafrom destroy : to delete resources

provider "aws" {
region = "us-east-1"
}

resource "aws_instance" "one" {
count = 5
ami = "ami-03eb6185d756497f8"
instance_type = "t2.micro"
}


terraform apply --auto-approve
terraform destroy --auto-approve


STATE FILE: used to store the resource information which is created by terraform
to track the resource activities
in real time entire resource info is on state file.
we need to keep it safe & Secure
if we lost this file we cant track the infra.
Command:
terraform state list

terraform target: used to destroy the specific resource 
terraform state list
single target: terraform destroy -auto-approve -target="aws_instance.one[3]"
multi targets: terraform destroy -auto-approve -target="aws_instance.one[1]" -target="aws_instance.one[2]"


BENIFITS OF STATE FILE:
1. its the source of truth for our infra.
2. it provides complete info about the infra.
3. it should not modified manually. 
4. Without state file we cant manage the infra.
5. always state file shouldn't match with existing infra.
6. it compares current state with desire state on every run.
7. we can improve the performance.


TERRAFORM FMT: 
used to give allignment and indentation for terraform files.
rewrite configuration files to a canonical format and style.


TERRAFORM VARIABLES:
when the values will change we will use variables.
in real time we keep all the variables in variable.tf to maintain the variables easily.


provider "aws" {
region = "us-east-1"
}

resource "aws_instance" "one" {
count = var.instance_count
ami = "ami-0b41f7055516b991a"
instance_type = var.instance_type
}

variable "instance_type" {
default = "t2.micro"
}

variable "instance_count" {
default = 5
}

terraform apply --auto-approve
terraform destroy --auto-appr

================================================================================

TERRAFORM VARIABLES:
when the values will change we will use variables.
in real time we keep all the variables in variable.tf to maintain the variables easily.



provider "aws" {
region = "us-east-1"
}

resource "aws_instance" "one" {
count = var.instance_count
ami = "ami-0b41f7055516b991a"
instance_type = var.instance_type
}

variable "instance_type" {
default = "t2.micro"
}

variable "instance_count" {
description = "*"
type = number
default = 5
}

terraform apply --auto-approve
terraform destroy --auto-approve

TERRAFORM FMT: 
used to give allignment and indentation for terraform files.
it rewrite configuration files to a canonical format and style.

Terraform tfvars:
When we have multiple configurations for terraform to create resource
we use tfvars to store different configurations.
on execution time pass the tfvars to the command it will apply the values of that file.

NOTE: BY DEFAULT TERRAFORM WILL PICK VALUES FROM TERRAFORM.TFVARS 
if you don't pass any tfvars files
terraform will pick values from terraform.tfvars.


ALTERNATE NAME:
terraform.tfvars.json

cat main.tf.   
provider "aws" {
region = "us-east-1"
}

resource "aws_instance" "one" {
count = var.instance_count
ami = "ami-0e001c9271cf7f3b9"
instance_type = var.instance_type
tags = {
Name = var.instance_name
}
}

cat variable.tf
variable "instance_count" {
}

variable "instance_type" {
}

variable "instance_name" {
}

cat dev.tfvars
instance_count = 1

instance_type = "t2.micro"

instance_name = "dev-server"

cat test.tfvars
instance_count = 2

instance_type = "t2.medium"

instance_name = "test-server"

cat variable.tfvars
instance_count = 3

instance_type = "t2.large"

instance_name = "prod-server"

terraform apply -auto-approve -var-file="dev.tfvars"
terraform apply -auto-approve -var-file="test.tfvars"
terraform apply -auto-approve -var-file="variable.tfvars"

rm -rf *tfvars

TERRAFORM CLI: 

cat main.tf
provider "aws" {
}

resource "aws_instance" "one" {
count = var.var.instance_name
ami = "ami-00b8917ae86a424c9"
instance_type = var.instance_type
tags = {
Name = var.instance_name
}
}

cat variable.tf
variable "instance_count" {
}

variable "instance_type" {
}


variable "instance_name" {
}


METHOD-1:
terraform apply --auto-approve
terraform destroy --auto-approve

METHOD-2:
terraform apply --auto-approve -var="instance_type=t2.micro" 
terraform destroy --auto-approve -var="instance_type=t2.micro"

NOTE: If you want to pass single variable from cli you can use -var or if you want to pass multiple variables from cli create terraform .tfvars files and use -var-file.


TERRAFORM OUTPUTS:
Whenever we create a resource by Terraform if you want to print any output of that resource we can use the output block this block will print the specific output as per our requirement.


provider "aws" {
}

resource "aws_instance" "one" {
ami = "ami-00b8917ae86a424c9"
instance_type = "t2.micro"
tags = {
Name = "raham-server"
}
}

output "raham" {
value = [aws_instance.one.public_ip, aws_instance.one.private_ip, aws_instance.one.public_dns]
}

TO GET COMPLTE OUTPUS:

output "raham" {
value = aws_instance.one
}


Note: when we change output block terraform will execute only that block
remianing blocks will not executed because there are no changes in those blocks.

=========================================================================

TERRAFORM TAINT
It allows for you to manually mark a resource for recreation
in real time some times resources fails to create so to recreate them we use taint
in new version we use -replace option
TO TAINT: terraform taint aws_instance.one[0]
TO UNTAINT: terraform untaint aws_instance.one[0]
terraform state list
terraform taint aws_instance.one[0]
terraform apply --auto-approve 

TERRAFORM REPLACE:
terraform apply --auto-approve -replace="aws_instance.one[0]"


CODE:
statefilebucketfroterraform

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  count         = 2
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = "raham"
  }
}


TERRAFORM IMPORT
when we create resource manually terraform wont track that resource.
Import command can used to import the resource which is created manually.
can only import one resource at a time (FROM CLI)
it can import both code to config file and state file.
FOR STATE FILE: terraform import aws_instance.one (not prefarable)

FIRST CREATE A MANUAL SERVER.

cat main.tf

provider "aws" {
  region = "us-east-1"
}

import {
  to = aws_instance.one
  id = "i-0c23bdc1b7b73d61c"
}

FOR STATEFILE & CODE: 
terraform plan -generate-config-out=ec2.tf
NOTE: DELETE LINE 20 AND 21 FROM ec2.tf
terraform apply -auto-approve

WORKSPACES:
Terraform workspaces allow you to maintain multiple environments 
       (e.g., development, testing, production) 
using the same Terraform configuration but with different states. 
Each workspace has its own state file (terraform.tfstate.d folder)
 resources created/managed in one workspace do not affect other workspaces.
the default workspace in Terraform  is default 
Each workspace is isloated (seperated with each other)


terraform workspace list : to list the workspaces
terraform workspace new dev : to create workspace
terraform workspace show : to show current workspace
terraform workspace select dev : to switch to dev workspace
terraform workspace delete dev : to delete dev workspace

    NOTE:
       1. we need to empty the workspace before delete
       2. we cant delete current workspace, we can switch and delete
       3. we cant delete default workspace

EXECUTION:
NOTE: TAKE CODES FROM TFVARS CONCEPT.

terraform workspace new dev
terraform apply -auto-approve -var-file="dev.tfvars"

terraform workspace new test
terraform apply -auto-approve -var-file="test.tfvars"

terraform workspace new prod
terraform apply -auto-approve -var-file="prod.tfvars"


FOR DELETION:

terraform destroy -auto-approve -var-file="prod.tfvars"
terraform workspace select test
terraform workspace delete prod

terraform destroy -auto-approve -var-file="test.tfvars"
terraform workspace select dev
terraform workspace delete test

terraform destroy -auto-approve -var-file="dev.tfvars"
terraform workspace select default
terraform workspace delete dev



TERRAFORM STATE COMMANDS

The terraform state command is used for advanced state management.
 There are some cases where you may need to modify the Terraform state.
Rather than modify the state directly use these commands.

terraform state list : to list the resources
terraform state show aws_subnet.two : to show specific resource info
terraform state mv aws_subnet.two aws_subnet.three : to rename block
terraform state rm aws_subnet.three : to remove state information of a resource
terraform state pull : to pull state file info from backend


TERRAFORM DEBUGGING
Terraform automates the infrastructure.
issues like misconfigurations or dependency errors can occur. 
Debugging helps understand issues and fix them efficiently.
You can set TF_LOG to one of the log levels TRACE, DEBUG, INFO, WARN or ERROR to change the verbosity of the logs, with TRACE being the most verbose.

export TF_LOG=TRACE 
export TF_LOG_PATH="logs.txt"
terraform apply

========================================================================

1. what is default backend: local
2. does local backend support state locking: no
3. why to lock state file: to prevent from corruption
4. when state file will lock: when two people work on same state file parallely
5. does local backend support state lcoking: no
6. why use s3 & dynamodb: to lock state file automatically.


TERRAFORM STATE FILE
Terraform Stores the infrastructure infromation on state file.
it will automatically refreshed when we run plan, apply & destroy.
In Terraform, a backend is a configuration that determines how and where Terraform stores its state file and how it manages operations like apply, plan, and destroy.
 By default Terraform uses local backend. 
it stores state file in terraform.tfstate in local folder.
it stores information in json format.
if we delete any resource it stores infromation in terraform.tfstate.backup.


TERRAFORM STATE FILE LOCKING
in Real time once we complete our work we need to lock state file.
it ensures that only one operation can be executed at a time. 
once you lock state file you cant modify the infrastructure anymore.
When two people working on state file at a time it will be locked automatically.
unfortunately if two people runs apply at same time unpredictable results, like creating duplicate resources or destroying the wrong infrastructure.
Not all Terraform backends support locking.


Terraform used local backend to manage state file.
But in that local backend only one person can able to access it.
in Real time it’s often necessary to have a centralized, consistent, and secure storage mechanism for the state file. 
Amazon S3 is a popular choice for this, and when combined with DynamoDB for locking, it ensures safe, consistent operations.
TERRAFORM S3 BACKEND

ADVANTAGES
Global Access
Team Collaboration
Secure and Scalable
Centralized State Management
State File Versioning
Disaster Recovery

WHY LOCKING HAPPEND

when 2 developers  work on the same project with same state file then  the locking will be happend.
if state file is locked only first operation execute and second operation waits.
to remove state lock use: terraform force-unlock <LOCK_ID>
after adding dynamodb run:  terraform init -reconfigure

Add that block to existing code and run terraform init -upgrade
dynamodb -- > create table -- > Partition key: LockID -- > create
now after apply state file will go to s3 bucket
dev-1 type destroy and dev-2 type apply now state file locked.
you can check lock-id in new items of table.
once destroy done for dev-1 state file will be unlocked and dev-2 can work.

CODE:

provider "aws" {
  region = "us-east-1"
}

terraform {
  backend "s3" {
    bucket = "mybucket"
    key    = "path/to/my/key"
    region = "us-east-1"
    dynamodb_table = "tablename"
  }
}

resource "aws_instance" "one" {
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = var.instance_name
  }
}



MIGRATING FROM  S3 TO LOCAL BACKEND

if we want state file to back on local use below method.
remove backend code from main.tf 
run terraform init -migrate-state

CODE:

provider "aws" {
  region = "us-east-1"
}


resource "aws_instance" "one" {
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = var.instance_name
  }
}


TERRAFORM REFRESH

This command will be use to refresh the state file.
terraform compares the current state to desired state, it it found any changes 
       on the current state it will update values to state file.
when we run plan, apply or destroy refresh will perform automatically.
if a server is manually created running terraform apply -refresh-only would detect those changes and update the state file to reflect the current state of the resource, but it won't attempt to change the infrastructure to match the Terraform configuration.
if you dont want to refresh while apply & destroy use 
        terraform apply/destroy -refresh=false


TERRAFORM BACKEND BLOCK

By default there is no backend configuration block within Terraform configuration Because Terraform will use it's default backend - local 
This is why we see the terraform.tfstate file in our working directory. 
FOR PARTIAL BACKEND: 
        path = "state_data/terraform.dev.tfstate"
FROM CLI: 
        terraform init -backend-config="path=state_data/terraform.prod.tfstate" -migrate-state
If want we can specify multiple partial backends too.

CODE:

provider "aws" {
  region = "us-east-1"
}

terraform {
  backend "local" {
    path = "/tmp/abc.tfstate"
  }
}


resource "aws_instance" "one" {
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = var.instance_name
  }
}

TERRAFORM SENSITIVE DATA

By default the local state is stored in plain text as JSON. 
There is no additional encryption beyond your hard disk. 
Terraform can store sensitive information in plain text.
Amazon S3 & Terraform cloud for  you can enable encryption

variable "first_name" {
  type = string
   sensitive = true
   default = "Terraform"
}
check state file it will show data.



Treat State as Sensitive Data
Encrypt State Backend
Control Access to State File
BEST PRACTICE

CODE:

provider "aws" {
  region = "us-east-1"
}


resource "aws_instance" "one" {
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = var.instance_name
  }
}

variable "instance_name" {
default = "root"
}

output "abc" {
sensitive = true
value = aws_instance.one.tags_all
}



provider "aws" {
  region = "us-east-1"
}

terraform {
  backend "local" {
    path = "/tmp/abc.tfstate"
  }
}

resource "aws_instance" "abc" {
  count         = 3
  ami           = "ami-01816d07b1128cd2d"
  instance_type = var.instance_type[count.index]
  tags = {
    Name = var.instance_name[count.index]
  }
}

variable "instance_type" {
  default = ["t2.micro", "t2.medium", "t2.large"]
}
variable "instance_name" {
  default = ["yesudas", "dhomini", "riya"]
}

terraform init

=======================================================================

===========================================================

TERRAFORM VALIDATE:
used validates the configuration files in your working directory.
it will show error when we havent given the values for variables.
command: terraform validate

TERRAFORM PLAN:

used to save plan in a file for future reference.
command: terraform plan -out myplan
  to apply    : terraform apply myplan
  to destroy: terraform plan -destroy

PROVIDER BLOCK:

By default provider plugins in terraform change version for every few weeks.
when we run init command, it download latest plugins always.
some code will not work with old plugins, so we need to update them.
To get latest provider plugins : https://registry.terraform.io/browse/providers.
when you add a new provider terraform init is must.
terraform providers: to list the providers which required to run code.
to create infra on any cloud all we need to have is provider.

TYPES:
1. OFFICIAL : MANAGED BY TERRAFORM
2. PARTNER  : MANAGE BY 3RD PATRY COMPANY
3. COMMUNITY: MANAGED BY INDIVIDUALS


CODE:

provider "aws" {
  region = "us-east-1"
}


terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = ">5.70.0"
    }
  }
}


MULTI PROVIDERS:

terraform {
  required_version = ">=1.9.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">5.70.0"
    }
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "4.9.0"
    }
    google = {
      source  = "hashicorp/google"
      version = "6.10.0"
    }
  }
}

TERRAFORM BLOCK:

This block is used to set global configurations and settings for the Terraform.
It usually includes details such as required providers, backend configuration, and version constraints.
terraform -v
terraform -version
terraform --version

TERRAFORM LOCAL BLOCK:

A local block is used to define  values.
if a value is repeating multiple times we can define it here.
This makes our code cleaner and easier to understand.
simply define value once and use for mutiple times.

CODE:

provider "aws" {
  region = "us-east-1"
}

locals {
env = "test"
}

resource "aws_vpc" "one" {
  cidr_block = "10.0.0.0/16"
  tags = {
    Name = "${local.env}-vpc"
  }
}

resource "aws_subnet" "two" {
  vpc_id     = aws_vpc.one.id
  cidr_block = "10.0.1.0/24"

  tags = {
    Name = "${local.env}-subent"
  }
}

resource "aws_instance" "three" {
  subnet_id     = aws_subnet.two.id
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = "${local.env}--server"
  }
}


TERRAFORM COMMENTS:
We use comments to make others code to understand easily.
Terraform supports three different syntaxes for comments.
#   -- > single line comment
//  -- > single line comment
/*   */  -- > multi line comment
Note: if we put comments for code, terraform thinks code is not existed and it will destroy the resource.

TLS PROVIDER:

it provides utilities for working with Transport Layer Security keys & certificates. 
It provides resources that allow private keys, certificates & CSR.
Add tls on your own and try this below code.

CODE:

provider "aws" {
  region = "us-east-1"
}

resource "tls_private_key" "rsa-4096-example" {
  algorithm = "RSA"
  rsa_bits  = 4096
}


resource "local_file" "private_key_pem" {
  content  = tls_private_key.rsa-4096-example.private_key_pem
  filename = "devops.pem"
}


LOCAL EXEC:
executes  command/script on local machine (where terraform is installed)
it will execute the command when resource is created

CODE:

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "three" {
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = "abc-server"
  }
  provisioner "local-exec" {
    command = "echo this is my local server"
  }
}


REMOTE EXEC:

executes  command/script on remote machine.
once the server got created it will execute the commands and scripts for 
installing the softwares and configuring them and deploying app also.


CODE:

provider "aws" {
}

resource "aws_instance" "one" {
  ami                    = "ami-04823729c75214919"
  instance_type          = "t2.micro"
  key_name               = "swikp"
  vpc_security_group_ids = ["sg-05f044979e305302e"]
  tags = {
    Name = " rahaminstance"
  }

  provisioner "remote-exec" {
    inline = [
      "sudo yum install httpd git -y",
      "sudo systemctl start httpd",
      "sudo cd /var/www/html",
      "sudo git clone https://github.com/karishma1521success/swiggy-clone.git",
      "sudo mv swiggy-clone/* .",
      "sudo mv /home/ec2-user/* /var/www/html"
    ]

    connection {
      type        = "ssh"
      user        = "ec2-user"
      private_key = file("~/.ssh/id_rsa")
      host        = self.public_ip
    }
  }
}


FILE PROVISONER: COMMANDS WE CAN EXECUTE THROUG FILE
NOTE: CREATE A DCRIPT ON YOUR LOCAL FOLDER


provider "aws" {
}

resource "aws_instance" "one" {
  ami                    = "ami-01816d07b1128cd2d"
  instance_type          = "t2.micro"
  key_name               = "swikp"
  vpc_security_group_ids = ["sg-0c656c667bc0861e0"]
  user_data              = "${file("apache.sh")}"

  tags = {
    Name = " rahaminstance"
  }
}


===================================================================================================
ALIAS & PROVIDERS:
we can map resource blocks to particular provider blocks.
used to create resources on different regions.

CODE:

provider "aws" {
region = "us-east-1"
}

resource "aws_instance" "three" {
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = "dev-server"
  }
}


provider "aws" {
region = "ap-south-1"
alias = "south"
}

resource "aws_instance" "abcd" {
  provider      = aws.south
  ami           = "ami-08bf489a05e916bbd"
  instance_type = "t2.micro"
  tags = {
    Name = "dev-server"
  }
}

=========================================================================

MODULES:
it divides the code into folder structure.
Modules are group of multiple resources that are used together.
This makes your code easier to read and reusable across your organization.
we can publish modules for others to use.
each module will be having sperate plugins.
modules plugins will be store on .terraform/modules/

TYPES:
Root Module: This is the main  directory where Terraform commands are run. 
All Terraform configurations belong to the root module.

Child Modules: These modules are called by other modules.

yum install tree -y

.
├── main.tf
├── modules
│   ├── my_instance
│   │   └── main.tf
│   ├── s3_module
│   │   └── main.tf
│   └── vpc_module
│       └── main.tf

CODE:

cat main.tf

provider "aws" {
  region = "us-east-1"
}


module "vpc" {
  source = "./modules/vpc_module"
}

module "ec2" {
  source = "./modules/my_instance"
}

module "s3" {
  source = "./modules/s3_module"
}


mkdir -p modules/my_instance/
vim modules/my_instance/main.tf
resource "aws_instance" "one" {
  ami           = "ami-0ddc798b3f1a5117e"
  instance_type = "t2.micro"
  tags = {
    Name = "module-server"
  }
}


modules/s3_module/
vim modules/s3_module/main.tf
resource "aws_s3_bucket" "example" {
  bucket = "rahamdemo-tf-test-bucket"
}

cat modules/vpc_module/main.tf
resource "aws_vpc" "main" {
  cidr_block = "10.0.0.0/16"
  tags = {
    Name = "module-vpc"
  }
}

MODULE INPUT & OUTPUTS:
We can add Input and Output Blocks for Terraform Modules 
Root module can refer both variables & values of child modules.
Child modules cant refer variables, but it can refer its values.


├── main.tf
├── modules
│   └── myec2
│       ├── main.tf
│       ├── output.tf
│       └── variable.tf


cat main.tf
provider "aws" {
region = "us-east-1"
}

module "server" {
source = "./modules/myec2"
}

output "module_output" {
value = module.server.public_ip
}


cat modules/myec2/main.tf
resource "aws_instance" "one" {
  ami           = var.ami
  instance_type = var.instance_type
  tags = {
    Name = var.instance_name
  }
}

cat modules/myec2/variable.tf
variable "ami" {
  default = "ami-0ddc798b3f1a5117e"
}

variable "instance_type" {
  default = "t2.micro"
}

variable "instance_name" {
  default = "module-server"
}

cat modules/myec2/output.tf
output "public_ip" {
  value = aws_instance.one.public_ip
}


PUBLIC MODULES:

The module must be on GitHub and must be a public repo.
NAMING FORMAT: terraform-<provider>-<name> (terraform-aws-ec2-instance)
must have a description.
 module structure will be main.tf, variables.tf, outputs.tf.
x.y.z tags for releases.


TERRAFORM GRAPH:
it shows the relationships between objects in a Terraform configuration.
using the DOT language.
once create infra run the command: terraform graph
Go to Google -- > Graphwiz online & copy paste the code

CODE:

provider "aws" {
region = "us-east-1"
}

resource "aws_vpc" "one" {
  cidr_block = "10.0.0.0/16"
  tags = {
    Name = "dev-vpc"
  }
}

resource "aws_subnet" "two" {
  vpc_id     = aws_vpc.one.id
  cidr_block = "10.0.1.0/24"

  tags = {
    Name = "dev-subent"
  }
}

resource "aws_instance" "three" {
  subnet_id     = aws_subnet.two.id
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = "dev-server"
  }
}

NAME: Graphwiz online

==============================================================================

HCP CLOUD:

HCP means HashiCorp Cloud Platform  
it is a managed platform to automate cloud infrastructure.
it provide privacy, security and isloation.
it supports multiple providers like AWS, Azure, and Google Cloud. 
it offers a suite of open-source tools for managing infrastructure, including Terraform, Vault, Consul, and Nomad. 
We can use Different Code Repos for a Project.
We can use Variable sets to apply same variables to all the workspaces.

ACCOUNT CREATION:

ACCONT-1: GITHUB -- > TO STORE CODE
CREATE A GITHUB ACCOUNT (https://github.com/ -- > singup -- > username,password,email)
create repo -- > name -- > create -- > add new file -- > write terraform code -- >  commit


ACCONT-2: HCP

Go to google & type : HCP CLOUD ACCOUNT SIGNIN
COTINUE WITH GITHUB
AUHORIZE
CLICK CHECK BOXES -- > CONTINUE

CREATE AN ORGINIZATION
CLICK ON TERRAFORM
CONTINUE TO HCP
CONTINUE WITH HCP ACCOUNT

CREATE ORG : NAME: SWIGGY

CREATE YOUR WORKSPACE
Version Control Workflow
INTEGRATE YOUR VCS  -- > GITHUB -- > SELECET REPO -- > NEXT -- > CONTINUE
COBFIGURE VARAIBLES -- > ADD VARIABLE

AWS_ACCESS_KEY_ID : VALUE: MARK AS ENV VARS -- >  SENSITIVE -- > SAVE 
AWS_SECRET_ACCESS_KEY: MARK AS ENV VARS -- > SENSITIVE -- > SAVE 
NOTE: MARK THEM AS ENV VARIBALE AND MAKE SURE NO SPACES ARE GIVEN

RUNS -- > NEW RUN -- > START  -- > confirm and apply
IT WILL AUTOMATICALLY PLAN & WE NEED TO APPLY BY MANUAL
SECOND TIME WHEN WE CHANGE CODE IT WILL AUTOMATICALLY PLAN  
PLAN & APPLY
DESTROY

Cost Estimation policy:
Cost Estimation policy: we can estimate the cost of infra before we create it.
by default this feature is disable, we need to enable 
click on terraform logo --  > orginization -- > settings -- > Cost Estimation -- > enable


SENTINEL POLICY: ITS A POLICY AS A CODE.
BY THIS SENTINEL POLICY WE CAN WRITE OUR OWN CONDITIONS.
IT WILL CHECK THE CONDITION OF RESOURCE BEFORE IT CREATED.
IF CONDITION IS SATISFIED IT WILL CREATE RESOURE, OTHERWISE IT WONT.
EX: TAGS, VERIFIED AMIS, SG --
click on terraform logo --  > orginization -- > settings -- > Policy -- > name: Hard


POLICY:
  "policy": "policy \"require-env-prod-tag\" {\n    description = \"Ensure EC2 instances in HCP Cloud have the tag Env=prod\"\n    \n    enforcement_level = \"hard-mandatory\"\n    \n    rule \"ec2_tag_env_prod\" {\n      \n      condition = all tfplan.resource_changes as _, rc {\n        rc.type == \"hcp_aws_instance\" && \n        rc.change.after.tags.Env == \"prod\"\n      }\n      \n      enforcement = \"hard-mandatory\"\n    }\n  }"


PLICTY SET -- > CONNECT TO POLICY SET -- > NAME -- > SELECT POLICY -- > SAVE

TERRAFORM LOGO -- > ORG -- > SETTINGS -- > POLICY 
TERRAFORM CLOUD FEATURES:
1. Workspaces
2. Projects
3. Runs
4. Variables and Variable Sets
5. Policies and Policy Sets
6. Run Tasks
7. Single Sign-On (SSO)
8. Remote State
9. Private Registry
10. Agents
11. Role-Based Access Control
12. Version Control Integration
13. Observability

TERRAFORM CLOUD ENTERPISE FEATURES:
Private Module Registry: Includes a private registry for sharing modules securely across teams.
Policy as Code: Integrates Sentinel for enforcing policies during provisioning.
Enhanced Automation: Offers advanced run triggers, notifications, and support for custom workflows.

Multi-Organization Support: Allows multiple teams or departments to manage their own infrastructure within a single account.
Advanced Collaboration: Provides role-based access controls and team management features, allowing for fine-grained permissions.
Enhanced Security: Features like SSO (Single Sign-On), audit logs, and compliance tools.

TO DELETE RESOURCES:
SETTINGS -- > DESTRUCTION AND DELETION -- > DELETE QUEUE

===================================================================

META ARGUMENTS:

PARALLELISM: by default terraform follows parallelism.
it will execute all the resources at a time.
by default parallelism limit is 10.
terraform apply -auto-approve -parallelism=1
NOTE: IT WILL APPLICALBLE FOR BOTH APPLY & DESTROY.


DEPENDS_ON: one resource creation will be depending on other.
this is called explicit dependency.

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  ami           = "ami-046d18c147c36bef1"
  instance_type = "t2.micro"
  tags = {
    Name = "raham-server"
  }
}

resource "aws_s3_bucket" "two" {
  bucket     = "terraformbucketabcd123"
  depends_on = [aws_instance.one]
}

2. COUNT: used to create similar objects.

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  count         = 3
  ami           = "ami-046d18c147c36bef1"
  instance_type = "t2.micro"
  tags = {
    Name = "dev-server-${count.index+1}"
  }
}

NOTE: it wont create resources with different configs.

3. FOR_EACH: it is a loop used to create resources.
we can pass different configuration to same code.
it will create resource with less code.

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  for_each      = toset(["dev-server", "test-server", "prod-server"])
  ami           = "ami-046d18c147c36bef1"
  instance_type = "t2.micro"
  tags = {
    Name = "${each.key}"
  }
}



4. LIFECYCLE: 

PREVENT DESTROY: used to prevent the resource to not be deleted.
it is bool.

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  ami           = "ami-046d18c147c36bef1"
  instance_type = "t2.micro"
  tags = {
    Name = "raham-server"
  }
  lifecycle {
    prevent_destroy = true
  }
}

NOTE: CHECK WITH TERRAFORM APPLY

IGNORE CHANGES:
when user modify anything on current state manually changes will be ignored.

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  ami           = "ami-046d18c147c36bef1"
  instance_type = "t2.micro"
  tags = {
    Name = "raham-server"
  }
  lifecycle {
    ignore_changes = all
  }
}

NOTE: MODIFY NAME MANUALLY AND CHECK WITH TERRAFORM APPLY

CREATE BEFORE DESTROY:
by default when we modify some properties terraform will first destroy old server and create new server later.

if we follow create before destroy lifecycle the new resource will create first
and later old resource is going to be destroyed.

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  ami           = "ami-0208b77a23d891325"
  instance_type = "t2.micro"
  tags = {
    Name = "raham-server"
  }
  lifecycle {
    create_before_destroy = true
  }
}


NOTE: CHNAGE AMI ID AND GIVE APPLY

SET: to eliminate the duplicate values

provider "aws" {
region = "us-east-1"
}


resource "aws_instance" "example" {
  for_each      = var.instance_type
  ami           = "ami-063d43db0594b521b"
  instance_type = each.value
}

variable "instance_type" {
  type        = set(string)
  default     = ["t2.micro", "t2.medium", "t2.large"]
}


DYNAMIC BLOCK:
it is used to reduce the length of code and used for reusabilty of code in loop.

provider "aws" {
  region = "us-east-1"
}

locals {
  ingress_rules = [{port = 443}, {port = 80}, {port = 22}, {port = 8080}]
}


resource "aws_instance" "ec2_example" {
  ami                    = "ami-0c02fb55956c7d316"
  instance_type          = "t2.micro"
  vpc_security_group_ids = [aws_security_group.main.id]
  tags = {
    Name = "Terraform EC2"
  }
}


resource "aws_security_group" "main" {
  egress = [
    {
      cidr_blocks      = ["0.0.0.0/0"]
      description      = "*"
      from_port        = 0
      ipv6_cidr_blocks = []
      prefix_list_ids  = []
      protocol         = "-1"
      security_groups  = []
      self             = false
      to_port          = 0
  }]

  dynamic "ingress" {
    for_each = local.ingress_rules

    content {
      description = "*"
      from_port   = ingress.value.port
      to_port     = ingress.value.port
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }
  }

  tags = {
    Name = "terra sg"
  }
}


STATE FILE VIEW FROM HCP TO LOCAL:
REMOTE BACKEND FROM HPC:
we can manage the logs/runs of terraform from HCP to Local.

terraform login
yes
copy the link and paste on browser.
generate token and give to terminal
add backend code and give commands 
see the output from HCP CLOUD.

provider "aws" {
  region = "us-east-1"
}

terraform {
  backend "remote" {
    hostname = "app.terraform.io"
    organization = "swiggy0099"

    workspaces {
      name = "terraform"
    }
  }
}

resource "aws_instance" "one" {
  ami           = "ami-0208b77a23d891325"
  instance_type = "t2.micro"
  tags = {
    Name = "raham-server"
  }
}


DATA BLOCK: GET THE DATA FROM EXISTING RESOURCES 
WE CAN TAKE DATA FROM ONE WORKSPACE TO ANOTHER WORKSPACE.

LIST: TO PAS MULTIPLE VALUES FOR VARIABLES

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  count         = length(var.instance_type)
  ami           = "ami-012967cc5a8c9f891"
  instance_type = var.instance_type[count.index]
}

variable "instance_type" {
type = list(string)
default = ["t2.micro", "t2.medium", "t2.large"]
}



MAP: 
used to pass both key and value to variables.
use mostly used to assign tags for resources.
KEY-VALUE can also called as object or Dictionary.


provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  ami           = "ami-012967cc5a8c9f891"
  instance_type = "t2.micro"
  tags          = var.tags
}

variable "tags" {
  type = map(string)
  default = {
    Name   = "abc-server"
    env    = "test"
    client = "swiggy"
  }
}

========================================================================================

OPENTOFU: 
its a Forked version of terraform.
it comes into market on 2023.

TERRAFORM is FREE & NOT-OPEN SOURCE & OPENTOFU is FREE & OPEN SOURCE.
TERRAFORM IS MAINTAINED BY IBM BUT OPEN TOFU MAINTED BY COMMUNITY.
TERRAFORM HAS LOT OF PROVIDERS & PLUGINS, BUT OPEN TOFU HAVING LIMTED.
TERRAFORM HAS MORE FEATURES, OPENTOFU HAS LESS FEATURES. 
TERRAFORM HAS HCP, BUT OPENTOFU DONT HAVE ANY CLOUD.


wget https://github.com/opentofu/opentofu/releases/download/v1.9.0-rc2/tofu_1.9.0-rc2_linux_amd64.zip
unzip tofu_1.9.0-rc2_linux_amd64.zip 
mv tofu /usr/local/bin/tofu 
tofu version

AFTER INSTALLNG OPENTOFU RUN THE FOLLOWING COMMAND:

vim /root/.bashrc
export PATH=$PATH:/usr/local/bin/
:wq
source /root/.bashrc


resource "aws_instance" "one" {
  ami           = "ami-0208b77a23d891325"
  instance_type = "t2.micro"
  tags = {
    Name = "raham-server"
  }
}


tofu init
tofu plan
tofu apply
tofu destroy

=============================
SET: to eliminate the duplicate values

provider "aws" {
region = "us-east-1"
}


resource "aws_instance" "example" {
  for_each      = var.instance_type
  ami           = "ami-063d43db0594b521b"
  instance_type = each.value
}

variable "instance_type" {
  type        = set(string)
  default     = ["t2.micro", "t2.medium", "t2.large"]
}

DYNAMIC BLOCK:
it is used to reduce the length of code and used for reusabilty of code in loop.

provider "aws" {
  region = "us-east-1"
}

locals {
  ingress_rules = [{port = 443}, {port = 80}, {port = 22}, {port = 8080}]
}


resource "aws_instance" "ec2_example" {
  ami                    = "ami-0c02fb55956c7d316"
  instance_type          = "t2.micro"
  vpc_security_group_ids = [aws_security_group.main.id]
  tags = {
    Name = "Terraform EC2"
  }
}


resource "aws_security_group" "main" {
  egress = [
    {
      cidr_blocks      = ["0.0.0.0/0"]
      description      = "*"
      from_port        = 0
      ipv6_cidr_blocks = []
      prefix_list_ids  = []
      protocol         = "-1"
      security_groups  = []
      self             = false
      to_port          = 0
  }]

  dynamic "ingress" {
    for_each = local.ingress_rules

    content {
      description = "*"
      from_port   = ingress.value.port
      to_port     = ingress.value.port
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }
  }

  tags = {
    Name = "terra sg"
  }
}


STATE FILE VIEW FROM HCP TO LOCAL:
REMOTE BACKEND FROM HPC:
we can manage the logs/runs of terraform from HCP to Local.

terraform login
yes
copy the link and paste on browser.
generate token and give to terminal
add backend code and give commands 
see the output from HCP CLOUD.

provider "aws" {
  region = "us-east-1"
}

terraform {
  backend "remote" {
    hostname = "app.terraform.io"
    organization = "swiggy0099"

    workspaces {
      name = "terraform"
    }
  }
}

resource "aws_instance" "one" {
  ami           = "ami-0208b77a23d891325"
  instance_type = "t2.micro"
  tags = {
    Name = "raham-server"
  }
}


DATA BLOCK: GET THE DATA FROM EXISTING RESOURCES 
WE CAN TAKE DATA FROM ONE WORKSPACE TO ANOTHER WORKSPACE.

MAP: 
used to pass both key and value to variables.
use mostly used to assign tags for resources.
KEY-VALUE can also called as object or Dictionary.


provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  ami           = "ami-012967cc5a8c9f891"
  instance_type = "t2.micro"
  tags          = var.tags
}

variable "tags" {
  type = map(string)
  default = {
    Name   = "abc-server"
    env    = "test"
    client = "swiggy"
  }
}

=============================================================

NODE EXPORTER -- > PROMETHEUS -- > GRAFANA -- > WE CAN SEE DATA FROM DASHBOARDS

NAME		: PROMETHEUS
TYPE		: FREE & OPEN SOURCE
MAJORLY		: CLOUD NATIVE APPLICATIONS
PURPOSE		: TO MONITOR SERVERS
WHAT IT MONITORS: METRICS (CPU, RAM, DISK ----)
STORE DATA	: TSDB [TIME SERIES DATA BASE]
INTEGRATE	: TO ALEART MANAGER LIKE EMAIL, MOBILE, SLACK --
PORT		: 9090
TO SHOW DATA	: PROMQL -- > PROMETHEUS QUERY LANGUAGE

TOOL		: NODE EXPORTER
TYPE		: ITS A DATA SOURCE
PURPOSE		: TO SEND METRIC FROM WORKER NODE TO PROMETHEUS
PORT		: 9100


TOOL		: GRAFANA
TYPE		: FREE & OPEN SOURCE
PURPOSE		: TO CREATE MONITROING DASHBOARD FOR SERVERS
PORT		: 3000
INTEGRATE	: TO ALEART MANAGER LIKE EMAIL, MOBILE, SLACK --
DEFAULT CREDS	: Username:admin & Password: admin

CONNECTION:
SETUP BOTH PROMETHEUS & GRAFAN FROM BELOW LINK
https://github.com/RAHAMSHAIK007/all-setups.git

PROMETHERUS: 9090
NODE EXPORTER: 9100
GRAFANA: 3000

CONNECTING PROMETHEUS TO GARAFANA:
connect to grafana dashboard -- > Data source -- > add -- > promethus -- > url of prometheus -- > save & test -- > top of page -- > explore data -- > if you want run some queries -- > top of page -- > click on + symbol -- > import dashboard -- > 1860 -- > laod --- > prometheus -- > import 

TO GIVE LOAD:
amazon-linux-extras install epel -y
yum install stress -y
stress
stress --cpu 8 --io 4 --vm 2 --vm-bytes 128M --timeout  100s


CONNECTION TO WORKER NODES
1. CREATE 2 WORKRER NODES
2. GO TO MONITORING SERVER [vim /etc/hosts] ADD IP OF YOUR WORKER NODES

public-ip node1  worker-1
public-ip node2  worker-2

3. INSTALL NODE EXPORTER {WITHOUT NODE EXPORTER WE CANT MONITOR WORKER-NODES}

NOTE: AFTER MODIFIYING PROPERTY OF ANY SERVERCIE WE NEED TO RESTART.
systemctl restart prometheus.service

IDS:
1860
10180
14731
11074
8919

PROMETHEUS: METRICS
LOKI& PROMTAIL: LOGS
GRAFANA	: DASHBOARDS

ELASTIC SEARCH	: LOGS, METRICS
KIBANA		: DASHBOARDS
=======================================================================
GIT: GLOBAL INFORMATION TRACKER
VCS: VERSION CONTROL SYSTEM
SCM: SOURCE CODE MANAGEMENT


VCS: TO STORE EACH VERSION OF CODE SEPERATELY.

V1 : 1 SERVICES : 100 LINES -- > REPO-1
V2 : 2 SERVICES : 200 LINES -- > REPO-2
V3 : 3 SERVICES : 300 LINES -- > REPO-3

WHY WE NEED TO STORE THEM SEPERATELY >
TO DO ROLLBACK: GOING BACK TO PREVIOUS VERSION OF APPLICATION
V3  -- > V2 

V1 : INDEX.HTML : 10
V2 : INDEX.HTML : 20
V3 : INDEX.HTML : 30



WHAT IS GIT?

Git is used to track the files.
It will maintain multiple versions of the same file.
It is platform-independent.
It is free and open-source.
They can handle larger projects efficiently.
It is 3rd generation of vcs.
it is written on c programming
it came on the year 2005

CVCS: CENTRALIZED VERSION CONTROL SYSTEM
STORES CODE ON SINGLE REPO
Ex: SVN

DVCS: DISTRIBUTED VERSION CONTROL SYSTEM
STORES CODE ON MULTIPLE REPO
Ex: GIT

GIT DOWNLOADING LINK: (COPY PASTE BELOW LINK IN BROWSER TO DOWNLOAD GIT)
https://github.com/git-for-windows/git/releases/download/v2.48.1.windows.1/Git-2.48.1-64-bit.exe

OPEN GITBAS IN DOWNLOAD FROM CHROME
NEXT --> NEXT -- > UNTILL IT INSTALL

cd Downloads
mkdir netflix



OPEN GITBASH ON YOUR WINDOWS


STAGES/ARCHITECTURE OF GIT:

IN GIT WE HAVE TOTAL 3 STAGES:

1. WORKING DIRECTORY	: WHERE WE WRITE THE CODE
2. STAGING AREA		: WHERE WE TRACK THE CODE
3. REPOSITORY		: WHERE WE STORE THE TRACKED THE CODE


PRACTICAL PART:
CREATE AN EC2 INSTANCE AND CONNECT TO BROWSER

yum install git -y : to install git

yum: package manager
install: action
git: package name 
-y : yes 

git --version
git -v


mkdir Paytm
cd Paytm

git init : used to Initialize the empty repository
with out .git commands will not work.


touch index.html	: to create a file
git status		: to show the file status 
git add index.html	: to track the file
git commit -m "commit-1" index.html	: to commit a file
git log			: to show commits history
git log --oneline	: commits on single line
git log --oneline -2	: to show last 2 commits


create -- > track -- > commit
touch  -- > git add -- > git commit

Note: here every dev works on the local laptop
at the end we want all dev codes to create an application.
so here we use GitHub to combine all dev codes together.

Create a GitHub account and create Repo 


git remote add origin https://github.com/revathisiriki78/paytm.git
git push origin movies
username:
password:


Note: in github passwords are not accepted we need to use token 

profile -- > settings -- > developer settings -- > Personal access token -- > classic -- > 
generate new token -- > classic -- > name: paytm -- > select 6 scopes -- > generate 

git show: to show the file that committed for specific commit id.
git show commit_id

===================================================================
MAVEN:

raw chicken -- > clean  --> marnet -- > ingredients -- > Chicken Biryani
raw code    -- > build   -- > test  -- > artifact -- > Deployment

ARTIFACT: its final product of our code.
developers will give raw code that code we are going to convert into artifact.

TYPES:
1. jar	: Java Archive       : Backend code
2. war	: Web Archive	     : Frontend code + Backend code
3. ear	: Enterprise Archive : jar + war 

JAR FILE:

.java -- > compile -- > .class -- > .jar file

.java	: basic raw
.class	: executable file
.jar	: artifact

all the artifacts are going to created by a build too1.


MAVEN:
Maven is the build tool.
its a free and opensource.
build: process of adding the libs & dependencies to code.
its is also called as Project management too1.
it will manage the complete structure of the project.
the main file in the maven tool is POM.XML

POM.XML: its a file which consist of complete project information.
Ex: name, artifact, tools, libs, dep --------

POM: PROJECT OBJECT MODEL
XML: EXTENSIBLE MARKUP LANGUAGE

WHO GIVE POM.XML : DEVELOPERS
dev will give both code and pom.xml in github

maven is written on java by apache software foundation.
supports: JAVA-1.8.0
year: 2004
home path: .m2


PRATCICAL PART:
1. CREATE AN EC2 INSTANCE AND CONNECT
2. yum install git java-1.8.0-openjdk maven tree -y
3. git clone https://github.com/devopsbyraham/jenkins-java-project.git
4. cd jenkins-java-project


MAVEN LIFECYCLE:
GOALS : a command used to run a task.
Goals refers pom.xml to execute.

NOTE: Without Pom.xml Goals will Not Execute.

PLUGIN: its a small software with makes our work automated.
instead of downloading tools we can download plugins.
this plugins will download automatically when we run goals.


1. mvn compile : used to compile the code (.java [src] -- > .class [target])
2. mvn test    : used to test the code    (.java [test] -- > .class [target])
3. mvn package : used to create artifact 
4. mvn install : used to copy artifact to .m2 (project folder -- > .m2)
5. mvn clean   : to delete the target folder

6. mvn clean package: compile -- > package

mvn clean     : remove old war files
mvn package   : compile -- > test -- > artifact


PROBLEMS WITHOUT MAVEN:
1. we cant create artifacts.
2. We cant create project structure.
3. we cant build and deploy the apps.

ALTERNATIVIES:
MAVEN, ANT, GRADLE, NPM

PROGRAMMING VS BUILD:

JAVA	: MAVEN
PYTHON	: GRADLE
.NET	: VS CODE
C, C#	: MAKE FILE
node.js	: npm

ALTERNETIAVES: 
ANT, GRADLE for java projects.

MAVEN VS ANT:
1. MAVEN IS BUILD & PROJECT MANAGEMNT, ANT IS ONLY BUILD TOOL
2. MAVEN HAS POM.XML, ANT HAS BUILD.XML
3. MAVEN HAS A LIFECYCLE, ANT WILL NOT HAVE LIFECYCLE
4. MAVEN PLUGINS ARE REUSABLE, ANT SCRIPTS ARE NOT RESUEABLE.
5. MAVEN IS DECLARATIVE, ANT IS PROCEDURAL.

SYNOPSIS:
maven is a build tool -- > artifacts -- > java & python -- > mvn clean package
remove old artifact and compile, test and create artifact to deploy the app

===================================================================

JENINS IS A CI/CD TOOL.
REALITY: JENKINS IS ONLY FOR CI.

CI : CONTINOUS INTEGRATION : CONTINOUS BUILD + CONTINOUS TEST (OLD CODE WITH NEW CODE)

DAY-1: 100 LINES : BUILD + TEST
DAY-2: 200 LINES : BUILD + TEST
DAY-3: 300 LINES : BUILD + TEST

BEFORE CI:
MANUAL PROCESS
TIME WASTE

AFTER CI:
AUTOMATED PROCESS
TIME SAVING

CD: CONTINOUS DELIVERY/DEPLOYMENT

ENV:

PRE-PROD/NON-PROD:
DEV	: developers
QA	: testers
UAT	: clients

LIVE/PROD ENV:
PROD	: users

CONTINOUS DELIVERY: Deploying the application to production in manual.
CONTINOUS DEPLOYMENT: Deploying the application to production in automatic.


PIPELINE: 

WAKEUP -- > DAILY ACTIVITIES -- > BREAKFAST -- > LUNCH -- > CLASS
CODE -- > COMPILE -- > TEST -- > ARTIFACT -- > DEPLOY

SETP BY STEP EXECUTION OF A PROCESS.
SERIES OF EVENTS INTERLINKED WITH EACHOTHER.

-------------------------------------------
JENKINS: 
ITS A FREE AND OPEN-SOURCE TOOL.
IT IS PLATFORM INDEPENDENT.
JENKINS WRITTEN ON JAVA.
IT CONSIST OF PLUGINS.
WE HAVE COMMUNITY SUPPORT.
IT CAN AUTOMATE ENTIRE SDLC. 
IT IS OWNED BY SUN MICRO SYSTEM AS HUDSON.
HUDSON IS PAID VERSION.
LATER ORACLE BROUGHT HUDSON AND MAKE IT FREE.
LATER HUDSON WAS RENAMED AS JENKINS.
INVENTOR: Kohsuke Kawaguchi
PORT NUMBER: 8080
JAVA: JAVA-17
DEFAULT PATH: /var/lib/jenkins

ALTERNATIVES:
BAMBOO, GO CI, CIRCLE CI, TARVIS, SEMAPHORE, BUDDY BUILD MASTER, GITLAB, HARNESS
ARGOCD -----

CLOUD: AWS CODEPIPELINE, AZURE PIPLEINE ---------------------


SETUP: Create an EC2 and Include all traffic/8080 in sg

#STEP-1: INSTALLING GIT JAVA-1.8.0 MAVEN 
yum install git java-1.8.0-openjdk maven -y

#STEP-2: GETTING THE REPO (jenkins.io --> download -- > redhat)
sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key

#STEP-3: DOWNLOAD JAVA17 AND JENKINS
yum install java-17-amazon-corretto -y
yum install jenkins -y
update-alternatives --config java

#STEP-4: START JENKINS (when we download service it will on stopped state)
systemctl start jenkins.service
systemctl status jenkins.service

CONNECT:
copy-public-ip:8080 (browser)
cat /var/lib/jenkins/secrets/initialAdminPassword (server)
paster password on browser -- > installing plugins --- > user details -- > start


JOB: it is used to perform task.
to do any work or task in jenkins we need to create a job.

to run commands we need to select execute shell on build steps.

build now: to run job
workspace: place where our job outputs will store

CREATING A JOB:
NEW ITEM -- > NAME: ci-job -- > FREESTYLE -- > OK -- > SCM -- > GIT -- > REPOURL: https://github.com/devopsbyraham/jenkins-java-project.git -- >Build Steps -- > ADD Build Steps -- > Execute shell -- > mvn clean package -- > save -- > build now

WORKSPACE: where your job output is going to be stored
Default: /var/lib/jenkins/workspace

CUSTOM WORKSPACE: 
GO TO JOB -- > CONFIGURE -- > Advance Options -- > Use custom workspace -- > /root/raham

cd 
mkdir raham
chown jenkins:jenkins /root
chown jenkins:jenkins /root/raham

Now we can see output on root folder after building.


CUSTOM PORT NUMBER:
FOR EVERY SERVICE WE HAVE A CONFIGURATION.
TO CHANGE ANY PROPERTY OF A SERVICE WE NEED TO MODIFY THE CONFIG FILE.

find / -name jenkins.service

vim /usr/lib/systemd/system/jenkins.service
LINE 72: 8080=8090
systemctl daemon-reload
systemctl restart Jenkins

BY DEFAULT JENKINS WILL EXECUTE BUILDS ON SEQUENTIALLY
IF WE WANT TO EXECUTE BUILDS PARALLELY 

GO TO JOB -- > CONFIGURE -- > Execute concurrent builds if necessary -- > SAVE

NOW CLICK ON BUILD NOW 2 TIMES U CAN SEE 2 JOBS RUNNING PARALLELY.

TO INCREASE BUILD EXECUTORS -- > CLICK ON BUILD EXECUTOR STATUS -- > BUILT IN NODE -- > CONFIGURE -- > 2 = 3 -- > SAVE

NOW U CAN MAKE 3 BUILDS AT A TIME

NOT: IN REAL TIME WE PREFER TO EXECUTE ONE BUILD AT A TIME.


SETTING CI SERVER USING SCRIPT:

CREATE A SERVER
sudo -i 

vim jenkins.sh

#STEP-1: INSTALLING GIT JAVA-1.8.0 MAVEN 
yum install git java-1.8.0-openjdk maven -y

#STEP-2: GETTING THE REPO (jenkins.io --> download -- > redhat)
sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key

#STEP-3: DOWNLOAD JAVA11 AND JENKINS
amazon-linux-extras install java-openjdk11 -y
yum install jenkins -y
update-alternatives --config java

#STEP-4: RESTARTING JENKINS (when we download service it will on stopped state)
systemctl start jenkins.service
systemctl status jenkins.service

:wq

To run script: sh jenkins.sh

To execute commands on jenkins use execute shell under build steps.

VARIABLES:
it is used to store values that are going to change frequently.
ex: date, season -----



TYPES OF VARIABLES IN JENKINS:
1. USER DEFINED
2. JENKINS ENV

1. USER DEFINED VARIABLES: these are defined by user

a. Local Variable: Variable will work inside of job.
will be working for only single job.

NEW ITEM -- > NAME: ABC -- > FREESTYLE -- > OK -- > BUILD -- >EXECUTE SHELL

name=raham
echo "hai all my name is $name, $name is from hyderabad, $name is teaching devops"

WITH PARAMETERS:
take branch as variable 

passing parameters:
configure -- > this project is parameterized -- > choice -- > name: branch values: main, master --
build with parameters.

b. Global Variable: Variable will work outside of job.
will be working for multiple job.


Dashboard -- > Manage Jenkins -- > System -- > Global properties  -- > Environment variables -- > add : Name: name value: raham -- > save 

NOTE: while defining variables spaces will not be given.
local variables will be high priority.

Limitation: some values cant be defined by user because these values will change build by build.
ex: build number, time, name, url -----

2. JENKINS ENV VARIABLES: these are defined by Jenkins itself.
a. these variables can be change from build to build.
b. these variables will be on upper case.
c. these variables can be defined only once.

echo "the build number is $BUILD_NUMBER, the job name is $JOB_NAME"

printenv: gives all env vars of jenkins



find / -name jenkins.service
find command used to find the path of a file.

when we modify any file from system folder we need to reload daemon


ADMIN TASKS:
1. CHANGING PORT NUMBER OF JENKINS:

vim /usr/lib/systemd/system/jenkins.service
line-70: 8080=8090 -- > save and exit
systemctl daemon-reload
systemctl restart jenkins.service

When we change configuration of any service we need to restart.

2. PASSWORDLESS LOGIN

vim /var/lib/jenkins/config.xml
line-10: true=false
systemctl daemon-reload
systemctl restart jenkins.service

now check the jenkins dashboard it wont ask password


BACKUP:
This plugin was created as alternative to existing Backup Plugin that would work periodically.
The main idea was to create a backup plugin that provides an easy way to be extended with new functionality in the future without the need of hacking around the code.

PLUGINS
PERIODIC BACKUP
CONFIG
GIVE S3 BUCKET AND AW CREDS INFO


TOMCAT ADMIN ACTIVITIES:
PASSWORD CHANGING:
vim apache-tomcat-9.0.98/conf/tomcat-users.xml
change password
sh apache-tomcat-9.0.98/bin/shutdown.sh
sh apache-tomcat-9.0.98/bin/startup.sh

PORT CHANGING:
vim apache-tomcat-9.0.98/conf/server.xml
change port (in 71-line)
sh apache-tomcat-9.0.98/bin/shutdown.sh
sh apache-tomcat-9.0.98/bin/startup.sh

LOGS:
tail -f apache-tomcat-9.0.98/logs/logfilename

3. HOW TO RESOLVE THE ISSUE IF JENKINS SERVER CRASHED ?
stop the jenkins server and start it 
systemctl restart jenkins

When we stop server the services will be also stopped
so we want to restart them 

systemctl stop jenkins.service
systemctl restart Jenkins.service


BUILD EXECUTORS & PARALLEL BUILDS:
Jenkins will run the jobs sequentially (one by one)
if i want to run multiple builds at same time we can configure like this

job -- > configure -- > Execute concurrent builds if necessary -- > save -- > build now 2 times
now we can see 2 jobs will be running on same time.

BUILD EXECUTORS: max number of builds we can run
build-executor status -- > Built-In Node -- > Configure -- > 2 - 5 -- >save
now build 5 times


WHEN WE STOP SERVER:
1. IP WILL CHANGE
2. SERVICES WILL BE STOPPED

=========================================================

================================================================
PIPELINE: STEP BY EXECUTION OF A PROCESS
SERIES OF EVENTS INTERLINKED WITH EACH OTHER.

code -- > build -- > test -- > artifact -- > deployment

IF PIPELINE FAILS ON STAGE-1 IT WONT GO FOR STAGE-2.

plugin download:
Dashboard
Manage Jenkins
Plugins
available 
pipeline stage view
install
Restart 


why to use ?
to automate the work.
to have clarity about the stage.

TYPES:
1. DECLARATIVE
2. SCRIPTED

pipeline syntax:
We use Groovy Script for jenkins Pipeline.
it consists of blocks that include stages.
it includes () & {} braces. 


SHORTCUT: PASSS

P	: PIPELINE
A	: AGENT
S	: STAGES
S	: STAGE
S	: STEPS 


SINGLE STAGE: this pipeline will have only one stage.

EX-1:
pipeline {
    agent any 
    
    stages {
        stage('abc') {
            steps {
               sh 'touch file1'
            }
        }
    }
}

EX-2:
pipeline {
    agent any 
    
    stages {
        stage('raham') {
            steps {
                sh 'touch file2'
            }
        }
    }
}

MULTI STAGE: this pipeline will have more than one stage.

pipeline {
    agent any
    
    stages {
        stage ('two') {
            steps {
                sh 'lsblk'
            }
        }
        stage ('three') {
            steps {
                sh 'lscpu'
            }
        }
        stage ('four') {
            steps {
                sh 'lsmem'
            }
        }
    }
}


CI PIPELINE:

CODE + BUILD + TEST + ARTIFACT

pipeline {
    agent any
    
    stages {
        stage ('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage ('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage ('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage ('artifact') {
            steps {
                sh 'mvn package'
            }
        }
    }
}


PIPELINE AS A CODE: Running more than one command/action inside a single stage.
to reduce the length of the code.
to save the time.

pipeline {
    agent any
    
    stages {
        stage ('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
                sh 'mvn compile'
                sh 'mvn test'
                sh 'mvn package'
            }
        }
    }
}

MULTI STAGE PIPELINE AS A CODE: Running more than one command/action in multiple stages.


pipeline {
    agent any
    
    stages {
        stage ('one') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
                sh 'mvn compile'
            }
        }
        stage ('two') {
            steps {
                sh 'mvn test'
                sh 'mvn package'
            }
        }
    }
}

PAAC OVER SINGLE SHELL: Running all the shell commands on a single shell.

pipeline {
    agent any
    
    stages {
        stage ('one') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
                sh '''
                mvn compile
                mvn test
                mvn package
                '''
            }
        }
    }
}


INPUT PARAMETERS: BASED ON USER INPUT THE PIPELINE IS GOING TO EXECUTE.
used to avoid the mistakes.
pipeline will wait until we provide the input.

pipeline {
    agent any
    
    stages {
        stage ('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage ('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage ('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage ('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage ('deploy') {
            input {
                message "is your inputs correct ?"
                ok "yes"
            }
            steps {
                echo "my code is deployed"
            }
        }
    }
}
NOTE: In real time providing manual approval is best practise for Jenkins pipelines.

NOTE: if we have syntax issue none of the stages will execute.

DIFF BLW SCRIPTED VS DECLARATVE

SCRIPTED: 	DECLARATIVE: 
SHORT   	LONG
NO STAGES       IT HAS STAGES
START NODE      START WITH PIPELINE
============================================================================

================================================================

MASTER AND SLAVE:
it is used to distribute the builds.
it reduce the load on jenkins server.
communication blw master and slave is ssh.
Here we need to install agent (java-11).
slave can use any platform.
SLAVE IS your instance.
label = way of assigning work for slave.

TIP-:
yum install git java-1.8.0-openjdk maven -y

SETUP:
#STEP-1 : Create a server and install java-11
sudo yum install java-17-amazon-corretto -y

#STEP-2: SETUP THE SLAVE SERVER
Dashboard -- > Manage Jenkins -- > Nodes  -- > New node -- > nodename: abc -- > permanaent agent -- > save 

CONFIGURATION OF SALVE:

Number of executors : 3 #Number of Parallel builds
Remote root directory : /tmp #The place where our output is stored on slave sever.
Labels : swiggy #place the op in a particular slave
useage: last option
Launch method : last option 
Host: (your privte ip)
Credentials -- > add -- >jenkins -- > Kind : ssh username with privatekey -- > username: ec2-user 
privatekey : pemfile of server -- > save -- > 
Host Key Verification Strategy: last option

DASHBOARD -- > JOB -- > CONFIGURE -- > RESTRTICT WHERE THIS JOB RUN -- > LABEL: SLAVE1 -- > SAVE

BUILD FAILS -- > WHY -- > WE NEED TO INSTALL PACKAGES
yum install git java-1.8.0-openjdk maven -y


pipeline {
    agent {
        label 'two'
    }
    
    stages {
        stage('one') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
                sh '''
                mvn compile
                mvn test
                mvn package
                mvn install
                mvn clean package
                '''
            }
        }
    }
}



POST BUILD ACTIONS:
Actions that perform after build is done.


1. always	: executes always
2. success	: executes when build is success only
3. failure	: executes when build is failed only


pipeline {
    agent any
    
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage('compile') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('Artifacts') {
            steps {
                sh 'mvn clean package'
            }
        }
    }
    post {
        always {
            echo "this build is completed"
        }
    }
}
==========================================================================


NEXUS:
Its an Artifactory storage service.
used to store artifacts on repo. (.JAR, .WAR, .EAR)
Nexus server -- > Repo -- > Artifact
we can use this server to rollback in real time.
it req t2.medium 
nexus uses java-17
PORT: 8081

ALTERTAVIVES: JFROG, S3, -----

SETUP SCIPT:
https://github.com/RAHAMSHAIK007/all-setups.git

STEPS: signin -- > username: admin & password: /app/sonatype-work/nexus3/admin.password -- > next -- > set password -- > disable anonymous access -- > save

CREATING REPO:
settings symbol -- > repositories -- > new -- > maven2(hosted) -- > name -- > save

NOTE: to integrate any tool with Jenkins we need to download the plugin.

NEXUS INTEGRATION TO PIPELINE:
1. Download the plugin (Nexus Artifact Uploader)
Manage Jenkins -- > plugins -- > Available Plugins -- > Nexus Artifact Uploader -- > install.
2. Configure it to pipeline by using pipeline syntax
search for nexus

NOTE: All the information will be available on pom.xml file.

ADDING CREDS:
Dashboard
Manage Jenkins
Credentials
global
Add creds


PIPELINE:

pipeline {
    agent any
    
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage('Artifact upload') {
            steps {
                nexusArtifactUploader artifacts: [[artifactId: 'NETFLIX', classifier: '', file: 'target/NETFLIX-1.2.2.war', type: '.war']], credentialsId: '968c23dd-b648-4f15-91bf-7d76981a1218', groupId: 'in.RAHAM', nexusUrl: '100.25.197.110:8081', nexusVersion: 'nexus3', protocol: 'http', repository: 'netflix', version: '1.2.2'
            }
        }
    }
}


TOMCAT:

WEBSITE: FRONTEND -- > DB IS OPT
WEBAPP: FRONTEND + BACKEND -- > DB IS MANDATORY

WEB APPLICATION SERVER/APPLICATION SERVER/APP SERVER = TOMCAT

ITS A WEB APPLICATION SERVER USED TO DEPLOY JAVA APPLICATIONS.
IT IS WRITTEN ON JAVA LANGUAGE.
AGENT: JAVA
PORT: 8080
WE CAN DEPLOY OUR ARTIFACTS.
ITS FREE AND OPENSOURCE.
Its Platform Independent.
YEAR: 1999 

war : tomcat/webapps
jar : tomcat/lib

ALTERNATIVES: NGINX, IIS, WEBSPHERE, JBOSS, GLASSFISH


SETUP: CREATE A NEW SERVER
INSTALL JAVA: sudo yum install java-17-amazon-corretto -y


STEP-1: DOWNLOAD TOMCAT (dlcdn.apache.org)
wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.98/bin/apache-tomcat-9.0.98.tar.gz

STEP-2: EXTRACT THE FILES
tar -zxvf apache-tomcat-9.0.98.tar.gz

STEP-3: CONFIGURE USER, PASSWORD & ROLES
vim apache-tomcat-9.0.98/conf/tomcat-users.xml

 56   <role rolename="manager-gui"/>
 57   <role rolename="manager-script"/>
 58   <user username="tomcat" password="raham123" roles="manager-gui, manager-script"/>

STEP-4: DELETE LINE 21 AND 22
vim apache-tomcat-9.0.96/webapps/manager/META-INF/context.xml

STEP-5: STARTING TOMCAT
sh apache-tomcat-9.0.96/bin/startup.sh

CONNECTION:
COPY PUBLIC IP:8080 
manager apps -- > username: tomcat & password: raham123

==============================================================================

RBAC:

RBAC: ROLE BASE ACCESS CONTROL.
TO restrict the user PERMISSIONS in jenkins.

bittu	= fresher
raham	= exp 

STEP-1: USER CREATION
manage jenkins -- > users -- > create users -- > suresh: bittu 

STEP-2: PLUGIN DOWNLOADING
Dashboard
Manage Jenkins
Plugins
Available plugin
Role-based Authorization Strategy  

STEP-3: CONFIGURE THE PLUGIN
Dashboard
Manage Jenkins
Security
Authorization 
Role-based  Strategy  
SAVE

STEP-4: MANAGE AND ASSIGN ROLES TO USERS
Manage Jenkins
Security
manage roles -- > add -- > fresher & exp -- > fresher: overall read & exp: admin -- > save
assign roles -- > add user -- > bittu: fresher -- > save


RESTRICTING TO SPECIFIC JOB:
MANAGE JENKINS -- > SECURITY --> AUTHORIZATION -- > PROJECT BASED MATRIX AUTHORIZATION STRATEGY -- > 

ADD USER -- > YESUDAS -- > Read


GO TO JOB -- > Enable project-based security
ADD USER -- > YESUDAS 
JOB: BUILD & READ
SAVE


LINKED JOBS:
ONE JOB IS LINKED WITH ANOTHER JOB
IF WE BUILD JOB-1 AUTOMATICALLY JOB-2 IS GOING TO BUILD.

CREATE 2 JOBS WITH NAME JOB-1 AND JOB-2
JOB-1 --> CONFIGURE -- > POSTBILD ACTIONS -- > BILD OTHER PROJECTS -- > JOB-2 -- > SAVE
BUILD JOB-1 AND JOB-2 WILL BE TRIGGERED

HOW TO ADD PARAMETERS:

PARAMETERS: Used to pass inputs for jobs

CHOICE: to pass single input at a time.
STRING: to pass multiple inputs at a time.
------------
MULTI-LINE STRING: to pass multiple inputs on multiple lines at a time.
FILE: to pass the file as input.
BOOL: to pass input either yes or no.


This project is parameterized
Name: server 
Choices: 
dev
test
save

CUSTOM THEMES:

dashboard -- > manage Jenkins -- > Appearance -- > Customizable theme -- > CSS
https://github.com/alefnode/jenkins-themes

https://cdn.rawgit.com/afonsof/jenkins-material-theme/gh-pages/dist/material-cyan.css

ADD HEADERS OF NARESHIT AND USE LOGO FROM IMAGES.
==============================================================

CREDS ADDING: 
Dashboard
Manage Jenkins
Credentials
System
Global credentials (unrestricted)
Add credentials 
username & pasword

STEPS FOR PROJECT:

STEP-1: WE CREATE THE CLIENT ACCOUNT IN AWS. [aws orginization]

STEP-2: CREATE THE INFRASTRUCTURE FROM HCP.
        Link: https://github.com/devopsbyraham/netflixdemoinfra.git

STEP-3: CONFIGURE THE SEVERS [SCRIPTS] [JENKINS, TOMCAT, NEXUS, MONITORING]
        Link: https://github.com/RAHAMSHAIK007/all-setups.git

SETP-4: WRITE PIPELINE FOR CI/CD.
        A. Configure Slave (as of now im using app server as slave)
        B. Download Plugins [pipeline stage view, nexus, Deploy to container, slack] 
        C. Write Pipeline Code

SETP-5: INTEGRATE TO SLACK

STEP-6: MONITOR THE APPLICATION FROM GRAFANA 

TOOLS:
GIT
GITHUB
MAVEN
JENKINS
NEXUS
TOMCAT
HCP
TERRAFORM
SLACK
PROMETHEUS
GRAFANA

PIPELINE CODE:

pipeline {
    agent any
    
    stages {
        stage('checkout') {
            steps {
                git branch: '$branch', url: 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage('Nexus') {
            steps {
                nexusArtifactUploader artifacts: [[artifactId: 'NETFLIX', classifier: '', file: 'target/NETFLIX-1.2.2.war', type: '.war']], credentialsId: '35cc5458-5e5a-4992-8e90-394d9a3c9ab1', groupId: 'in.RAHAM', nexusUrl: 'ec2-3-80-145-80.compute-1.amazonaws.com:8081', nexusVersion: 'nexus3', protocol: 'http', repository: 'abcd', version: '1.2.2'
            }
        }
        stage('deploy') {
            steps {
                deploy adapters: [
                    tomcat9(
                        credentialsId: '85e5e800-4740-461a-868d-519b51fa90ed',
                        path: '',
                        url: 'http://ec2-54-234-53-68.compute-1.amazonaws.com:8080/'
                    )
                ],
                contextPath: 'netflix',
                war: 'target/*.war'
            }
        }
    }
    post {
        always {
            echo "slack notify"
            slackSend (
                 channel: '#netflix', message: "*${currentBuild.currentResult}:* Job ${env.JOB_NAME} \n build ${env.BUILD_NUMBER} \n More info at: ${env.BUILD_URL}"
            )
        }
    }
}

PATH FOR PROMTHEUS: vim /etc/prometheus/prometheus.yml





========================================================================================================


Automated: Deployment, Installation [ONE AT A TIME]

To perform end-to-end automation we can use Ansible.
Creating servers
configure servers
deployment application on servers
Not with single, it can deal with multiple server.
ANSIBLE ALTERNATIVES: CHEF, PUPPET, SALTSTACK ----

ANSIBLE:
its a Configuration Management Tool.
Configuration: Hardware and Software 
Management: Pkgs update, installing, remove ----
Ansible is used to manage and work with multiple servers together.
its a free and Opensource.
it is used to automate the entire deployment process on multiple servers.
We install Python on Ansible.
we use a key-value format for the playbooks.

PLAYBOOK:
create servers
install packages & software
deploy apps
---------

Jenkins = pipeline = groovy
ansible = playbooks = yaml
YAML: YET ANOTHER MARKUP LANGUAGE

HISTORY:
in 2012 dev called Maichel Dehaan who developed ansible.
After few years RedHat taken the ansible.
it is platform-independent & will work on all linux flavours.


ARCHITECTURE:
PLAYBOOK: its a file which consist of code
INVENTORY: its a file which consist ip of nodes
SSH: used to connect with nodes
Ansible is Agent less.
Means no need to install any software on worker nodes.

SETUP: 
CREATE 5 SERVERS [1=ANSIBLE, 2=DEV, 2=TEST]

EXECUTE THE BELOW COMMANDS ON ALL SERVERS:
sudo -i
hostnamectl set-hostname ansible/dev-1/dev-2/test-1/test-2
sudo -i

passwd root  -- > to login to other servers
vim /etc/ssh/sshd_config (38 & 61 uncomment both lines) 
systemctl restart sshd
systemctl status sshd
hostname -i

THE BELOW STEPS NEED TO BE RUN ON ANSIBLE SERVER:

amazon-linux-extras install ansible2 -y
yum install python3 python-pip python-dlevel -y (optional)

vim /etc/ansible/hosts
# Ex 1: Ungrouped hosts, specify before any group headers.
[dev]
172.31.20.40
172.31.21.25
[test]
172.31.31.77
172.31.22.114

ssh-keygen -- > enter 4 times 
ssh-copy-id root@private ip of dev-1 -- > yes -- > password -- > ssh private ip -- > ctrl d
ssh-copy-id root@private ip of dev-2 -- > yes -- > password -- > ssh private ip -- > ctrl d
ssh-copy-id root@private ip of test-1 -- > yes -- > password -- > ssh private ip -- > ctrl d
ssh-copy-id root@private ip of test-2 -- > yes -- > password -- > ssh private ip -- > ctrl d

ansible -m ping all : To check worker node connection with ansible server.

1. ADHOC COMMANDS:
these are simple Linux commands. 
these are used for temp works.
these commands will be over ridden.

ansible all -a "yum install git -y"
ansible all -a "yum install maven -y"
ansible all -a "mvn --version"
ansible all -a "touch file1"
ansible all -a "touch raham.txt"
ansible all -a "ls"
ansible all -a "yum install httpd -y"
ansible all -a "systemctl status httpd"
ansible all -a "systemctl start httpd"
ansible all -a "useradd raham"
ansible all -a "cat /etc/passwd"
ansible all -a "yum remove git* maven* httpd* -y"


2. MODULES:
its a key-value pair.
modules are reusable.
we can use different modules for different purposes.
module flag is -m 

ansible all -m yum -a "name=git state=present"
ansible all -m yum -a "name=maven state=present"
ansible all -m yum -a "name=maven state=present"	[present=installed]
ansible all -m service -a "name=httpd state=started"	[started=restart]
ansible all -m service -a "name=httpd state=stopped"	[stopped=stop]
ansible all -m yum -a "name=http state=absent"		[absent=uninstall]
ansible all -m user -a "name=vikram state=present"
ansible all -m user -a "name=vikram state=absent"
ansible all -m copy -a "src=raham.txt dest=/tmp"

3. PLAYBOOKS:
playbooks used to execute multiple modules.
we can reuse the playbook multiple times.
in real time we use a playbook to automate our work.
for deployment, pkg installation, Server Creation ----
here we use key-value pairs.
Key-Value can also be called as Dictionary.
ansible-playbook will be written on YAML syntax.
YAML = YET ANOTHER MARKUP LANGUAGE
extension for playbook is .yml or .yaml
playbook start with --- and end with ... (opt)


EX-1:

- hosts: all
  tasks:
    - name: installing git
      yum: name=git state=present

    - name: installing httpd
      yum: name=httpd state=present

    - name: starting httpd
      service: name=httpd state=started

    - name: create user
      user: name=jayanth state=present

    - name: copy a file
      copy: src=index.html dest=/root

    

TO EXECUTE: ansible-playbook playbok.yml

Gather facts: it will get information of worker nodes
its by default task performed by ansible.

ok=total number of tasks
changed= no.of tasks successfully executed

BY DEFAULT ANSIBLE WILL EXECUTE PLAYBOOK ON SEQUENTIAL.
IF TASK-2 GOT FAILED IT WILL STOP EXECUTING REMAING TASKS.

EX-2:
 hosts: all
  ignore_errors: true
  tasks:
    - name: installing git
      yum: name=git state=absent

    - name: installing httpd
      yum: name=httpd state=absent

    - name: starting httpd
      service: name=httpd state=started

    - name: create users
      user: name=pushpa state=absent

    - name: copying a file
      copy: src=raham.txt dest=/root


TAGS: by default ansible will execute all tasks sequentially in a playbook.
we can use tags to execute a specific tasks or to skip a specific tasks.


EX-1:

- hosts: all
  ignore_errors: yes
  tasks:
    - name: installing git
      yum: name=git state=present
      tags: a

    - name: installing httpd
      yum: name=httpd state=present
      tags: b

    - name: starting httpd
      service: name=httpd state=started
      tags: c

    - name: create a user
      user: name=kohli state=present
      tags: d

    - name: copy a file
      copy: src=index.html dest=/tmp
      tags: e

SINGLE TAG: ansible-playbook raham.yml --tags d
MULTI TAGS: ansible-playbook raham.yml --tags b,c

EX-2:

- hosts: all
  ignore_errors: yes
  tasks:
    - name: uninstalling git
      yum: name=git* state=absent
      tags: a

    - name: uninstalling httpd
      yum: name=httpd state=absent
      tags: b

    - name: starting httpd
      service: name=httpd state=started
      tags: c

    - name: delete a user
      user: name=kohli state=absent
      tags: d

    - name: copy a file
      copy: src=index.html dest=/tmp
      tags: e

SKIP A SINGLE TASK: ansible-playbook raham.yml --skip-tags c
SKIP MULTIPLE TASK: ansible-playbook raham.yml --skip-tags a,c


==========================================================

HANDLERS:
when we have two tasks in a single playbook if task 2 is depending upon task 1 so then we can use the concept called handlers .
once task one is executed successfully it will notify task 2 to perform the operation. 
the name of the notify and the name of the task two must be same.


- hosts: all
  tasks:
    - name: installing httpd
      yum: name=httpd state=present
      notify: starting httpd
  handlers:
    - name: starting httpd
      service: name=httpd state=started

sed -i 's/present/absent/g' raham.yml

- hosts: all
  tasks:
    - name: installing httpd
      yum: name=httpd state=absent
      notify: starting httpd
  handlers:
    - name: starting httpd
      service: name=httpd state=started

SETUP MODULE: used to print the complete info of worker nodes
ansible all -m setup 

ansible all -m setup  | grep -i family
ansible all -m setup  | grep -i pkg
ansible all -m setup  | grep -i cpus
ansible all -m setup  | grep -i mem


CONDITIONS:
CLUSTER: Group of servers
HOMOGENIUS: all servers have having same OS and flavour.
HETROGENIUS: all servers have different OS and flavour.

used to execute this module when we have different Clusters.

RedHat=yum
Ubuntu=apt

- hosts: all
  tasks:
    - name: installing git on RedHat
      yum: name=git state=present
      when: ansible_os_family == "RedHat"

    - name: installing git on Debian
      apt: name=git state=present
      when: ansible_os_family == "Debian"


- hosts: all
  tasks:
    - name: installing httpd
      yum: name=httpd state=present
      when: ansible_nodename == "dev-1"

    - name: installing mysql
      yum: name=mysql state=present
      when: ansible_nodename == "dev-2"

    - name: installing python
      yum: name=python state=present


VALIDATORS:
1. YAMLINT
2. YAML FORMATTER
3. YAML VALIDATOR
4. CHATGPT

SHELL VS COMMAND VS RAW:

- hosts: all
  tasks:
    - name: installing maven
      shell: yum install maven -y

    - name: installing httpd
      command: yum install httpd -y

    - name: installing docker
      raw: yum install docker -y

raw >> command >> shell.

ansible all -a "mvn -v"
ansible all -a "htppd -v"
ansible all -a "docker -v"


L	: LINUX
A	: APACHE
M	: MYSQL
P	: PYTHON


- hosts: all
  tasks:
    - name: installing apache
      yum: name=httpd state=present

    - name: installing mysql
      yum: name=mysql state=present

    - name: installing python
      yum: name=python3 state=present


 ansible all -a "httpd --version"
 ansible all -a "python3 --version"
 ansible all -a "mysql --version"


=============================================


SHORTCUTS FOR LINUX:
vim .bashrc
alias ar='ansible-playbook raham.yml'
alias info='ansible all -m setup'

source .bashrc

VARIABLES:

STATIC VARS: we can define these vars inside the playbook and use for multiple times, once a variable is defined here it will not change untill we change.


- hosts: all
  vars:
    a: maven
    b: httpd
  tasks:
    - name: installing maven
      yum: name={{a}} state=present
    - name: installing httpd
      yum: name={{b}} state=present

TO EXECUTE: ansible-playbook playbbok.yml

DYNAMIC VARS: therse vars will be defined outside the playbook and these will change as per our requirments.

- hosts: all
  vars:
  tasks:
    - name: installing maven
      yum: name={{a}} state=absent
    - name: installing httpd
      yum: name={{b}} state=absent


ansible-playbook raham.yml --extra-vars "a=docker b=httpd"


LOOPS: We can use loops to reduce the length of the code for the playbook

- hosts: all
  tasks:
    - name: installing pkg-1
      yum: name={{item}} state=present
      with_items:
        - git
        - java-1.8.0-openjdk
        - maven
        - docker
        - httpd


ansible all -a "git -v"
ansible all -a "java -v"
ansible all -a "maven -v"
ansible all -a "docker -v"
ansible all -a "httpd -v"


- hosts: all
  tasks:
    - name: installing pkg-1
      yum: name={{item}} state=absent
      with_items:
        - git
        - java-1.8.0-openjdk
        - maven
        - docker
        - httpd

ansible all -a "git -v"
ansible all -a "java -v"
ansible all -a "maven -v"
ansible all -a "docker -v"
ansible all -a "httpd -v"

EX-2:

- hosts: all
  tasks:
    - name: creating users
      user: name={{item}} state=present
      with_items:
        - ravi
        - shiva
        - rajesh
        - shivani
        - luckyy


- hosts: all
  tasks:
    - name: creating users
      user: name={{item}} state=absent
      with_items:
        - ravi
        - shiva
        - rajesh
        - shivani
        - lucky


DEBUG: to print the messages from a playbook.

- hosts: all
  tasks:
    - name: printing a msg
      debug:
        msg: hai all welcome to my session

ansible all -m setup

NAME	: ansible_nodename
FAMILY  : ansible_os_family
PKG	: ansible_pkg_mgr
CPU	: ansible_processor_cores
MEM	: ansible_memtotal_mb
FREE	: ansible_memfree_mb


- hosts: all
  tasks:
    - name: print a msg
      debug:
        msg: "my node name is: {{ansible_nodename}}, the os is: {{ansible_os_family}}, the package manager is: {{ansible_pkg_mgr}}, total cpus is: {{ansible_processor_cores}}, the total ram: {{ansible_memtotal_mb}}, free ram is: {{ansible_memfree_mb}}"


JINJA2 TEMPLATE: used to get the customized op, here its a text file which can extract the variables and these values will change as per time.

INLINE MODULE: Used to change specific line in a files on worker node.


FETCH: Used to fetch data in files from remote servers to local.
Note: Create a file in worker nodes

- hosts: all
  tasks:
    - name: fetching data
      fetch: src=/root/raham.txt dest=/tmp/abc.txt


LOOKUPS: this module used to get data from files, db and key values

- hosts: dev
  vars:
    a: "{{lookup('file', '/root/creds.txt') }}"
  tasks:
    - debug:
        msg: "hai my user name is {{a}}"

cat creds.txt
user=raham

CREATING EC2 FROM PLAYBOOK:

NOTE: If Ansible want to create an Ec2 instance in the cloud it need to have permission 
so to allocate the permission for our Ansible we can create IAM user

IAM -- > create user -- > name: Ansible -- > next -- > Attach policies directly -- > ec2fullAccess -- > next -- > create user 

Ansible -- > Security Credentials -- > Create access key -- > CLI -- > checkbox -- > create access key --> download .csv file

COME TO ANSIBLE SERVER:
aws configure -- > giving ansible user permissions to server

AWS Access Key ID [None]: xxxxxxxxxxxxx
AWS Secret Access Key [None]: xxxxxxxxxxxx
Default region name [None]: us-east-1
Default output format [None]: table

sudo yum install pip -y
sudo pip install boto

- hosts: localhost
  tasks:
    - name: creating ec2 instance
      ec2:
        region: "us-east-1"
        count: 3
        image: "ami-04ff98ccbfa41c9ad"
        instance_type: "t2.micro"
        instance_tags:
          Name: "abc"

STRATAGIES: Way of executing the playbook.

LINEAR: execute tasks sequentially 
if task-1 is executed on server-1 it will wait till task-2 execution
FREE: execute all tasks on all node at same time
if task-1 is executed on server-1 it wont wait till task-2 execution
ROLLING:
BATCH:

====================================================================================

└── roles
    ├── pkgs
    │   └── tasks
    │       └── main.yml
    ├── users
    │   └── tasks
    │       └── main.yml    └── webserver
        └── tasks
            └── main.yml


ROLES:
roles is a way of organizing playbooks in a structured format.
main purpose of roles is to encapsulate the data.
we can reuse the roles multiple times.
length of the playbook is decreased.
it contains on vars, templates, task -----
in real time we use roles for our daily activities.
yum install tree -y

mkdir playbooks
cd playbooks/

mkdir -p roles/pkgs/tasks
vim roles/pkgs/tasks/main.yml

- name: installing pkgs
  yum: name=git state=present
- name: install maven
  yum: name=maven state=present
- name: installing docker
  yum: name=docker state=present

mkdir -p roles/users/tasks
vim roles/users/tasks/main.yml

- name: create users
  user: name={{item}} state=present
  with_items:
    - uday
    - naveen
    - rohit
    - lokesh
    - saipallavi
    - supriya

mkdir -p roles/webserver/tasks
vim roles/web/tasks/main.yml

- name: installing httpd
  yum: name=httpd state=present

- name: starting httpd
  service: name=httpd state=started

cat master.yml

- hosts: all
  roles:
    - pkgs
    - users
    - webserver

find . -type f -exec sed -i 's/present/absent/g' {} \;


ANSIBLE GALAXY:

Ansible Galaxy is a  website where users can share roles and to a command-line tool for installing, creating, and managing roles.
Ansible Galaxy gives greater visibility to one of Ansible's most exciting features, such as application installation or reusable roles for server configuration. 
Lots of people share roles in the Ansible Galaxy.
Ansible roles consist of many playbooks, which is a way to group multiple tasks into one container to do the automation in a very effective manner with clean, directory structures.

ANSIBLE VAULT:
it is used to encrypt the files, playbooks ----
Technique: AES256 (USED BY FACEBOOK, AWS)
vault will store our data very safely and securely.
if we want to access any data which is in the vault we need to give a password.
Note: we can restrict the users to access the playbook also.


cat creds.txt
user=raham
passowrd=test123

ansible-vault create creds1.txt		: to create a vault
ansible-vault edit creds1.txt		: to edit a vault
ansible-vault rekey creds1.txt		: to change password for a vault
ansible-vault decrypt creds1.txt	: to decrypt the content	
ansible-vault encrypt creds1.txt	: to encrypt the content	
ansible-vault view creds1.txt		: to show the content without decrypt

PIP: its a pkg manager used to install python libs/modules

Redhat: yum
ubuntu: apt
python: pip

- hosts: all
  tasks:
    - name: install pip
      yum: name=pip state=present

    - name: installing NumPy
      pip: name=NumPy state=present

    - name: installing Pandas
      pip: name=Pandas state=present

ASYNCHRONOUS & POLLING ACTIONS:
for every task in  ansible we can set time limit
if the task is not performed in that time limit ansible will stop playbook execution
this is called as asynchronous and polling.

- hosts: all
  ignore_errors: yes
  tasks:
    - name: sleeping
      command: sleep 30
      async: 20
      poll: 10

    - name: install git
      yum: name=git state=present


async: time we set for task to complete
poll: it will check if task is completed or not for every 10 sec

WEB SERVER : TO SHOW THE APP : httpd  : 80  : /var/www/html
frontend code
APP SERVER : TO USE THE APP : Tomcat  : 8080  : tomcat/webapps
frontend code + backend code


- hosts: all
  tasks:
    - name: installing httpd
      yum: name=httpd state=present

    - name: starting httpd
      service: name=httpd state=started

    - name: installing git
      yum: name=git state=present

    - name: checkout
      git:
        repo: https://token@github.com/RAHAMSHAIK007/netflix-clone.git
        dest: /var/www/html
      tags: a


===============================================================================================

LINK FOR SCRIPTS & PLAYBOOKS : https://github.com/RAHAMSHAIK007/all-setups.git
LINK FOR PROJECT: https://github.com/devopsbyraham/jenkins-java-project.git

Install Jenkins on ansible server and connect to dashboard

SETP-1: PUSHING THE CODE FROM GIT TO GITHUB : N0
STEP-2: PERFORM CI ON JENKINS : NO
STEP-3: STORE ARTIFACT ON S3 : YES
STRP-4: INSTALL TOMCAT ON NODES : YES
SETP-5: COPY WAR FILE TO TOMCAT

INTEGRTAING S3 WITH JENKINS:
1. installing s3 publish plugin.
2. configure s3profile 
(Manage Jenkins -- > System -- > s3 prfile -- > give creds -- > test -- > save)
3. use pipeline syntax to give details

INTEGRTAING ANSIBE WITH JENKINS:

1. install ansible plugin
	
manage jenkins -- > tools -- > ansible -- > name: ansible & Path to ansible executables directory: /usr/bin -- > save (NOTE: /usr/bin is a folder where all Linux commands will store)

3. SAMPLE STEP: ANSIBLE PLAYBOOK
pipeline syntax -- > sample steps -- > select ansible playbook
Ansible tool: ansible -- > 
Playbook file path in workspace: /etc/ansible/playbook.yml -- > 
Inventory file path in workspace: /etc/ansible/hosts -- > 
SSH CREDS: give creds of ansible & worker nodes -- > 
Disable the host SSH key check -- > 
generate script
node {
    stage('code') {
        git 'https://github.com/devopsbyraham/jenkins-java-project.git'
    }
    stage('build') {
        sh 'mvn compile'
    }
    stage('test') {
        sh 'mvn test'
    }
    stage('artifact') {
        sh 'mvn package'
    }
    stage('s3') {
        s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'netflixartifact0099', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: false, noUploadOnFailure: false, selectedRegion: 'ap-south-1', showDirectlyInBrowser: false, sourceFile: 'target/NETFLIX-1.2.2.war', storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: false]], pluginFailureResultConstraint: 'FAILURE', profileName: 's3-bucket', userMetadata: []
    }
    stage('deploy') {
        ansiblePlaybook credentialsId: 'ansible', disableHostKeyChecking: true, installation: 'ansible', inventory: '/etc/ansible/hosts', limit: '$server', playbook: '/etc/ansible/deploy.yml', vaultTmpPath: ''
    }
}



cat playbook.yml
- hosts: all
  tasks:

    - name: task1
      copy:
        src: /var/lib/jenkins/workspace/pipeline/target/NETFLIX-1.2.2.war
        dest: /root/tomcat/webapps


========================================================

APPLICATION: Collection of services 

MONOLITHIC: multiple services are deployed on single server with single database.
MICRO SERVICES: multiple services are deployed on multiple servers with multiple database.

BASED ON USERS AND APP COMPLEXITY WE NEED TO SELECT THE ARCHITECTURE.

FACTORS AFFECTING FOR USING MICROSERVICES:
F-1: COST 
F-2: MAINTAINANCE

CONTAINERS:
its free of cost and can create multiple containers.
its same as a server/vm.
it will not have any operating system.
os will be on images.
(SERVER=AMI, CONTAINER=IMAGE)

CONTAINER TOOLS: DOCKER, CONTAINERD, ROCKET, CRI-O, PODMAN, KATA CONTAINER ----

DOCKER: 
Its an free & opensource tool.
it is platform independent.
used to create, run & deploy applications on containers.
it is introduced on 2013 by solomenhykes & sebastian phal.
We used GO language to develop the docker.
here we write files on YAML.
before docker user faced lot of problems, but after docker there is no issues with the application.
Docker will use host resources (cpu, mem, n/w, os).
Docker can run on any OS but it natively supports Linux distributions.

CONTAINERIZATION/DOCKERIZATION:
Process of packing an application with its dependencies.
ex: PUBG

APP= PUBG & DEPENDECY = MAPS
APP= CAKE & DEPENDECY = KNIFE

os level of virtualization.

VIRTUALIZATION:
able to create resource with our hardware properties.

ARCHITECTURE & COMPONENTS:
client: it will interact with user
user gives commands and it will be executed by docker client

daemon: manages the docker components(images, containers, volumes)

host: where we install docker (ex: linux, windows, macos)

Registry: manages the images.

ARCHITECTURE OF DOCKER:
yum install docker -y    #client
systemctl start docker	 #client,Engine
systemctl status docker


COMMANDS:
docker pull ubuntu	: pull ubuntu image
docker images		: to see list of images
docker run -it --name cont1 ubuntu : to create a container
-it (interactive) - to go inside a container
cat /etc/os-release	: to see os flavour


apt update -y	: to update 
redhat=yum
ubuntu=apt
without update we cant install any pkg in ubuntu


apt install git -y
apt install apache2 -y
service apache2 start
service apache2 status

ctrl p q		: to exit container
docker ps -a		: to list all containers
docker attach cont_name	: to go inside container
docker stop cont_name	: to stop container
docker start cont_name	: to start container
docker pause cont_name	: to pause container
docker unpause cont_name: to unpause container
docker inspect cont_name: to get complete info of a container
docker rm cont_name	: to delete a container

STOP: will wait to finish all process running inside container
KILL: wont wait to finish all process running inside container
==============================================================================

==============================================================================

OS LEVEL OF VIRTUALIZATION:
ability to take backup of complete os and reuse it.

docker pull ubuntu
docker run -it --name cont1 ubuntu
apt update -y
apt install mysql-server apache2 python3 -y
touch file{1...5}
apache2 -v
mysql-server --version
python3 --version
ls

ctrl p q

docker commit cont1 raham:v1
docker run -it --name cont2 raham:v1
apache2 -v
mysql-server --version
python3 --version
ls


DOCKERFILE:
it is an automation way to create image.
here we use components to create image.
in Dockerfile D must be Capiatl.
Components also capital.
To write our instructions we need to use components in Dockerfile
This Dockerfile will be Reuseable.
here we can create image directly without container help.
Name: Dockerfile

docker kill $(docker ps -qa)
docker rm $(docker ps -qa)
docker rmi -f $(docker images -qa)

COMPONENTS:

FROM		: used to base image
RUN		: used to run linux commands (During image creation)
CMD		: used to run linux commands (After container creation)
ENTRYPOINT	: high priority than cmd
COPY		: to copy local files to container
ADD		: to copy internet files to container
WORKDIR		: to open req directory
LABEL		: to add labels for docker images
ENV		: to set env variables (inside container)
ARGS		: to pass env variables (outside containers)
EXPOSE		: to indicate port number


EX-1:
FROM ubuntu
RUN apt update
RUN apt install apache2 mysql-server python3 -y
RUN touch file{1..5}

docker build -t raham:v1 .
docker run -it --name cont1 raham:v1 

EX-2:
FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
RUN apt install python3 -y
CMD apt install mysql-server -y


docker build -t raham:v2 .
docker run -it --name cont2 raham:v2

EX-3:
FROM ubuntu
COPY index.html /tmp
ADD http://dlcdn.apache.org/tomcat/tomcat-9/v9.0.89/bin/apache-tomcat-9.0.89.tar.gz /tmp

docker build -t raham:v3 .
docker run -it --name cont3 raham:v3

EX-4:

FROM ubuntu
COPY index.html /tmp
ADD http://dlcdn.apache.org/tomcat/tomcat-9/v9.0.89/bin/apache-tomcat-9.0.89.tar.gz /tmp
WORKDIR /tmp
LABEL author rahamshaik

docker build -t raham:v4 .
docker run -it --name cont4 raham:v4


EX-5:
FROM ubuntu
LABEL author rahamshaik
ENV client swiggy
ENV server appserver

docker build -t raham:v5 .
docker run -it --name cont5 raham:v5

INDEX.HTML LINK: https://www.w3schools.com/howto/tryit.asp?
filename=tryhow_css_form_icon

NETFLIX-DEPLOYMENT:

yum install git -y
git clone https://github.com/RAHAMSHAIK007/netflix-clone.git
cd netflix-clone/

 Vi Dockerfile

FROM ubuntu
RUN apt update
RUN apt install apache2 -y
COPY * /var/www/html/
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]


docker build -t netflix:v1 .
docker run -it --name netflix1 -p 80:80 netflix:v1

MULTI-STAGE BUILD:
With multi-stage builds, you use multiple FROM statements in your Dockerfile
if we build image-1 from docker file and use that image-1 to build other image.

Dockerfile  -- > image1

Dockerfile
FROM image1  -- > multi-stage build
-----
-------

ADV:
less time
less work
less complexity

===========================================================================

SESSION-62:

VOLUMES:
It is used to store data inside container.
volume is a simple directory inside container.
containers uses host resources (cpu, ram, rom).
single volume can be shared to multiple containers.
ex: cont-1 (vol1)  --- > cont2 (vol1) & cont3 (vol1) & cont4 (vol1)
at a time we can share single volume to single container only.
every volume will store under /var/lib/docker/volumes

METHOD-1:
DOCKER FILE:

FROM ubuntu
VOLUME ["/volume1"]

docker build -t raham:v1 .
docker run -it --name cont1 raham:v1
cd volume1/
touch file{1..5}
cat>file1
ctrl p q

docker run -it --name cont2 --volumes-from cont1  ubuntu
cd /volume1
ll
touch file{6..10}
ctrl pq

docker attach cont1
cd volume1
ll

docker run -it --name cont3 --volumes-from cont1 ubuntu

METHOD-2:
FROM CLI:

docker run -it --name cont4 -v volume2 ubuntu
cd volume2/
touch java{1..5}
ctrl p q

docker run -it --name cont5 --volumes-from cont4  ubuntu
cd volume2
ll
touch java{6..10}
ctrl p q
docker attach cont4
ls



METHOD-3: VOLUME MOUNTING

docker volume ls 		: to list volumes
docker volume create name	: to create volume
docker volume inspect volume3	: to get info of volume3
cd /var/lib/docker/volumes/volume3/_data 
touch python{1..5}
docker run -it --name cont5 --mount source=volume3,destination=/volue3 ubuntu
docker volume rm 	: to delete volumes
docker volume prune	: to delete unused volumes

HOST -- > CONTAINER:

cd /root
touch raham{1..5}
docker volume inspect volume4
cp * /var/lib/docker/volumes/volume4/_data
docker exec cont5 ls /volume4


RESOURCE MANAGEMENT:
By default, docker containers will not have any limits for the resources like cpu ram and memory so we need to restrict resource use for container.

By default docker containers will use host resources(cpu, ram, rom)
Resource limits of docker container should not exceed the docker host limits.

docker stats  --> to check live cpu and memory

docker run -it --name cont7 --cpus="0.1" --memory="300mb" ubuntu
docker update cont7 --cpus="0.7" --memory="300mb"

JENKINS SETUP BY DOCKER:
docker run -it --name jenkins -p 8080:8080 jenkins/jenkins:lts

PROMETHEUS SETUP BY DOCKER:
docker run -it --name prometheus -p 9090:9090 bitnami/prometheus:latest

GRAFANA SETUP BY DOCKER:
docker run -it --name=grafana -p 3000:3000 grafana/grafana


====================================================================================

SESSION-63:

vim Dockerfile

FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
COPY index.html /var/www/html
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]

Index.html: take form w3 schools 
https://www.w3schools.com/howto/tryit.asp?filename=tryhow_css_form_icon	

docker build -t movies:v1 .
docker run -itd --name movies -p 81:80 movies:v1

docker build -t train:v1 .
docker run -itd --name train -p 82:80 train:v1

docker build -t dth:v1 .
docker run -itd --name dth -p 83:80 dth:v1

docker build -t recharge:v1 .
docker run -itd --name recharge -p 84:80 recharge:v1

docker ps -a -q		: to list container ids
docker kill $(docker ps -a -q) : to kill all containers 
docker rm $(docker ps -a -q) : to remove all containers 

Note: In the above process all the containers are managed and created one by one in real time we manage all the containers at same time so for that purpose we are going to use the concept called Docker compose.


DOCKER COMPOSE:
It's a tool used to manage multiple containers in single host.
we can create, start, stop, and delete all containers together.
we write container information in a file called a compose file.
compose file is in YAML format.
inside the compose file we can give images, ports, and volumes info of containers.
we need to download this tool and use it.

INSTALLATION:
sudo curl -L "https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
ls /usr/local/bin/
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
docker-compose version


In Linux majorly you are having two type of commands first one is inbuilt commands which come with the operating system by default 
second one is download commands we are going to download with the help of yum, apt or Amazon Linux extras.
some commands we can download on binary files.

NOTE: linux will not give some commands, so to use them we need to download seperately
once a command is downloaded we need to move it to /usr/local/bin
because all the user-executed commands in linux will store in /usr/local/bin
executable permission need to execute the command



vim docker-compose.yml

version: '3.8'
services:
  movies:
    image: movies:v1
    ports:
      - "81:80"
  train:
    image: train:v1
    ports:
      - "82:80"
  dth:
    image: dth:v1
    ports:
      - "83:80"
  recharge:
    image: recharge:v1
    ports:
      - "84:80"

COMMANDS:
docker-compose up -d		: to create and start all containers
docker-compose stop		: to stop all containers
docker-compose start		: to start all containers
docker-compose kill		: to kill all containers
docker-compose rm		: to delete all containers
docker-compose down		: to stop and delete all containers
docker-compose pause		: to pause all containers
docker-compose unpause		: to unpause all containers
docker-compose ps -a		: to list the containers managed by compose file
docker-compose images		: to list the images managed by compose file
docker-compose logs		: to show logs of docker compose
docker-compose top		: to show the process of compose containers
docker-compose restart		: to restart all the compose containers
docker-compose scale train=10	: to scale the service


CHANGING THE DEFULT FILE:

by default the docker-compose wil support the following names
docker-compose.yml, docker-compose.yaml, compose.yml, compose.yaml

mv docker-compose.yml raham.yml
docker-compose up -d	: throws an error

docker-compose -f raham.yml up -d
docker-compose -f raham.yml ps
docker-compose -f raham.yml down


images we create on server.
these images will work on only this server.

git (local) -- > github (internet) = to access by others
image (local) -- > dockerhub (internet) = to access by others

Replace your username 

STEPS:
create dockerhub account
create a repo

docker tag movies:v1 vijaykumar444p/movies
docker login -- > username and password
docker push vijaykumar444p/movies


docker tag train:v1 vijaykumar444p/train
docker push vijaykumar444p/train


docker tag dth:v1 vijaykumar444p/dth
docker push vijaykumar444p/dth

docker tag recharge:v1 vijaykumar444p/recharge
docker push vijaykumar444p/recharge

docker rmi -f $(docker images -q)
docker pull vijaykumar444p/movies:latest

DOCKER SAVE:
docker image save swiggy:v1 > swiggy:v1.tar :covert image to file 
docker image history swiggy:v1
docker rmi swiggy:v1
docker images
docker image load < swiggy\:v1.tar

COMMAND TO ZIP:  gzip dummy:v5.tar abc.zip
DECOMPRESS COMMAND: gzip movies:latest.gz -d

COMPRESSING DOCKER IMAGE SIZE:
1. push to dockerhub
2. use multi stage docker build
3. reduce layers
4. use tar balls
==============================================================================================
SESSION-64:

High Avaliabilty: more than one server
why: if one server got deleted then other server will gives the app

DOCKER SWARM:
its an orchestration tool for containers. 
used to manage multiple containers on multiple servers.
here we create a cluster (group of servers).
in that cluster, we can create same container on multiple servers.
here we have the manager node and worker node.
manager node will create & distribute the container to worker nodes.
worker node's main purpose is to maintain the container.
without docker engine we cant create the cluster.
Port: 2377
worker node will join on cluster by using a token.
manager node will give the token.



SETUP:
create 3 servers
install docker and start the service
hostnamectl set-hostname manager/worker-1/worker-2
Enable 2377 port 

docker swarm init (manager) -- > copy-paste the token to worker nodes
docker node ls

Note: individual containers are not going to replicate.
if we create a service then only containers will be distributed.

SERVICE: it's a way of exposing and managing multiple containers.
in service we can create copy of conatiners.
that container copies will be distributed to all the nodes.

service -- > containers -- > distributed to nodes

http://54.242.16.170:81/
http://54.158.250.28:81/
http://3.85.8.127:81/

docker service create --name movies --replicas 3 -p 81:80 vijaykumar444p/movies:latest
docker service ls		: to list services
docker service inspect movies	: to get complete info of service
docker service ps movies	: to list the containers of movies
docker service scale movies=10	: to scale in the containers
docker service scale movies=3	: to scale out the containers
docker service rollback movies	: to go previous state
docker service logs movies	: to see the logs
docker service rm movies	: to delete the services.

when scale in it follows lifo pattern.
LIFO MEANS LAST-IN FIRST-OUT.

Note: if we delete a container it will recreate automatically itself.
it is called as self healing.


CLUSTER ACTIVIES:
docker swarm leave (worker)	: to make node inactive from cluster
To activate the node copy the token.
docker node rm node-id (manager): to delete worker node which is on down state
docker node inspect node_id	: to get comple info of worker node
docker swarm join-token manager	: to generate the token to join

Note: we cant delete the node which is ready state
if we want to join the node to cluster again we need to paste the token on worker node


DOCKER NETWORKING:
Docker networks are used to make communication between the multiple containers that are running on same or different docker hosts. 

We have different types of docker networks.
Bridge Network		: SAME HOST
Overlay network		: DIFFERENT HOST
Host Network
None network

BRIDGE NETWORK: It is a default network that container will communicate with each other within the same host.

OVERLAY NETWORK: Used to communicate containers with each other across the multiple docker hosts.

HOST NETWORK: When you Want your container IP and ec2 instance IP same then you use host network

NONE NETWORK: When you don’t Want The container to get exposed to the world, we use none network. It will not provide any network to our container.


To create a network: docker network create network_name
To see the list: docker network ls
To delete a network: docker network rm network_name
To inspect: docker network inspect network_name
To connect a container to the network: docker network connect network_name container_id/name
docker exec -it cont1  /bin/bash
apt update
apt install iputils-ping -y : command to install ping checks
ping ip-address of cont2
crtl pq

To disconnect from the container: docker network disconnect network_name container_name
To prune: docker network prune

DATABASE SETUP:
docker run -itd --name dbcont -e MYSQL_ROOT_PASSWORD=raham123 mysql:9.0.1
docker exec -it dbcont /bin/bash
mysql -u root -p

docker run -it --rm -p 8888:8080 tomcat:9.0

==================================================================================================

SESSION-65:

PROJECT PIPELINE CODE:

pipeline {
    agent any
    
    tools {
        jdk 'jdk17'
        nodejs 'node16'
    }
    environment {
        SCANNER_HOME = tool 'mysonar'
    }
    stages {
        stage("Clean WS") {
            steps {
                cleanWs()
            }
        }
        stage("Code") {
            steps {
                git "https://github.com/devops0014/Zomato-Project.git"
            }
        }
        stage("Sonarqube Analysis") {
            steps {
                withSonarQubeEnv('mysonar') {
                    sh """$SCANNER_HOME/bin/sonar-scanner \
                        -Dsonar.projectName=zomato \
                        -Dsonar.projectKey=zomato"""
                }
            }
        }
        stage("Quality Gates") {
            steps {
                script {
                    waitForQualityGate abortPipeline: false, credentialsId: 'sonar-token'
                }
            }
        }
        stage("Install Dependencies") {
            steps {
                sh 'npm install'
            }
        }
        stage("OWASP") {
            steps {
                dependencyCheck additionalArguments: '--scan ./ --disableYarnAudit --disableNodeAudit', odcInstallation: 'DP-Check'
                dependencyCheckPublisher pattern: '**/dependency-check-report.xml'
            }
        }
        stage("Trivy") {
            steps {
                sh 'trivy fs . > trivyfs.txt'
            }
        }
        stage("Build") {
            steps {
                sh 'docker build -t image1 .'
            }
        }
        stage("Tag & Push") {
            steps {
                script {
                    withDockerRegistry(credentialsId: 'docker-password') {
                        sh 'docker tag image1 rahamshaik/mydockerprojectfor5pm:myzomatoimage'
                        sh 'docker push rahamshaik/mydockerprojectfor5pm:myzomatoimage'
                    }
                }
            }
        }
        stage("Scan the Image") {
            steps {
                sh 'trivy image rahamshaik/mydockerprojectfor5pm:myzomatoimage'
            }
        }
        stage("Container") {
            steps {
                sh 'docker run -d --name cont1 -p 3000:3000 rahamshaik/mydockerprojectfor5pm:myzomatoimage'
            }
        }
    }
}


Key Differences
Feature		OWASP				SonarQube
Focus		Web app security risks		Code quality & security
Type		Security framework/tools	Static code analysis tool
Approach	Scanning live apps, guidelines	Scanning source code
Use Cases	Finding web vulnerabilities	Improving code quality, security

When to Use What?
Use OWASP when you need security best practices, penetration testing, and live application security assessments.

Use SonarQube when you need automated code analysis for vulnerabilities, bugs, and maintainability in your source code.

WHAT IS TRIVY: 

Trivy is an open-source security scanner for containers, Kubernetes, infrastructure as code (IaC), and dependencies. It helps detect vulnerabilities, misconfigurations, and secrets in various environments.

How Trivy Works
It fetches vulnerability data from sources like NVD, Red Hat, Debian, and GitHub Security Advisories.
Scans images, filesystems, repositories, or Kubernetes clusters.
Outputs results in various formats like JSON, table, and SARIF (for security tools).

Why Use Trivy?
✅ Fast & lightweight – Minimal setup and quick scanning.
✅ Comprehensive – Covers multiple security aspects (CVEs, misconfigurations, secrets).
✅ Easy integration – Works with Docker, Kubernetes, and CI/CD pipelines.
✅ Free & open-source – Maintained by Aqua Security.

========================================================================
SESSION-66:

ECS & ECR:

ECS:

Amazon Elastic Container Service (Amazon ECS) is a highly scalable and fast container management service. 
Used to create, run, stop, and manage containers on a cluster. 
ECS doesn’t have any server and compute power (CPU & Ram).
Can be able to do roll back.
With Amazon ECS, your containers are defined in a task definition that you use to run an individual task or task within a service.
Deploy & load balance application across multiple servers.
It will do auto scaling to handle the large traffic.
You can run your tasks and services on a serverless infrastructure that's managed by AWS Fargate. 
Alternatively, for more control over your infrastructure, you can run your tasks and services on a cluster of Amazon EC2 instances that you manage.


DOCKER-COMPOSE VS ECS:
When we use the docker-compose file to deploy it contains all the configuration of containers.
For example if we have 3 servers we cannot deploy to all the containers at the same time, and if you do the same deployment on all three servers individually it cannot able to know all the 3 servers are manging the same application which is major disadvantage.
Docker compose cannot able to do automatic load balancing and Auto scaling.
Instead of using K8S, Swarm, Apahe Mesos and NoMad we can use ECS which is alternative for those
Apache Mesos is the opposite of virtualization because in virtualization one physical resource is divided into multiple virtual resources, while in Mesos multiple physical resources are clubbed into a single virtual resource.


ECS LAUNCH TYPES
WORKING WITH EC2:
Here we are going to manage the infrastructure ie instances.
We need to install ECS agent to communicate.
Need to set Firewall.
We need to make sure all the patches is done for recent updates.
Finally you can manage the containers and configure it as per your requirments.

WORKING WITH FARGET:
It follow serverless Architecture.
We don’t have EC2 instances so need not maintain them.
It will create servers on demand.
We will pay for what we use here.


ECS TASKS:
A task definition is required to run Docker containers in Amazon ECS. Simply it is the blue print for containers and how it is deployed.  It will contain
The Docker image to use with each container in your task for Application.
How much CPU and memory to use with each task or each container within a task
The infrastructure that your tasks are hosted on
The Docker networking mode to use for the containers in your task
The logging configuration to use for your tasks
Whether the task continues to run if the container finishes or fails
The command that the container runs when it's started
Any data volumes that are used with the containers in the task
The IAM role that your tasks use

ECS SERVICES:
A service definition defines how to run your Amazon ECS service.
It will ensure that certain tasks are running at all times.
It will restart the containers that is crashed or exited.
For example if you have an application we want 2 instances/containers tp run our application all time, we say this to service then it will create 2 instances/containers and start running our application.
If any instance fail it will restart the task.
LOAD BALANCERS:
The main intention is to distribute the traffic here.
If we have app is deployed we can assign the LB and route the traffic to resources.
If we scale our instance the LB is here able to drive the traffic to newly created instance.



HOSTING AN APPLICATION ON ECS:
UPLOADING IMAGE TO ECR:
Amazon Elastic Container Registry (ECR) is a fully managed container registry.
Easy to store, manage, share, and deploy your container images and artifacts anywhere.
It supports private repositories with resource-based permissions using AWS IAM. 
Specified users or EC2 instances can access your container repositories and images. 
You can use your preferred CLI to push, pull, and manage Docker images, Open Container Initiative (OCI) images, and OCI compatible artifacts.




ECR -- > CREATE A REPO -- > VISIBILITY: PUBLIC -- > NAME: RAHAM -- > CREATE 
aws configure -- > PLS GENERATE A NEW USER KEYS
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 247004640985.dkr.ecr.us-east-1.amazonaws.com

docker build -t raham .
docker tag raham:latest 247004640985.dkr.ecr.us-east-1.amazonaws.com/raham:latest
docker push 247004640985.dkr.ecr.us-east-1.amazonaws.com/raham:latest
CREATE ECS CLUSTER
ECS -- > APACHE -- > APP LOAD BALANCER -- > CLUSTER: NAME : RAHAM -- > CREATE


SECOND WAY ECS FARGATE:
ECS -- > CREATE CLUSTER -- > NAME: RAHAM -- > CREATE

TASK DEFINATION -- > CREATE A NEW TASK DEFINATION -- > TASK DEFINATION FAMILY: swiggy-dev-task-defination -- > NAME: Hello & URL: nginxdemos/hello -- > NEXT -- > NEXT -- > 

CREATE SERVICE -- > EXITING CLUSTER: RAHAM -- > SERVICE NAME: HELLOWORLD -- > 
REPLICAS : 3 -- > SG : (ALL TRAFFIC) -- > LOAD BALANCER: APPLICATION -- > NEW -- > NAME : -- >
TG NAME: RAHAM-TG -- > GRACE PEROID : 20



STEPS: WORK ON MUMBAI REGION

1. CREATE EC2 AND INSTALL GIT & DOCKER
yum install git docker -y
systemctl start docker
systemctl status docker

2. GET THE CODE AND CREATE A IMAGE
git clone https://github.com/RAHAMSHAIK007/netflix-clone.git
cd netflix-clone

vim Dockefile

FROM ubuntu
RUN apt update && apt install apache2 -y
COPY * /var/www/html
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]

docker build -t swiggy:v1 .

3. CREATE A REPO ON ECR AND PUSH THEIMAGE

ECR -- > CREATE -- > NAME: SWIGGY -- > SCAN IMAGE -- > CREATE REPO

View push commands
NOTE: USE ACCESS KEY AND SECRET KEYS FOR AUTHENTICATION


4. CREATE A CLUSTER BY USING FARGETE.
ECS -- > CLUSTER -- > CREATE -- > NAME -- > 
AWS Fargate (serverless) -- > CREATE

5. CREATE TASK DEFINATION.	
TASK DEFINATAION -- > CREATE -- > Task definition family: Netflix -- > AWS Fargate -- > VALUES UPTO YOUR CHOICE -- > CREATE

6. CREATE SERVICE 

ECS =-- > SERVICE -- > FARGATE -- > TASK -- > NETFLIX -- > DESIRED TASKS: 3 -- > CREATE

NOTE: DELETE TASKS BEFORE DELETE CLUSTER.

================================================================================================================

SESSION-67: RESUME BUILDING

LINKEDIN -- > DEVOPS JOBS -- > POSTS -- > LATEST -- > PAST 24 HRS -- > APPLY

DEVOPS JOBS
AWS JOBS
AWS DEVOPS JOBS
SRE JOBS
SYSTEM ENGINNER

========================================================
SESSION-68: K8S-DAY-01: 20-02-2025

DOCKER ALTERNATIVES: containerd, rocket, cri-o


CHIRU   : CLUSTER
NAGABABU: NODE
PAWAN   : POD
CHARAN	: CONTAINER
ALLU	: APP

K8S:

LIMITATIONS OF DOCKER SWARM:
1. CANT DO AUTO-SCALING AUTOMATICALLY
2. CANT DO LOAD BALANCING AUTOMATICALLY
3. CANT HAVE DEFAULT DASHBOARD
4. WE CANT PLACE CONATINER ON REQUITED SERVER.
5. USED FOR EASY APPS. 

HISTORY:
Initially Google created an internal system called Borg (later called as omega) to manage its thousands of applications 
later they donated the borg system to cncf and they make it as open source. 
initial name is Borg but later cncf rename it to Kubernetes 
the word Kubernetes originated from Greek word called pilot or Hailsmen.
Borg: 2014
K8s first version came in 2015.


INTRO:

IT is an open-source container orchestration platform.
It is used to automates many of the manual processes like deploying, managing, and scaling containerized applications.
Kubernetes was developed by GOOGLE using GO Language.
MEM -- > GOOGLE -- > CLUSTER -- > MULTIPLE APPS OF GOOGLE -- > BORG -- > OMEGA -- > CNCF : K8S
K8s first version came in 2015.



ARCHITECTURE:

DOCKER : CNCA
K8S: CNPCA

C : CLUSTER
N : NODE
P : POD
C : CONTAINER
A : APPLICATION


COMPONENTS:
MASTER:

1. API SERVER: communicate with user (takes command execute & give op)
2. ETCD: database of cluster (stores complete info of a cluster ON KEY-VALUE pair)
3. SCHEDULER: select the worker node to shedule pods (depends on hw of node)
4. CONTROLLER: control the k8s objects (n/w, service, Node)

WORKER:

1. KUBELET : its an agent (it will inform all activites to master) It create containers.
2. KUBEPROXY: it deals with nlw (ip, networks, ports)
3. POD: group of conatiners (inside pod we have app)

Note: all components of a cluster will be created as a pod.

API SERVER          : FATHER
ETCD                : MARRIAGE BROKER
SCHEDULER           : BRIDE
CONTROLLER          : EVEN ORG 

KUBLET              : SIBBLINGS
KUBE PROXY          : RELATIVES 
POD                 : FUNCTION HALL


CLUSTER TYPES:

1. SELF MANAGED: WE NEED TO CREATE & MANAGE THEM

minikube = single node cluster
kubeadm = multi node cluster (manual)
kops = multi-node cluster (automation)
kind:
k9s:
kubespray:


2. CLOUD-BASED: CLOUD PROVIDERS WILL MANAGE THEM

AWS = EKS = ELASTIC KUBERNETES SERVICE
AZURE = AKS = AZURE KUBERENETS SERVICE
GOOGLE = GKE = GOOGLE KUBERENETS ENGINE



MINIKUBE:
It is a tool used to setup single node cluster on K8's. 
Here Master and worker runs on same server.
It contains API Servers, ETDC database and container runtime
It is used for development, testing, and experimentation purposes on local. 
It is a platform Independent.
Installing Minikube is simple compared to other tools.

NOTE: But we don't implement this in real-time Prod

REQUIREMENTS:

2 CPUs or more
2GB of free memory
20GB of free disk space
Internet connection
Container or virtual machine manager, such as: Docker.

Kubectl is the command line tool for k8s
if we want to execute commands we need to use kubectl.

SETUP:
sudo apt update -y
sudo apt upgrade -y
sudo apt install curl wget apt-transport-https -y
sudo curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo mv minikube-linux-amd64 /usr/local/bin/minikube
sudo chmod +x /usr/local/bin/minikube
sudo minikube version
sudo curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
sudo echo "$(cat kubectl.sha256) kubectl" | sha256sum --check
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
sudo minikube start --driver=docker --force

NOTE: When you download a command as binary file it need to be on /usr/local/bin 
because all the commands in linux will be on /usr/local/bin 
and need to give executable permission for that binary file to work as a  command.



POD:
It is a smallest unit of deployment in K8's.
It is a group of containers.
Pods are ephemeral (short living objects)
Mostly we can use single container inside a pod but if we required, we can create multiple containers inside a same pod.
when we create a pod, containers inside pods can share the same network namespace, and can share the same storage volumes .
While creating pod, we must specify the image, along with any necessary configuration and resource limits.
K8's cannot communicate with containers, they can communicate with only pods.
 We can create this pod in two ways, 
1. Imperative(command) 
2. Declarative (Manifest file)


IMPERATIVE:

kubectl run pod1 --image vinodvanama/paytmmovies:latest
kubectl get pods/pod/po
kubectl get pod -o wide
kubectl describe pod pod1
kubectl delete pod pod1

DECRALATIVE: by using file called manifest file

MANDATORY FEILDS: without these feilds we cant create manifest

apiVersion:
kind:
metadata:
spec:


vim pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - image: vinodvanama/paytmtrain:latest
      name: cont1

execution: 
kubectl create -f pod.yml
kubectl get pods/pod/po
kubectl get pod -o wide
kubectl describe pod pod1
kubectl delete -f raham.yml

DRAWBACK: once pod is deleted we can't retrieve the pod.

================================================================================
SESSION: 69 - DAY-02(K8S) : 21-02-2025


DAY-02:

LABELS:
Labels are key-value pairs that are attached to pods, RC and services. 
They can be added or modified at the during creation or run time.
Rc manages pods based on labels only.
kubectl run pod-1 --image=nginx  --labels="env=dev, app=swiggy"
kubectl get pods --show-labels


SELECTOR:
Selector filter the Pods with same labels.
There are two kinds of selectors: Equality based and Set base


REPLICASET:

It will create multiple replicas of same Pod.
If One Pod deleted it will automatically create new Pod.
All the pods will have same config. (from Template)
We can do Auto Scaling and Load Balancing Through ReplicaSet.
In Background Replication Controller  will be Responsible to create Replicas.
ReplicaSets will use Labels and Selectors to identify Pods.
Replication Controller is Older Version and ReplicaSet is New Version.

CODE:

vim replicaset.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: movies
  labels:
    app: paytm
spec:
  replicas: 3
  selector:
    matchLabels:
      app: paytm
  template:
    metadata:
      labels:
        app: paytm
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest

COMANDS:
TO CREATE              : kubectl create -f abc.yml      
TO LIST                : kubectl get rs
NODE INFO              : kubectl get pod -o wide
COMPLETE INFO          : kubectl describe rs movies
TO EDIT                : kubectl edit rs movies
TO DELETE              : kubectl delete rs movies
TO SCALE               : kubectl scale rs/movies --replicas=10 (LIFO)
TO SHOW LABELS         : kubectl get pods -l app=Paytm

we cant Rollin and rollout, we cant update the application in rs.

DEPLOYMENT:
It has features of RS and some other extra features like updating and rollbacking to a particular version.
The best part of Deployment is we can do it without downtime.
Deployment will create ReplicaSet, ReplicaSet will created Pods.

CODE:

vim deployment.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: paytm
spec:
  replicas: 3
  selector:
    matchLabels:
      app: paytm
  template:
    metadata:
      labels:
        app: paytm
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest



COMMANDS:
TO CREATE              : kubectl create -f abc.yml      
TO LIST                : kubectl get rs
NODE INFO              : kubectl get pod -o wide
COMPLETE INFO          : kubectl describe rs movies
TO EDIT                : kubectl edit rs movies
TO DELETE              : kubectl delete rs movies
TO SCALE               : kubectl scale rs/movies --replicas=10 (LIFO)
TO SHOW LABELS         : kubectl get pods -l app=Paytm
IMPERATIVE             :kubectl create deploy movies --image=name --replicas=4


NOTE: TO GET INFO OF  DEPLOYMENT TO FILE
kubectl create deployment movies  --image=name  --replicas=4 --dry-run=client -o yaml > movies-deployment.yml

ROLLOUT COMMANDS:
kubectl rollout history deployment movies 	: to show deployment versions
kubectl rollout status deployment movies 	: to show deployment status
kubectl rollout pause deployment movies 	: to show pause deployment
kubectl rollout resume deployment movies 	: to show unpause deployment 
 
===========================================================================================
SESSION-70: K8S-PART-3: 22-02-2025

METRIC SERVER:
if we install metric server in k8s cluster it can collects metrics like cpu, ram -- from all the pods and nodes in cluster.
we can use kubectl top po/no to see metrics
previously we can called it as heapster.


MINIKUBE INSTALLATION:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
minikube addons enable metrics-server #(only for minikube)

kubectl top nodes
kubectl top pods


Metrics Server offers:

    A single deployment that works on most clusters (see Requirements)
    Fast autoscaling, collecting metrics every 15 seconds.
    Resource efficiency, using 1 milli core of CPU and 2 MB of memory for each node in a cluster.
    Scalable support up to 5,000 node clusters.


You can use Metrics Server for:
CPU/Memory based horizontal autoscaling (Horizontal Autoscaling)
Automatically adjusting/suggesting resources needed by containers (Vertical Autoscaling)

================================================================================================
SESSION-71: KOPS : 24-02-2025

MINIKUBE ALTERNATIVES:
KUBEADM, KOPS, KIND, RANCHER, EKS, AKS, GKE, KIND9

KOPS:
INFRASTRUCTURE: Resources used to run our application on cloud.
EX: Ec2, VPC, ALB, ASG-------------


Minikube -- > single node cluster
All the pods on single node 
if that node got deleted then all pods will be gone.

KOPS:
kOps, also known as Kubernetes operations.
it is an free and  open-source tool.
used to create, destroy, upgrade, and maintain a highly available, production-grade Kubernetes cluster. 
Depending on the requirement, kOps can also provide cloud infrastructure.
AWS (Amazon Web Services) and GCE (Google Cloud Platform) are currently officially supported, with DigitalOcean, Hetzner and OpenStack in beta support, and Azure in alpha.

ADVANTAGES:
•	Automates the provisioning of AWS and GCE Kubernetes clusters
•	Deploys highly available Kubernetes masters
•	Supports rolling cluster updates
•	Autocompletion of commands in the command line
•	Generates Terraform and CloudFormation configurations
•	Manages cluster add-ons.
•	Supports state-sync model for dry-runs and automatic idempotency
•	Creates instance groups to support heterogeneous clusters

ALTERNATIVES:
Amazon EKS , MINIKUBE, KUBEADM, RANCHER, TERRAFORM.



STEP-1: GIVING PERMISSIONS 

KOps Is a third party tool if it want to create infrastructure on aws 
aws need to give permission for it so we can use IAM user to allocate permission for the kops tool

IAM -- > USER -- > CREATE USER -- > NAME: KOPS -- > Attach Polocies Directly -- > AdministratorAccess -- > NEXT -- > CREATE USER
USER -- > SECURTITY CREDENTIALS -- > CREATE ACCESS KEYS -- > CLI -- > CHECKBOX -- >  CREATE ACCESS KEYS -- > DOWNLOAD 

aws configure (run this command on server)

SETP-2: INSTALL KUBECTL AND KOPS

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
wget https://github.com/kubernetes/kops/releases/download/v1.25.0/kops-linux-amd64
chmod +x kops-linux-amd64 kubectl
mv kubectl /usr/local/bin/kubectl
mv kops-linux-amd64 /usr/local/bin/kops

vim .bashrc
export PATH=$PATH:/usr/local/bin/  -- > save and exit
source .bashrc

SETP-3: CREATING BUCKET 
aws s3api create-bucket --bucket rahamsdevopsbatchnewkopsclass.k8s.local --region us-east-1
aws s3api put-bucket-versioning --bucket rahamsdevopsbatchnewkopsclass.k8s.local --region us-east-1 --versioning-configuration Status=Enabled
export KOPS_STATE_STORE=s3://rahamsdevopsbatchnewkopsclass.k8s.local

SETP-4: CREATING THE CLUSTER
kops create cluster --name rahams.k8s.local --zones us-east-1a --master-count=1 --master-size t2.medium --node-count=2 --node-size t2.micro 
kops update cluster --name rahams.k8s.local --yes --admin


Suggestions:
 * list clusters with: kops get cluster
 * edit this cluster with: kops edit cluster rahams.k8s.local
 * edit your node instance group: kops edit ig --name=rahams.k8s.local nodes-us-east-1a
 * edit your master instance group: kops edit ig --name=rahams.k8s.local master-us-east-1a

ERROR:
Error: State Store: Required value: Please set the --state flag or exp                  
SOL: export KOPS_STATE_STORE=s3://rahamsdevopsbatchmay14102024am.k8s.local


ADMIN ACTIVITIES:
To scale the worker nodes:
kops edit ig --name=rahams.k8s.local nodes-us-east-1a
kops update cluster --name rahams.k8s.local --yes --admin 
kops rolling-update cluster --yes

ADMIN ACTIVITIES:
kops edit ig --name=rahams.k8s.local master-us-east-1a
kops update cluster --name rahams.k8s.local --yes
kops rolling-update cluster

SCALING IN GUI: ASG -- > MASTER/WORKER NODE -- > EDIT -- > DESIRED: 4 -- > SAVE

NOTE: In real time we use five node cluster two master nodes and three worker nodes.

NOTE: its My humble request for all of you not to delete the cluster manually and do not delete any server use the below command to delete the cluster.

TO DELETE: kops delete cluster --name rahams.k8s.local --yes

===============================================================
SESSION-72: NAMESPACE, SERVICES : 25-02-2025


NAMESPACE: 

It is used to divide the cluster to multiple teams on real time.
Used for isolating groups of resources within cluster.
By Default we work on Default Name space in K8's.
We create NameSpaces when we work for Prod level Workloads.
If we create pod on one namespace it cant be access by other namespaces.
We can set access limits by RBAC and Limits of Cpu, RAM by Quotas.
It is  applicable only for namespaced objects (e.g. Deployments, Services, etc.)
It wont apply for cluster-wide objects (e.g. StorageClass, Nodes, PV).


CLUSTER: HOUSE
NAMESPACES: ROOM
TEAM MATES: FAMILY MEM

Each namespace is isolated.
if your are room-1 are you able to see room-2.
If dev team create a pod on dev ns testing team cant able to access it.
we cant access the objects from one namespace to another namespace By Default.


TYPES:

default           : Is the default namespace, all objects will create here only
kube-node-lease   : it will store object which is taken from one node to another.
kube-public	  : all the public objects will store here.      
kube-system 	  : default k8s will create some objects, those are storing on this ns.

NOTE: Every component of Kubernetes cluster is going to create in the form of pod
And all these pods are going to store on kUBE-SYSTEM ns.

kubectl get pod -n kube-system	: to list all pods in kube-system namespace
kubectl get pod -n default	: to list all pods in default namespace
kubectl get pod -n kube-public	: to list all pods in kube-public namespace
kubectl get po -A		: to list all pods in all namespaces
kubectl get po --all-namespaces

kubectl create ns dev	: to create namespace
kubectl config set-context --current --namespace=dev : to switch to the namespace
kubectl config view : to see current namespace
kubectl run dev1 --image nginx
kubectl run dev2 --image nginx
kubectl run dev3 --image nginx
kubectl create ns test	: to create namespace
kubectl config set-context --current --namespace=test : to switch to the namespace
kubectl config view --minify | grep namespace : to see current namespace
kubectl get po -n dev
kubectl delete pod dev1 -n dev
kubectl delete ns dev	: to delete namespace
kubectl delete pod --all: to delete all pods

NOTE: BY DEFAULT K8S NAMESPACE WILL PROVIDE ISOLATION BUT NOT RESTRICTION.
TO RESTRICT THE USER TO ACCESS A NAMESPACE IN REAL TIME WE USE RBAC.
WE CREATE USER, WE GIVE ROLES AND ATTACH ROLE.

alias switch="kubectl config set-context --current"
switch --namespace=default
switch --namespace=dev

SERVICE: 
Used to expose Pods to the users.
If Front end Pod need to communicate with backend pod we use service for it.

COMMAND: kubectl api-resources

TYPES:
CLUSTER-IP
NODE PORT
LOAD BALANCER

COMPONENTS OF SERVICES:
Selector: To select pods
Port: Associated to Service
TargetPort: Associated to Pod
nodePort: Associated to Node
Type: Type of the service


TYPES:
1. CLUSTERIP: It will work inside the cluster.
it will not expose to outer world.
Ideal for internal communication within a cluster.
Suitable for backend services like databases, caches, or internal APIs 
Preferred in Development and Testing Envs.
Not accessible from outside the cluster.



apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: movies
  name: movies-deploy
spec:
  replicas: 10
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
      - name: cont1
        image: rahamshaik/moviespaytm:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: service1
spec:
  type: ClusterIP
  selector:
    app: movies
  ports:
    - port: 80

DRAWBACK:
We cannot use app outside.

2. NODEPORT: 

Node Port Range= 30000 - 32767
NodePort expose Pod on a static port on each node.
if Port is not given it will assign automatically.
if target Port is not Given it takes Port value.
NodePort services are typically used for smaller applications with a lower traffic volume.


apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: movies
  name: movies-deploy
spec:
  replicas: 10
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
      - name: cont1
        image: rahamshaik/moviespaytm:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: service1
spec:
  type: NodePort
  selector:
    app: movies
  ports:
    - port: 80
      nodePort: 31111


NOTE: UPDATE THE SG (REMOVE OLD TRAFFIC AND GIVE ALL TRAFFIC)
DRAWBACK:
EXPOSING PUBLIC-IP & PORT 
PORT RESTRICTION.

3. LOADBALACER: 
In LoadBalaner we can expose application externally with the help of Cloud Provider LoadBalancer.
it is used when an application needs to handle high traffic loads and requires automatic scaling and load balancing capabilities.
After the LoadBalancer service is created, the cloud provider will created the Load Balancer.
REPLACE  NodePort with LoadBalancer


apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/trainservice:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: abc
spec:
  type: LoadBalancer
  selector:
    app: swiggy
  ports:
    - port: 80
==============================================================================
SESSION-73: DATA-DOG, CLOUD FRONT : 26-2-2025

WHAT IS DATADOG
Datadog is a monitoring and analytics tool.
It monitors servers, databases and tools.
used to show performance metrics & Event monitoring for infra and cloud services.
It is platform  independent which supports multiple clouds too.
Datadog uses a Go-based agent and its backend is made from Apache Cassandra, PostgreSQL and Kafka. 
YEAR: 2010

SUPPORTED INTEGRATIONS
CLOUDS: AWS, AZURE, GCP, ALI BABA------
TOOLS: Kubernetes, Chef, Puppet, Ansible,  Bitbucket ----
DATABASE: MYSQL, CASSANDRA, POSTGRES, REDIS ----
NOTIFICATION: SLACK, TEAMS, MAIL, PAGER DUTY
SERVERLESS: CONTAINERS, LAMBDA, FARGET


SECURITY OPTIONS
ENCRYPTION IN TRANSIT & IN REST
RBAC IMPLEMENTATION
MFA 
DATA ISOLATION
AUDITING


INSTALLATION & SETUP
STEP-1: CREATE AN AMAZON-LINUX EC2 INSTANCE
STEP-2: CREATE DATADOG ACCOUNT
STEP-3: SELECT AGENT AMAZONLINUX AND INSTALL SCRIPT ON EC2 (wait for few mins)
STEP-4: systemctl start datadog-agent && systemctl status datadog-agent
STEP-5: datadog-agent health && datadog-agent version
STEP-6: datadog-agent hostname
STEP-7: AUTOMATICALL SHOWS INFO ON DASHBOARD AFTER SOMETIME


CDN: CONTENT DELIVERY NETWORK
USED TO DELIVER APP FROM EDGE LOCATION
IT GIVES FAST RESPONSE 

WE NEED TO CREATE ORIGIN
ORIGIN: FROM WHERE YOU ORIGINAL APPLICATION IS COMMING

EXAMPLE OF ORIGINS: S3, ELB, API GATEWAY


STEP-1: CREATE 2 SERVERS AND DEPLOY AMAZON APP
STEP-2: CREATE A LOAD BALANCER
SETP-3: CLOUD FRONT -- > ORIGIN DOMAIN: ELB (SELECT YOUR LB) -- > Enable Origin Shield: US-EAST-1  -- > Protocol: HTTPS -- > Viewer protocol policy: Redirect HTTP to HTTPS -- >  SELECT WAF -- > IPv6: OFF -- > CREATE
	
NOTE: IT WILL TAKE 2 MINS TO ACTIVATE 

HOW TO BLOCK USERS FROM DIFF LOCATIONS:
SECURITY -- > CloudFront geographic restrictions
==========================================================================
SESSION-74: STATIC PODS

STATIC PODS:
Kubelet Create Pods without API Server is called as Static Pods.
we put Pod manifest in /etc/kubernetes/manifests [on worker node]
Kubelet Read files and Create Pods and Manages it.
If we made any change to Manifest File Kubelet Recreates the Pod.
If we remove file then pod will be deleted automatically.
We can change path of location in kubelet.service file.
use [ --config=abc.yml ] and in abc.yml [ staticPodPath: /etc/xyz]
Even if we create Pod from Kubelet API Server knows about that Pod.
Because API will have a Read only Mirror of a Pod but cant edit and delete.
Kubelet can create static and dynamic pods at a same time.
USE CASES: used to Deploy control-plane components as a pods in Master.
NOTE: RS, DEPLOYMENT cant create because they required Controllers.


PRACTICAL:
cd /etc/kubernetes/manifests [Run this on Worker node]
Create a manifest file and run it.
Go to Kops server and Check

==========================================================================
SESSION-75: CONFIGMAP, SECRETS, MCP

3/1/2025

DEPLOYMENT ROLLOUTS:
When we create a deployment a revision-1 is created.
When we update a deployment a revision-2 is created.
BY DEFUALT K8S WILL FOLLOW ROLLING-UPDATE STRATEGY.
when we want to rollback to previous version of application we can use

COMMANDS:
kubectl rollout history deploy/movies
kubectl rollout undo deploy/movies
kubectl rollout status deploy/movies
kubectl rollout pause deploy/movies
kubectl rollout resume deploy/movies

ENV VARS:
In k8s we can set env vars using evn feild.
env is array means we can set multiple values.
it has key-value fromat.
to set env vars we use configmaps and secrets.


CONFIG MAPS:
it is used to pass  configuration data to pods in key-value fromat.
we pod is created inject configmap, so data is used as env varibles.
First create configmap and later inject to pod.
But the data should be non confidential data ()
But it does not provider security and encryption.
Limit of config map data in only 1 MB (for more data use volumes).
To pass values from cli use literal.

COMMAND:
kubectl create cm raham --from-literal=password=raham123 [FROM CLI]
kubectl create cm raham --from-file=password=raham123 [FROM FILE]
kubectl get cm
kubectl describe cm raham

CODES:

cat config.yml

apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  user: "raham"
  password: "test123"

cat pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: env-configmap
spec:
  containers:
    - name: app
      image: nginx
      envFrom:
        - configMapRef:
            name: my-configmap



SECERETS:
Used to store sensitive information like passwords, keys ---
it wont encrypt data but it will encode them in base64.
when pod is created inject Secret.
Dont push to github becuse they are not encrypted but encode.
it wont encrypt in etcd also.
Data feild indicates number of secrets in secret.
USE BELOW COMMAND TO ENCODE: echo -n "raham123" | base64
SE BELOW COMMAND TO DECODE: echo -n "raham123" | base64 -d

COMMAND:
kubectl create secret generic raham --from-literal=password=raham123
kubectl create secret generic raham --from-file=password=raham123
kubectl get secret raham -o yaml
kubectl describe secret raham

cat secret.yml

apiVersion: v1
kind: Secret
metadata:
  name: my-secrets
data:
  user: "cmFoYW0="
  password: "dGVzdDEyMw=="


cat pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: env-secret
spec:
  containers:
    - name: app
      image: nginx
      envFrom:
        - secretRef:
            name: my-secret


NOTE:
Dont push to github becuse they are not encrypted but encode.
it wont encrypt in etcd also.
Enable Encryption at Rest for Secrets so they stored as encrypted in ETCD
secret is not written to disk storage, Kubelet stores them to tmpfs.
Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.

CREATE A DEPLOY AND ADDING SECRETS TO IT
kubectl create deploy swiggy --image mariadb
kubectl get po
kubectl describe po swiggy-64ff5fcfb5-ln6kj
kubectl logs swiggy-64ff5fcfb5-ln6kj
IN THE LOGS IT IS TELLING THERE IS NO PASSWORD TO DB
kubectl create secret generic mariadb --from-literal=MARIADB_ROOT_PASSWORD=raham123
kubectl edit deploy swiggy
kubectl get po


MULTI-CONTAINER POD:
It will have more than one container in a pod.
each container have different purpose to work on.
They created and destroyed together and share same n/w and volume.
If any of them fails, the POD restarts.


SIDE CAR:
It creates a helper container to main container.
main container will have the app and helper container Helps main container.

Adapter Design Pattern:
enable communication and coordination between containers.

Ambassador Design Pattern:
used to connect containers with the outside world

Init Container:
it initialize the first work and exits later.

CODE:
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: cont1
    image: nginx:1.14.2
  - name: cont2
    image: nginx:1.18
=======================================================================
SESSION 76: KUBEADM AND QUOTAS 

CREATE 3 SERVERS [1 MASTER 2 WORKERS]

STEP-1: INSTALL KUBELET, KUBEADM, KUBECTL

sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl gpg

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

STEP-2: INSTALL CONTAINER RUNTIME
sudo apt-get update
sudo apt install containerd -y

STEP-3: Configuring the systemd cgroup driver 
sudo mkdir -p /etc/containerd/
containerd config default
containerd config default > /etc/containerd/config.toml
sed -i '139s/false/true/' /etc/containerd/config.toml
systemctl restart containerd.service
systemctl status containerd.service	

STEP-4: CREATE A CLUSTER (ON MASTER NODE)
sudo echo 1 | sudo tee /proc/sys/net/ipv4/ip_forward
sudo kubeadm init --apiserver-advertise-address 172.31.86.121 --pod-network-cidr "10.244.0.0/16" --upload-certs

kubeadm join 172.31.31.163:6443 --token s9nzbw.q7fnbrwpeksbz5bc \
        --discovery-token-ca-cert-hash sha256:267ee04431d660e3e170651bf3fb783fd9a5bf867856b3f706e3a80c5d50b791

#COPY TOKEN TO WORKER NODES

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

#COPY ABOVE COMMAND TO MASTER ONLY

STEP-5: DEPLOY A NETWORK PLUGIN (WEAVENET)
GO TO ADDONS PAGE IN K8S DOCS AND FIND IT

kubectl apply -f https://reweave.azurewebsites.net/k8s/v1.29/net.yaml




RESORCE QUOTAS:

Scheduler checks the  CPU, RAM of node before we Place a Pod
If Node CPU, RAM is not matching for Pod, it select another Node in Cluster.
If any node is not having Sufficient CPU, RAM then pod will go Pending state.
Without limits a container in pod can consume all CPU, RAM of Node.
So we need to set limits in Real time to restrict the Containers.
Note: Limits are set in container level, if we have 2 containers in a pod then set values individually.
A container cant use more Cpu’s than Specified limit, But Not Memory.
Pod will be terminated when it use more than limits and we get OOM error.
To set limits and Request we use Quotas and Limit Ranges in Real time.
LimitRange applies to individual containers or pods.
ResourceQuota applies to the entire namespace.

kubectl create ns dev
kubectl config set-context $(kubectl config current-context) --namespace=dev

vim dev-quota.yml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: dev
spec:
  hard:
    pods: "5"
    limits.cpu: "1"
    limits.memory: 1Gi

kubectl create -f dev-quota.yml
kubectl get quota


EX-1: Mentioning Limits  = SAFE WAY

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest
          resources:
            limits:
              cpu: "0.2"
              memory: 200Mi

==================================================================================
SESSION-77: CLUSTER UPGRADE : 4-3-2025

K8S CLUSTER UPDATE:

1. ALL AT ONCE : WE WILL LOSE ACCESS TO APP
2. ONE AT A TIME : WE DONT LOSE ACCESS TO APP
3. REPLACE: WE REPLACE OLD SERVER WITH NEW SERVER


LINK: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

CLUSTER UPGRADING: RUN ALL COMMANDS ON MASTER NODES

echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg


sudo apt update
sudo apt-cache madison kubeadm


sudo apt-mark unhold kubeadm && \
sudo apt-get update && sudo apt-get install -y kubeadm='1.32.0-1.1' && \
sudo apt-mark hold kubeadm


kubeadm version
kubeadm upgrade plan --force
kubeadm upgrade apply v1.32.0  --force

kubectl get no

NOW UPDATE KUBELETE:
STEP-1: MOVE THE EXISTING PODS
kubectl drain <node-to-drain> --ignore-daemonsets


sudo apt-mark unhold kubelet kubectl && \
sudo apt-get update && sudo apt-get install -y kubelet='1.32.0-1.1' kubectl='1.32.0-1.1' && \
sudo apt-mark hold kubelet kubectl

sudo systemctl daemon-reload
sudo systemctl restart kubelet

kubectl uncordon <node-to-uncordon>
kubectl get no 


UPGRADE WORKER NODES: RUN ALL COMMANDS ON WORKER NODES

echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg


sudo apt update
sudo apt-cache madison kubeadm


sudo apt-mark unhold kubeadm && \
sudo apt-get update && sudo apt-get install -y kubeadm='1.32.0-1.1' && \
sudo apt-mark hold kubeadm


kubeadm version
sudo kubeadm upgrade node
sudo apt-mark unhold kubelet kubectl && \
sudo apt-get update && sudo apt-get install -y kubelet='1.32.0-1.1' kubectl='1.32.0-1.1' && \
sudo apt-mark hold kubelet kubectl

sudo systemctl daemon-reload
sudo systemctl restart kubelet

kubectl get no -- > master node


DAEMONSET:
used to create Only one pod on each workernode.
Its the old version of Deployment.
if we create a new node a pod will be automatically created.
if we delete a old node a pod will be automatically removed.
daemonsets will not be removed at any case in real time.
Kube-Proxy is Deployed as a DaemonSet in all Nodes.
DeamonSet uses NodeAffinity and Default Scheduler to Place Pods.
USECASES: we can create pods for Logging, Monitoring of nodes.
COMMAND: kubectl get ds

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: movies
  labels:
    app: movies
spec:
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest

	

=====================================================================================================================

SESSION-78: CLUSTER UPGRADE : 5-3-2025

FILE TO CHECK FOR USER, CLUSTER, NAMESPACE DETAILS: /root/.kube/config
RBAC:

role : set of permissions for one ns
role binding: adding users to role
these will work on single namespace

cluster role: set of permissions for entire Cluster
cluster role binding: adding users to cluster role
these will work on all namespaces


when we run kubectl get po k8s api server will authenticate and check authorization
authentication: permission to login
authorization: permission to work on resources

To authenticate API requests, k8s uses the following options:
client certificates,
bearer tokens,
authenticating proxy,
or HTTP basic auth.

Kubernetes doesn’t have an API for creating users.
Though, it can authenticate and authorize users.

We will choose the client certificates as it is the simplest among the four options.

why certs needed on k8s: for authentication purpose.
certs will have users & keys for login.

LINK: https://kubernetes.io/docs/tasks/administer-cluster/certificates/

1. Create a client certificate
We’ll be creating a key and certificate sign request (CSR) needed to create the certificate. Let’s create a directory where to save the certificates. I’ll call it cert:

mkdir dev1 && cd dev1

1. Generate a key using OpenSSL:
openssl genrsa -out dev1.key 2048


2. Generate a Client Sign Request (CSR) :

openssl req -new -key dev1.key -out dev1.csr -subj "/CN=dev1/O=group1"
ls ~/.minikube/

3. Generate the certificate (CRT):

openssl x509 -req -in dev1.csr -CA ~/.minikube/ca.crt -CAkey ~/.minikube/ca.key -CAcreateserial -out dev1.crt -days 500

Now, that we have the .key and the .crt, we can create a user.



Create a user
1. Set a user entry in kubeconfig
kubectl config set-credentials dev1 --client-certificate=dev1.crt --client-key=dev1.key

2.Set a context entry in kubeconfig
kubectl config set-context dev1-context --cluster=minikube --user=dev1
kubectl config view

3.Switching to the created user
kubectl config use-context dev1-context
$ kubectl config current-context # check the current context
dev1-context
But, now, dev1 doesn’t have any access privileges to the cluster. For that we’ll have access denied if we try to create any resource:

$ kubectl create namespace ns-test
kubectl get po
Error from server (Forbidden): namespaces is forbidden: User "dev1" cannot create resource "namespaces" in API group "" at the cluster scope

3. Grant access to the user
To give access to manage k8s resources to dev1, we need to create a Role and a BindingRole.

kubectl config use-context minikube
kubectl create ns dev



3.1. Create a Role

vim role.yml

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: dev-role
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "create", "watch", "list", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dev-rolebind
  namespace: dev
subjects:
- kind: User
  name: dev1
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: dev-role
  apiGroup: rbac.authorization.k8s.io


kubectl apply -f role.yml

kubectl config use-context dev1-context
kubectl config view | grep -i namespace
kubectl config set-context --current --namespace=dev

kubectl run pod1 --image nginx : work
kubectl get po : work
kubectl delete po pod1 : error -- > no permissions

TO UPDATE PERMISSION GO TO MINIKUBE USER
kubectl config use-context minikube

ADD THIS IN FILE:
verbs: ["get", "create", "watch", "list", "delete"]
kubectl apply -f role.yml
kubectl config use-context dev1-context
kubectl delete po pod1 : work


kubectl create deploy raham --image nginx --replicas 2
kubectl create cm raham --from-literal=user=raham

NOW UPATE ROLE WITH DEPLOYMEN AND CM

kubectl config use-context minikube

ADD THIS IN FILE:
- apiGroups: ["*"] # 
resources: ["pods", "deployments", "configmaps"]
kubectl apply -f role.yml
kubectl config use-context dev1-context



BY DEFAULT API VERSION IN ROLE IS: v1 ("")
SO TO WORK WITH ALL RESOURCES PUT *

CLUSTER ROLE: GIVES PERMISSION TO WORK WITH ENTIRE CLUSTER

vim clusterrole.yml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: dev-role
rules:
- apiGroups: ["*"] # "" indicates the core API group
  resources: ["pods", "deployments", "configmaps", "namespaces"]
  verbs: ["get", "create", "watch", "list", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: dev-rolebind
subjects:
- kind: User
  name: dev1
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: dev-role
  apiGroup: rbac.authorization.k8s.io


kubectl create ns test
kubectl config use-context dev1-context
kubectl config view | grep -i namespace
kubectl config set-context --current --namespace=dev



SOME OBJECT IN K8S CREATE IN CLUSTER LEVEL SO WE NEED TO CREATE CLUSTERROLE AND CLUSTER ROLEBIND
PV, PVC, NS


CLI: BEST WAY TO WORK

kubectl create role -h
kubectl create role test --verb=create,delete,list --resource=pods -n test

kubectl create rolebinding -h
kubectl create rolebinding testbinding --role=test --user=dev1 -n test

EXAMPLE:  kubectl get deploy raham -n test -o yaml > dep.yml

ASSIGNMENTS: CREATE ON CLI COMMANDS AND EXTRAC TO FILE AFTER CREATING THEM
CREATE A POD WITH NGINX IMAGE AND LABEL CALL APP=SWIGGY -- > EXTRACT TO FILE POD.YML
CREATE A DEPLOY WITH BUSYBOX IMAGE AND LABEL CALL APP=ZOMATO --REPLICAS 5 -- > EXTRACT TO FILE DEP.YML
CREATE A CM RAHAM APP=UBER ON TEST NS -- >  EXTRACT TO FILE CM.YML
CREATE A SECRETS RAHAM APP=OLA ON TEST NS -- >  EXTRACT TO FILE SECRET.YML
CREATE ROLE AND ROLEBIND IN DEV NS -- > EXTRACT TO FILE ROLE.YML
CREATE CLUSTERROLE AND CLUSTERROLEBIND  -- > EXTRACT TO FILE CLUSTERROLE.YML


====================================================

SESSION-79: CLUSTER UPGRADE : 4-3-2025

A Service Account is a special type of account used by applications, scripts, or services to interact with APIs, cloud services, or other systems without requiring user authentication. 
Unlike user accounts, service accounts are not tied to a person and do not require manual login.

Key Features of Service Accounts:
Automated Authentication: Used for non-human access to resources.
Access Control: Can be assigned specific permissions using roles.
No Passwords: Uses keys or tokens for authentication.
Security Focused: Designed to limit access and minimize security risks.

Where Are Service Accounts Used?
Cloud Platforms (e.g., AWS, Google Cloud, Azure) – To access cloud resources securely.
CI/CD Pipelines – For automated deployments and integrations.
API Access – To interact with third-party or internal APIs.
Server-to-Server Communication – For microservices and backend operations.


source <(kubectl completion bash) # set up autocomplete in bash into the current shell, bash-completion package should be installed first.
echo "source <(kubectl completion bash)" >> ~/.bashrc # add autocomplete permanently to your bash shell.


vim sa.yml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: jenkins
  namespace: default

kubectl create sa jenkins -n default

NOTE: TO WORK IN REAL TIME SA NEED TOKENS TO WORK WITH K8S CLUSTER

THIS COMMAND WILL CREATE A ROLE FOR JENKINS SA:
kubectl create role jenkinsrole --verb=create,delete,list  --resource=pods

THIS COMMAND WILL ATTACH ROLE FOR JENKINS SA:
kubectl create rolebinding jenkinsrolebind --role=jenkinsrole --serviceaccount=default:jenkins


VERIFY:
kubectl auth can-i list pods --as=system:serviceaccount:kube-system:jenkins
no
kubectl auth can-i create pods --as=system:serviceaccount:default:jenkins
yes
kubectl auth can-i delete pods --as=system:serviceaccount:default:jenkins
yes
kubectl auth can-i delete cm --as=system:serviceaccount:default:jenkins
no



WORK WITH GRAFAN:

kubectl create sa Grafana
kubectl create clusterrole grafanaclusterrole --verb=create,delete,list --resource=pods,deployments

kubectl create clusterrolebinding grafanaclusterrolebind1 --clusterrole=grafanaclusterrole --serviceaccount=default:grafana

kubectl create clusterrolebinding grafanaclusterrolebind2 --clusterrole=grafanaclusterrole --serviceaccount=kube-system:grafana

kubectl create clusterrolebinding grafanaclusterrolebind1 --clusterrole=grafanaclusterrole --serviceaccount=kube-public:grafana

VERIFY:

kubectl auth can-i list pods --as=system:serviceaccount:default:grafana
yes
kubectl auth can-i list pods --as=system:serviceaccount:kube-system:grafana
yes
kubectl auth can-i list pods --as=system:serviceaccount:kube-public:grafana
yes
kubectl auth can-i list pods --as=system:serviceaccount:kube-node-lease:grafana
no


NOTE: FOR ALL NAMESPACES CLUSTER ROLE BIND WILL NOT ACCEPT BY DEFAULT
SO WE NEED TO CREATE CLUSTER ROLE BIND FOR ALL NS SEPERATELY



K8S BACKUPS:

IN K8S ALL COMPONENTS IN CLUSTER WILL INTERACT SAFELY AND SECURLY WITH CERTS.
ONLY API SERVER CAN COMMUNICATE WITH ETCD.

STEP-1: INSTALL ETCD

#! /bin/bash
sudo apt update
sudo apt install vim wget curl
export RELEASE=$(curl -s https://api.github.com/repos/etcd-io/etcd/releases/latest|grep tag_name | cut -d '"' -f 4)
wget https://github.com/etcd-io/etcd/releases/download/${RELEASE}/etcd-${RELEASE}-linux-amd64.tar.gz
tar xvf etcd-${RELEASE}-linux-amd64.tar.gz
cd etcd-v3.5.19-linux-amd64
sudo mv etcd etcdctl etcdutl /usr/local/bin 
etcd --version


STEP-2: ETCD POD INFO 
kubectl describe po etcd-minikube -n kube-system

STEP-3: TAKE BACKUP (get cret values from above etcd pod)

ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379   --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key   snapshot save /tmp/raham.db

STEP-4: RESTORE
etcdctl --data-dir /var/lib/etcd/abcd snapshot restore raham.db


=============================================================================

SESION: 80 : SCHEDULING

BEST EXAMPLE: PET DOG LOYALITY

SCHEDULER:
Scheduler main work is to select the node to place the pod.
while a pod is created it will select a node based on pod requirment.
if scheduler cant find node then pod will be on pending state.
Initially we can place a pod on node by writing node name on manifest.
scheduler runs as a pod if it stop/delete scheduling wont happen.




TAINTS & TOLERATIONS:
By Default scheduler can place any pod on any Node.
if we want to place Pod-1 on  Node-1 then we use Taints & Tolerations.
First we taint the Node-01 so that no pod will be placed on any node.
Then we can tolerate the Particular pod what we want to place on Node-01.
We set Taint for Nodes & Tolerations for Pods.
EX: Kubectl taint node01 app=swiggy:NoSchedule
NOTE: it wont guarantee that  pod-1 definitely place on Node-01 all the time
By Default k8s master is tainted thats why no pod is placed on master.
To schedule pod on a master add - at end by running taint command.

kubectl taint node i-04a90983debd1c8ec app=swiggy:NoSchedule


apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: paytm
spec:
  replicas: 10
  selector:
    matchLabels:
      app: paytm
  template:
    metadata:
      labels:
        app: paytm
    spec:
      containers:
        - name: cont1
          image: nginx
      tolerations:
        - key: "app"
          operator: "Equal"
          value: "swiggy"
          effect: "NoSchedule"


NODE SELECTOR:
Kubernetes can select nodes to place pods based on the labels of a node.
First we can label the node & later same labels we can add to pod manifest.
Now all pods will be set on same node.
NOTE: It can place pod on node with single label, But if we want to place pod on a node with multiple lables this wont work.
kubectl label node node_id key=value

kubectl label nodes node-1 hero=tillu


---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: paytm
spec:
  replicas: 10
  selector:
    matchLabels:
      app: paytm
  template:
    metadata:
      labels:
        app: paytm
    spec:
      containers:
        - name: cont1
          image: nginx
      nodeSelector:
        hero: "tillu"


NOTE: IF NODE IS HAVING ANY TAINTS THE PODS WILL NOT SCHEDULE
kubectl taint node i-06c09c90c2768ceba app=swiggy:NoSchedule-

kubcetl create -f dep.yml

TO REMOVE LABEL: kubectl label node i-0b17bfc7885c1c9f3 hero=diwakar --overwrite



NODE AFFINITY:
its a feature in Kubernetes that facilitates us to specify the rules for scheduling the pod based on the node labels. 
we can use operators with multiple values too.
OPERATORS: [In, NotIn, Exists, DoesNotExist, Gt, Lt]
TYPES:
requiredDuringSchedulingIgnoredDuringExecution:  While scheduling matching the label is mandatory, after placing pod optional.

PreferredDuringSchedulingIgnoredDuringExecution: While scheduling matching the label is not mandatory, after placing pod optional.



GIVE LABELS AND TRY THIS 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
spec:
  replicas: 10
  selector:
    matchLabels:
      app: paytm
  template:
    metadata:
      labels:
        app: paytm
    spec:
      containers:
      - name: nginx
        image: nginx:latest
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: hero
                operator: In
                values:
                - yesudas
                - tillu
=======================================================================================================================================================

SESSION-81: 08-03-2025

BULLET POINTS:
1. ITS A SERVICE USED TO EXPOSE APP
2. IT EXPOSE APP BASED ON URL
3. IT SUPPORTS ENCRYPTION AND SSL REDIRECTION
4. TO DISTRIBUTE TRAFFIC EFFICIENTLY WE CAN USE INGRESS
5. BY DEFAULT THIS IS NOT INSTALLED ON CLUSTER WE NEED TO INSTALL.

INGRESS IN K8S:
Ingress helps to expose the HTTP and HTTPS routes from outside of the cluster.
Ingress supports 
Path-based  
Host-based routing
Ingress supports Load balancing and SSL termination.
It redirects the incoming requests to the right services based on the Web URL or path in the address.
Ingress provides the encryption feature and helps to balance the load of the applications.

Ingress is used to manage the external traffic to the services within the cluster which provides features like host-based routing, path-based routing, SSL termination, and more. Where a Load balancer is used to manage the traffic but the load balancer does not provide the fine-grained access control like Ingress.

Example:
Suppose you have multiple Kubernetes services running on your cluster and each service serves a different application such as example.com/app1 and example.com/app2. With the help of Ingress, you can achieve this. However, the Load Balancer routes the traffic based on the ports and can't handle the URL-based routing.

To install ingress, firstly we have to install nginx ingress controller:
command: kubectl create -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.1/deploy/static/provider/cloud/deploy.yaml

Once we install ingress controller, we have to deploy 2 applications. 
github url: https://github.com/mustafaprojectsindevops/kubernetes/tree/master/ingress
After executing all the files, use kubectl get ing to get ingress. After 30 seconds it will provide one load balancer dns.

access those applications using dns/nginx and dns/httpd. So the traffic will route into both the applications as per the routing


CAST-AI: SHOWS THE COST AND OPTIMZE THE COST FOR K8S CLUSTER
CREATE CLUSTER
CREATE CASTAI ACCOUNT
CONNECT TO CLUSTER -- > SELECT KOPS -- > COPY PASTE COMMANDS IN CLUSTER -- > I RAN SCRIPT
CONNECT CLUSTER



K8SGPT: CHATGPT FOR K8S WORKS
install kops cluster

curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.24/k8sgpt_amd64.rpm
sudo rpm -ivh -i k8sgpt_amd64.rpm
k8sgpt version

TO CONNECT:
k8sgpt auth add
LOGIN TO OPENAI ACCOUNT -- > PROFILE -- > APIKEYS
copy past the key u generate from openai chatgpt
k8sgpt analyze
k8sgpt analyze -o json
k8sgpt analyze -o json | jq .
k8sgpt analyze --explain --filter=Pod --namespace=default

LINK: https://github.com/k8sgpt-ai/k8sgpt
LINK: https://www.kubiya.ai/
LINK: https://www.eraser.io/diagramgpt

========================================================================================================
EKS:

INTRO:
Amazon Elastic Kubernetes Service (Amazon EKS) 
it's a service used to run Kubernetes on AWS.
EKS runs control plane across multiple AZs to ensure high availability
EKS Automatically detects and replaces unhealthy control plane nodes.
You can use a range of AWS storage services with Amazon EKS for the storage.


ADVANTAGES:
Fully Managed Service
Easy Cluster Updates
High Availability
Scalability
Security

DISADVANTAGES:

Cost
Regional Availability
Time
Limited Flexibility in Control Plane Management


REAL-TIME USE CASES TO USE EKS:
TO WORK YOUR K8S IN AWS ONLY
ROBUST SCALING
TO REDUCE COMPLEX MANAGEMNETS

SETUP:

#STEP-1: UPDATE

apt update && apt install unzip -y

#STEP-2: EKS & KUBECTL & AWS CLI INSTALLATION

curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
aws configure

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
mv kubectl /usr/local/bin/kubectl


sudo wget https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz
sudo tar -xzvf eksctl_$(uname -s)_amd64.tar.gz -C /usr/local/bin
eksctl version


#STEP-3: CREATE CLUSTER
eksctl create cluster --name=raham-cluster --version 1.28 --zones=us-east-1a,us-east-1b,us-east-1c --without-nodegroup

kubectl cluster-info

eksctl utils associate-iam-oidc-provider --region us-east-1 --cluster raham-cluster --approve

#STEP-4: CREATE NODEGROUP
eksctl create nodegroup --cluster=raham-cluster --region=us-east-1 \
--name=raham-cluster-ng-1 --node-type=t2.micro \
--nodes=2 --nodes-min=2 --nodes-max=4 --node-volume-size=20 \
--ssh-access  --ssh-public-key=devopsbyraham \
--managed --asg-access --external-dns-access --full-ecr-access \
--appmesh-access --alb-ingress-access


eksctl get clusters
eksctl get nodegroup --cluster my-cluster 

#STEP-5: DELETE RESOURCES  
eksctl delete nodegroup --cluster my-cluster --name my-cluster-ng-1 
eksctl delete cluster --name=my-cluster


SCALING:
EKS -- > SELECT CLUSTER -- > COMPUTE -- > EDIT -- > 

AUTOMODE:

CLUSTER UPDATING:
AT A TIME IN K8S WE CAN UPDATE ONE VERSION

1.28 -- > 1.29

TYPES:
1. ROLLING UPDATE: UPDATING THE VERSION FOR ONE BY ONE
2. ALL AT A TIME: ALL THE SERVER WILL BE UPDATE AT A TIME
3. REPLACING: WE WILL REPLACE OLD VERSION  SERVER WITH NEW VERSION SERVERS.


eksctl delete nodegroup --cluster EKS-2 --name my-cluster-ng-1 


What is an IAM OIDC identity provider?

OpenID Connect (OIDC) is an identity authentication protocol that is an extension of open authorization (OAuth) 2.0 to standardize the process for authenticating and authorizing users when they sign in to access digital services. OIDC provides authentication, which means verifying that users are who they say they are.


TERRAFORM CODE:
provider "aws" {
  region = "us-east-1"
}

resource "aws_vpc" "raham_vpc" {
  cidr_block = "10.0.0.0/16"
  enable_dns_support = true
  enable_dns_hostnames = true
}

resource "aws_subnet" "public_subnet_1" {
  vpc_id            = aws_vpc.raham_vpc.id
  cidr_block        = "10.0.1.0/24"
  availability_zone = "us-east-1a"
}

resource "aws_subnet" "public_subnet_2" {
  vpc_id            = aws_vpc.raham_vpc.id
  cidr_block        = "10.0.2.0/24"
  availability_zone = "us-east-1b"
}

resource "aws_eks_cluster" "raham_cluster" {
  name     = "raham-cluster"
  role_arn = aws_iam_role.eks_role.arn
  version  = "1.27"
  vpc_config {
    subnet_ids = [aws_subnet.public_subnet_1.id, aws_subnet.public_subnet_2.id]
  }
}

resource "aws_eks_node_group" "node_group" {
  cluster_name    = aws_eks_cluster.raham_cluster.name
  node_group_name = "worker-nodes"
  node_role_arn   = aws_iam_role.worker_nodes.arn
  subnet_ids      = [aws_subnet.public_subnet_1.id, aws_subnet.public_subnet_2.id]
  scaling_config {
    desired_size = 2
    min_size     = 1
    max_size     = 3
  }
}

resource "aws_iam_role" "eks_role" {
  name = "eks-cluster-role"
  assume_role_policy = jsonencode({
    Statement = [{
      Action = "sts:AssumeRole",
      Effect = "Allow",
      Principal = { Service = "eks.amazonaws.com" }
    }]
  })
}

resource "aws_iam_role" "worker_nodes" {
  name = "eks-node-role"
  assume_role_policy = jsonencode({
    Statement = [{
      Action = "sts:AssumeRole",
      Effect = "Allow",
      Principal = { Service = "ec2.amazonaws.com" }
    }]
  })
}



kubectl get po -o custom-columns='POD-NAME:.metadata.name,NAMESPACE:.metadata.namespace,POD-IP:.status.podIP, IMAGE:.spec.containers[*].image'


POD-NAME   NAMESPACE   POD-IP         IMAGE
pod1       default     192.168.1.4    nginx
pod2       default     192.168.1.5    httpd


========================================================================================================

Charts.yml : it has app info (name, version)
template: its a folder which as k8s files
values.yml : it consist of values for resources

=======================================================================

SESSION: 84

HELM:

In K8S Helm is a package manager to install packages
in Redhat: yum & Ubuntu: apt & K8s: helm 

it is used to install applications on clusters.
we can install and deploy applications by using helm
it manages k8s resources packages through charts 
chart is collection of manifest files.
chart is a collection of files organized on a directory structure.
a running instance of a chart with a specific config is called a release.

INSTALLATION OF HELM:
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
helm version


INSTALLATION OF METRIC SERVER:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability-1.21+.yaml


DOWNLOADING CHARTS PROMETHEUS & GRAFANA:
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts

UPDATE HELM CHART REPOS:
helm repo update
helm repo list

CREATE PROMETHEUS NAMESPACE:
kubectl create namespace prometheus
kubectl get ns

INSTALL PROMETHEUS:
helm install prometheus prometheus-community/prometheus --namespace prometheus --set alertmanager.persistentVolume.storageClass="gp2" --set server.persistentVolume.storageClass="gp2"
kubectl get pods -n prometheus
kubectl get all -n prometheus

CREATE GRAFANA NAMESPACE:
kubectl create namespace grafana

INSTALL GRAFANA:
helm install grafana grafana/grafana --namespace grafana --set persistence.storageClassName="gp2" --set persistence.enabled=true --set adminPassword='RahamDevOps' --set  service.type=LoadBalancer
kubectl get pods -n grafana
kubectl get service -n grafana

Copy the EXTERNAL-IP and paste in browser

Go to Grafana Dashboard → Add the Datasource → Select the Prometheus
add the below url in Connection and save and test
http://prometheus-server.prometheus.svc.cluster.local/


Import Grafana dashboard from Grafana Labs
grafana dashboard → new → Import → 6417 → load → select prometheus → import



NOW DEPLOY ANY APPLICATION AND SEE THE RESULT IN DASHBOARD.


ADD 315 PORT TO MONITOR THE FOLLOWING TERMS:
Network I/O pressure.
Cluster CPU usage.
Cluster Memory usage.
Cluster filesystem usage.
Pods CPU usage.

ADD 1860 PORT TO MONITOR NODES INDIVIDUALLY 

11454 -- > for pv and pvcs
747 -- > pod metrics
14623 -- > k8s overview db

PORTAINER:
it is a container organizer, designed to make tasks easier, whether they are clustered or not. 
abel to connect multiple clusters, access the containers, migrate stacks between clusters
it is not a testing environment mainly used for production routines in large companies.
Portainer consists of two elements, the Portainer Server and the Portainer Agent. 
Both elements run as lightweight Docker containers on a Docker engine

Must have swarm mode and all ports enable with docker engine
curl -L https://downloads.portainer.io/ce2-16/portainer-agent-stack.yml -o portainer-agent-stack.yml
docker stack deploy -c portainer-agent-stack.yml portainer
 docker ps
public-ip of swamr master:9000


JFROG:

#! /bin/bash
sudo yum update -y
sudo yum install java-1.8* -y
sudo wget https://jfrog.bintray.com/artifactory/jfrog-artifactory-oss-6.9.6.zip
sudo yum install unzip -y
sudo unzip  jfrog-artifactory-oss-6.9.6.zip
sudo sh /root/artifactory-oss-6.9.6/bin/artifactory.sh start 


=========================================================================
SESSION-86:


K8S DEPLOYEMNT STRATAGIES:



ROLLING UPDATE:

ROLLING STRATEGIES:
If you have some application servers running then you will create some new servers you will remove old.
in that case your old code and new code will be capable of running at same time in parallel environment that is old environment and new environment.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-canary
  labels:
    app: nginx
    version: canary
spec:
  replicas: 6
  selector:
    matchLabels:
      app: nginx
      version: canary
  template:
    metadata:
      labels:
        app: nginx
        version: canary
    spec:
      containers:
        - name: nginx
          image: nginx:1.19
          ports:
            - containerPort: 80

RECREATE:

We need to Down all the servers and we need to deploy new version then we need to bring the services into Running state.
We can also achieve this on another way we need to create a separate infrastructure.
And you can migrate to new infra to minimise the downtime.
But in this case we can run previous, current and new code run in same time.
Here we required some downtime.


apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-canary
  labels:
    app: nginx
    version: canary
spec:
  replicas: 6
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nginx
      version: canary
  template:
    metadata:
      labels:
        app: nginx
        version: canary
    spec:
      containers:
        - name: nginx
          image: nginx:1.19
          ports:
            - containerPort: 80




BLUE GREEN: 

BLUE-GREEN DEPLOYMENT:
If we have some existing servers then we create new servers and we will route traffic by using ELB from existing servers to the new servers.
If it will not work properly we need to do Rollback
Blue is the Old code and the Green is New code.
Blue/Green Deployment: Version B is released alongside Version A, and then the traffic is switched over to Version B.



apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-blue
  labels:
    app: nginx
    version: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
      version: blue
  template:
    metadata:
      labels:
        app: nginx
        version: blue
    spec:
      containers:
        - name: nginx
          image: nginx:1.18
          ports:
            - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-green
  labels:
    app: nginx
    version: green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
      version: green
  template:
    metadata:
      labels:
        app: nginx
        version: green
    spec:
      containers:
        - name: nginx
          image: nginx:1.19
          ports:
            - containerPort: 80


CANARY:
Canary: Version B is released to a subgroup of users, then proceeds to a full rollout

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
    version: stable
spec:
  replicas: 5
  selector:
    matchLabels:
      app: nginx
      version: stable
  template:
    metadata:
      labels:
        app: nginx
        version: stable
    spec:
      containers:
        - name: nginx
          image: nginx:1.18
          ports:
            - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-canary
  labels:
    app: nginx
    version: canary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
      version: canary
  template:
    metadata:
      labels:
        app: nginx
        version: canary
    spec:
      containers:
        - name: nginx
          image: nginx:1.19
          ports:
            - containerPort: 80

kubectl describe po | grep -i image
